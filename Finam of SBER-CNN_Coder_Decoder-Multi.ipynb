{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "colab": {
   "name": "Finam of SBER-CNN_Coder_Decoder-Multi.ipynb",
   "provenance": [
    {
     "file_id": "1u14juAdYey4S5vL5kYmpixo9mjjqNzBJ",
     "timestamp": 1584103025516
    },
    {
     "file_id": "1E5Zc5QIeUtwzjNC4s2pOE1URZ5k3Rpr9",
     "timestamp": 1584075321729
    },
    {
     "file_id": "1NXjPSAXS501exqlzAkrUJOv1pmUH2RKs",
     "timestamp": 1582895755061
    },
    {
     "file_id": "1iOm8wU4QyvTYOXZrAPFkQu2iKd8bakXL",
     "timestamp": 1582790559148
    }
   ],
   "collapsed_sections": []
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "ROOT_DIR = os.path.abspath(os.curdir)\n",
    "print(ROOT_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\r\n",
      "  Using cached pandas-1.3.4-cp38-cp38-macosx_10_9_x86_64.whl (11.4 MB)\r\n",
      "Collecting matplotlib\r\n",
      "  Using cached matplotlib-3.5.0-cp38-cp38-macosx_10_9_x86_64.whl (7.3 MB)\r\n",
      "Collecting statsmodels\r\n",
      "  Using cached statsmodels-0.13.1-cp38-cp38-macosx_10_15_x86_64.whl (9.6 MB)\r\n",
      "Collecting tensorflow\r\n",
      "  Using cached tensorflow-2.7.0-cp38-cp38-macosx_10_11_x86_64.whl (207.1 MB)\r\n",
      "Collecting sklearn\r\n",
      "  Using cached sklearn-0.0-py2.py3-none-any.whl\r\n",
      "Collecting pytz>=2017.3\r\n",
      "  Using cached pytz-2021.3-py2.py3-none-any.whl (503 kB)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in ./venv/lib/python3.8/site-packages (from pandas) (2.8.2)\r\n",
      "Collecting numpy>=1.17.3\r\n",
      "  Using cached numpy-1.21.4-cp38-cp38-macosx_10_9_x86_64.whl (16.9 MB)\r\n",
      "Collecting pillow>=6.2.0\r\n",
      "  Using cached Pillow-8.4.0-cp38-cp38-macosx_10_10_x86_64.whl (3.0 MB)\r\n",
      "Collecting cycler>=0.10\r\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./venv/lib/python3.8/site-packages (from matplotlib) (3.0.6)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.8/site-packages (from matplotlib) (21.3)\r\n",
      "Collecting kiwisolver>=1.0.1\r\n",
      "  Using cached kiwisolver-1.3.2-cp38-cp38-macosx_10_9_x86_64.whl (61 kB)\r\n",
      "Collecting fonttools>=4.22.0\r\n",
      "  Using cached fonttools-4.28.1-py3-none-any.whl (873 kB)\r\n",
      "Collecting setuptools-scm>=4\r\n",
      "  Using cached setuptools_scm-6.3.2-py3-none-any.whl (33 kB)\r\n",
      "Collecting scipy>=1.3\r\n",
      "  Using cached scipy-1.7.2-cp38-cp38-macosx_10_9_x86_64.whl (33.0 MB)\r\n",
      "Collecting patsy>=0.5.2\r\n",
      "  Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB)\r\n",
      "Collecting keras<2.8,>=2.7.0rc0\r\n",
      "  Using cached keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\r\n",
      "Collecting grpcio<2.0,>=1.24.3\r\n",
      "  Using cached grpcio-1.42.0-cp38-cp38-macosx_10_10_x86_64.whl (4.0 MB)\r\n",
      "Collecting h5py>=2.9.0\r\n",
      "  Using cached h5py-3.6.0-cp38-cp38-macosx_10_9_x86_64.whl (3.1 MB)\r\n",
      "Collecting absl-py>=0.4.0\r\n",
      "  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\r\n",
      "Collecting libclang>=9.0.1\r\n",
      "  Using cached libclang-12.0.0-py2.py3-none-macosx_10_9_x86_64.whl (12.2 MB)\r\n",
      "Collecting keras-preprocessing>=1.1.1\r\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\r\n",
      "Collecting google-pasta>=0.1.1\r\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\r\n",
      "Collecting astunparse>=1.6.0\r\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\r\n",
      "Collecting protobuf>=3.9.2\r\n",
      "  Using cached protobuf-3.19.1-cp38-cp38-macosx_10_9_x86_64.whl (1.0 MB)\r\n",
      "Collecting termcolor>=1.1.0\r\n",
      "  Using cached termcolor-1.1.0-py3-none-any.whl\r\n",
      "Collecting tensorboard~=2.6\r\n",
      "  Using cached tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\r\n",
      "Collecting typing-extensions>=3.6.6\r\n",
      "  Using cached typing_extensions-4.0.0-py3-none-any.whl (22 kB)\r\n",
      "Collecting opt-einsum>=2.3.2\r\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\r\n",
      "Collecting gast<0.5.0,>=0.2.1\r\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\r\n",
      "Collecting wrapt>=1.11.0\r\n",
      "  Using cached wrapt-1.13.3-cp38-cp38-macosx_10_9_x86_64.whl (33 kB)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (0.36.2)\r\n",
      "Requirement already satisfied: six>=1.12.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.16.0)\r\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.21.0\r\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.22.0-cp38-cp38-macosx_10_14_x86_64.whl (1.6 MB)\r\n",
      "Collecting flatbuffers<3.0,>=1.12\r\n",
      "  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\r\n",
      "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\r\n",
      "  Using cached tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\r\n",
      "Collecting scikit-learn\r\n",
      "  Using cached scikit_learn-1.0.1-cp38-cp38-macosx_10_13_x86_64.whl (7.9 MB)\r\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.8/site-packages (from setuptools-scm>=4->matplotlib) (57.0.0)\r\n",
      "Collecting tomli>=1.0.0\r\n",
      "  Using cached tomli-1.2.2-py3-none-any.whl (12 kB)\r\n",
      "Collecting google-auth<3,>=1.6.3\r\n",
      "  Using cached google_auth-2.3.3-py2.py3-none-any.whl (155 kB)\r\n",
      "Collecting requests<3,>=2.21.0\r\n",
      "  Using cached requests-2.26.0-py2.py3-none-any.whl (62 kB)\r\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\r\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\r\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\r\n",
      "  Using cached tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\r\n",
      "Collecting werkzeug>=0.11.15\r\n",
      "  Using cached Werkzeug-2.0.2-py3-none-any.whl (288 kB)\r\n",
      "Collecting markdown>=2.6.8\r\n",
      "  Using cached Markdown-3.3.6-py3-none-any.whl (97 kB)\r\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\r\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB)\r\n",
      "Collecting joblib>=0.11\r\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\r\n",
      "Collecting threadpoolctl>=2.0.0\r\n",
      "  Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\r\n",
      "Collecting cachetools<5.0,>=2.0.0\r\n",
      "  Using cached cachetools-4.2.4-py3-none-any.whl (10 kB)\r\n",
      "Collecting pyasn1-modules>=0.2.1\r\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\r\n",
      "Collecting rsa<5,>=3.1.4\r\n",
      "  Using cached rsa-4.7.2-py3-none-any.whl (34 kB)\r\n",
      "Collecting requests-oauthlib>=0.7.0\r\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\r\n",
      "Collecting importlib-metadata>=4.4\r\n",
      "  Using cached importlib_metadata-4.8.2-py3-none-any.whl (17 kB)\r\n",
      "Collecting idna<4,>=2.5\r\n",
      "  Using cached idna-3.3-py3-none-any.whl (61 kB)\r\n",
      "Collecting charset-normalizer~=2.0.0\r\n",
      "  Using cached charset_normalizer-2.0.7-py3-none-any.whl (38 kB)\r\n",
      "Collecting certifi>=2017.4.17\r\n",
      "  Using cached certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\r\n",
      "Collecting urllib3<1.27,>=1.21.1\r\n",
      "  Using cached urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\r\n",
      "Requirement already satisfied: zipp>=0.5 in ./venv/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\r\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\r\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\r\n",
      "Collecting oauthlib>=3.0.0\r\n",
      "  Using cached oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\r\n",
      "Installing collected packages: urllib3, pyasn1, idna, charset-normalizer, certifi, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, numpy, importlib-metadata, google-auth, werkzeug, tomli, threadpoolctl, tensorboard-plugin-wit, tensorboard-data-server, scipy, pytz, protobuf, markdown, joblib, grpcio, google-auth-oauthlib, absl-py, wrapt, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, setuptools-scm, scikit-learn, pillow, patsy, pandas, opt-einsum, libclang, kiwisolver, keras-preprocessing, keras, h5py, google-pasta, gast, fonttools, flatbuffers, cycler, astunparse, tensorflow, statsmodels, sklearn, matplotlib\r\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-4.2.4 certifi-2021.10.8 charset-normalizer-2.0.7 cycler-0.11.0 flatbuffers-2.0 fonttools-4.28.1 gast-0.4.0 google-auth-2.3.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.42.0 h5py-3.6.0 idna-3.3 importlib-metadata-4.8.2 joblib-1.1.0 keras-2.7.0 keras-preprocessing-1.1.2 kiwisolver-1.3.2 libclang-12.0.0 markdown-3.3.6 matplotlib-3.5.0 numpy-1.21.4 oauthlib-3.1.1 opt-einsum-3.3.0 pandas-1.3.4 patsy-0.5.2 pillow-8.4.0 protobuf-3.19.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 pytz-2021.3 requests-2.26.0 requests-oauthlib-1.3.0 rsa-4.7.2 scikit-learn-1.0.1 scipy-1.7.2 setuptools-scm-6.3.2 sklearn-0.0 statsmodels-0.13.1 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.7.0 tensorflow-estimator-2.7.0 tensorflow-io-gcs-filesystem-0.22.0 termcolor-1.1.0 threadpoolctl-3.0.0 tomli-1.2.2 typing-extensions-4.0.0 urllib3-1.26.7 werkzeug-2.0.2 wrapt-1.13.3\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install pandas matplotlib statsmodels tensorflow sklearn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'keras-tuner'...\r\n",
      "remote: Enumerating objects: 7477, done.\u001B[K\r\n",
      "remote: Counting objects: 100% (835/835), done.\u001B[K\r\n",
      "remote: Compressing objects: 100% (431/431), done.\u001B[K\r\n",
      "remote: Total 7477 (delta 516), reused 620 (delta 376), pack-reused 6642\u001B[K\r\n",
      "Receiving objects: 100% (7477/7477), 1.65 MiB | 3.41 MiB/s, done.\r\n",
      "Resolving deltas: 100% (5221/5221), done.\r\n",
      "/Users/mazeinsv/PycharmProjects/python.Neural.Market/keras-tuner\n",
      "Processing /Users/mazeinsv/PycharmProjects/python.Neural.Market/keras-tuner\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l-\b \bdone\r\n",
      "\u001B[?25hRequirement already satisfied: packaging in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from keras-tuner===master) (21.3)\r\n",
      "Requirement already satisfied: numpy in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from keras-tuner===master) (1.21.4)\r\n",
      "Requirement already satisfied: requests in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from keras-tuner===master) (2.26.0)\r\n",
      "Requirement already satisfied: scipy in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from keras-tuner===master) (1.7.2)\r\n",
      "Requirement already satisfied: tensorboard in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from keras-tuner===master) (2.7.0)\r\n",
      "Requirement already satisfied: ipython in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from keras-tuner===master) (7.29.0)\r\n",
      "Collecting kt-legacy\r\n",
      "  Using cached kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\r\n",
      "Requirement already satisfied: pickleshare in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from ipython->keras-tuner===master) (0.7.5)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from ipython->keras-tuner===master) (3.0.22)\r\n",
      "Requirement already satisfied: matplotlib-inline in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from ipython->keras-tuner===master) (0.1.3)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from ipython->keras-tuner===master) (0.18.1)\r\n",
      "Requirement already satisfied: setuptools>=18.5 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from ipython->keras-tuner===master) (57.0.0)\r\n",
      "Requirement already satisfied: decorator in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from ipython->keras-tuner===master) (5.1.0)\r\n",
      "Requirement already satisfied: pygments in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from ipython->keras-tuner===master) (2.10.0)\r\n",
      "Requirement already satisfied: backcall in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from ipython->keras-tuner===master) (0.2.0)\r\n",
      "Requirement already satisfied: appnope in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from ipython->keras-tuner===master) (0.1.2)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from ipython->keras-tuner===master) (4.8.0)\r\n",
      "Requirement already satisfied: traitlets>=4.2 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from ipython->keras-tuner===master) (5.1.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from packaging->keras-tuner===master) (3.0.6)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from requests->keras-tuner===master) (1.26.7)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from requests->keras-tuner===master) (2021.10.8)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from requests->keras-tuner===master) (3.3)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from requests->keras-tuner===master) (2.0.7)\r\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from tensorboard->keras-tuner===master) (1.42.0)\r\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from tensorboard->keras-tuner===master) (0.36.2)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from tensorboard->keras-tuner===master) (3.3.6)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from tensorboard->keras-tuner===master) (2.0.2)\r\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from tensorboard->keras-tuner===master) (3.19.1)\r\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from tensorboard->keras-tuner===master) (1.0.0)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from tensorboard->keras-tuner===master) (0.4.6)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from tensorboard->keras-tuner===master) (1.8.0)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from tensorboard->keras-tuner===master) (2.3.3)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from tensorboard->keras-tuner===master) (0.6.1)\r\n",
      "Requirement already satisfied: six in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from absl-py>=0.4->tensorboard->keras-tuner===master) (1.16.0)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner===master) (4.2.4)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner===master) (0.2.8)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner===master) (4.7.2)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner===master) (1.3.0)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from jedi>=0.16->ipython->keras-tuner===master) (0.8.2)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard->keras-tuner===master) (4.8.2)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from pexpect>4.3->ipython->keras-tuner===master) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner===master) (0.2.5)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner===master) (3.6.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner===master) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/mazeinsv/PycharmProjects/python.Neural.Market/venv/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner===master) (3.1.1)\r\n",
      "Building wheels for collected packages: keras-tuner\r\n",
      "  Building wheel for keras-tuner (setup.py) ... \u001B[?25l-\b \b\\\b \bdone\r\n",
      "\u001B[?25h  Created wheel for keras-tuner: filename=keras_tuner-master-py3-none-any.whl size=102914 sha256=b12e4ad990dc7b1e85b4413983982cda240554e7431a8b164e71b030c9f8b385\r\n",
      "  Stored in directory: /Users/mazeinsv/Library/Caches/pip/wheels/e6/6e/33/dd30707f7149940ba5d61c238cddba56f98c3b260c04665ad8\r\n",
      "\u001B[33m  WARNING: Built wheel for keras-tuner is invalid: Metadata 1.2 mandates PEP 440 version, but 'master' is not\u001B[0m\r\n",
      "Failed to build keras-tuner\r\n",
      "Installing collected packages: kt-legacy, keras-tuner\r\n",
      "    Running setup.py install for keras-tuner ... \u001B[?25l-\b \b\\\b \bdone\r\n",
      "\u001B[33m  DEPRECATION: keras-tuner was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368\u001B[0m\r\n",
      "\u001B[?25hSuccessfully installed keras-tuner-master kt-legacy-1.0.4\r\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/keras-team/keras-tuner.git\n",
    "# %cd keras-tuner\n",
    "# !pip install ."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !pip uninstall tensorflow-gpu\n",
    "# !pip install tensorflow-gpu"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from urllib.parse import urlencode\n",
    "from urllib.request import urlopen\n",
    "import urllib.request\n",
    "from datetime import datetime, timedelta\n",
    "import statsmodels.api as sm\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Для работы с google диском\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "# import sys\n",
    "# sys.path.append('/content/gdrive/My Drive/Python/Market/')\n",
    "# data_path = '/content/gdrive/My Drive/Python/Market/Data/'\n",
    "\n",
    "#Для работы локально на виртуальной машине\n",
    "data_path = ROOT_DIR + '/data/'\n",
    "os.mkdir(data_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#periods = {'tick': 1, '1m': 2, '5m': 3, '10m': 4, '15m': 5, '30m': 6, 'H': 7, 'D': 8, 'W': 9, 'M': 10}\n",
    "seccode = \"S&P\"\n",
    "period = \"D\"\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "print(\"today: \" + today)\n",
    "path_best_model = data_path + today + '_' + seccode + '_' + period + '_' + 'best.h5'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================= Мoдуль извлечения котировок с сайта www.finam.ru ==================="
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "FINAM_URL = \"http://export.finam.ru/\"  # сервер, на который стучимся\n",
    "# каждому таймфрейму на Финаме соответствует цифровой код:\n",
    "# periods = {'tick': 1, 'min': 2, '5min': 3, '10min': 4, '15min': 5, '30min': 6, 'hour': 7, 'daily': 8, 'week': 9, 'month': 10}\n",
    "periods = {'tick': 1, '1m': 2, '5m': 3, '10m': 4, '15m': 5, '30m': 6, 'H': 7, 'D': 8, 'W': 9, 'M': 10}\n",
    "# каждому символу Финам присвоил цифровой код:\n",
    "symbols = {'S&P': 13944, 'USDRUB': 901, 'ED':83, 'GD':18953, 'MICEX': 420450, 'BZ': 19473, 'ABRD':82460,'AESL':181867,'AFKS':19715,'AFLT':29,'AGRO':399716,'AKRN':17564,'ALBK':82616,'ALNU':81882,'ALRS':81820,'AMEZ':20702,'APTK':13855,'AQUA':35238,'ARMD':19676,'ARSA':19915,'ASSB':16452,'AVAN':82843,'AVAZ':39,'AVAZP':40,'BANE':81757,'BANEP':81758,'BGDE':175840,'BISV':35242,'BISVP':35243,'BLNG':21078,'BRZL':81901,'BSPB':20066,'CBOM':420694,'CHEP':20999,'CHGZ':81933,'CHKZ':21000,'CHMF':16136,'CHMK':21001,'CHZN':19960,'CLSB':16712,'CLSBP':16713,'CNTL':21002,'CNTLP':81575,'DASB':16825,'DGBZ':17919,'DIOD':35363,'DIXY':18564,'DVEC':19724,'DZRD':74744,'DZRDP':74745,'ELTZ':81934,'ENRU':16440,'EPLN':451471,'ERCO':81935,'FEES':20509,'FESH':20708,'FORTP':82164,'GAZA':81997,'GAZAP':81998,'GAZC':81398,'GAZP':16842,'GAZS':81399,'GAZT':82115,'GCHE':20125,'GMKN':795,'GRAZ':16610,'GRNT':449114,'GTLC':152876,'GTPR':175842,'GTSS':436120,'HALS':17698,'HIMC':81939,'HIMCP':81940,'HYDR':20266,'IDJT':388276,'IDVP':409486,'IGST':81885,'IGST03':81886,'IGSTP':81887,'IRAO':20516,'IRGZ':9,'IRKT':15547,'ISKJ':17137,'JNOS':15722,'JNOSP':15723,'KAZT':81941,'KAZTP':81942,'KBSB':19916,'KBTK':35285,'KCHE':20030,'KCHEP':20498,'KGKC':83261,'KGKCP':152350,'KLSB':16329,'KMAZ':15544,'KMEZ':22525,'KMTZ':81903,'KOGK':20710,'KRKN':81891,'KRKNP':81892,'KRKO':81905,'KRKOP':81906,'KROT':510,'KROTP':511,'KRSB':20912,'KRSBP':20913,'KRSG':15518,'KSGR':75094,'KTSB':16284,'KTSBP':16285,'KUBE':522,'KUNF':81943,'KUZB':83165,'KZMS':17359,'KZOS':81856,'KZOSP':81857,'LIFE':74584,'LKOH':8,'LNTA':385792,'LNZL':21004,'LNZLP':22094,'LPSB':16276,'LSNG':31,'LSNGP':542,'LSRG':19736,'LVHK':152517,'MAGE':74562,'MAGEP':74563,'MAGN':16782,'MERF':20947,'MFGS':30,'MFGSP':51,'MFON':152516,'MGNT':17086,'MGNZ':20892,'MGTS':12984,'MGTSP':12983,'MGVM':81829,'MISB':16330,'MISBP':16331,'MNFD':80390,'MOBB':82890,'MOEX':152798,'MORI':81944,'MOTZ':21116,'MRKC':20235,'MRKK':20412,'MRKP':20107,'MRKS':20346,'MRKU':20402,'MRKV':20286,'MRKY':20681,'MRKZ':20309,'MRSB':16359,'MSNG':6,'MSRS':16917,'MSST':152676,'MSTT':74549,'MTLR':21018,'MTLRP':80745,'MTSS':15523,'MUGS':81945,'MUGSP':81946,'MVID':19737,'NAUK':81992,'NFAZ':81287,'NKHP':450432,'NKNC':20100,'NKNCP':20101,'NKSH':81947,'NLMK':17046,'NMTP':19629,'NNSB':16615,'NNSBP':16616,'NPOF':81858,'NSVZ':81929,'NVTK':17370,'ODVA':20737,'OFCB':80728,'OGKB':18684,'OMSH':22891,'OMZZP':15844,'OPIN':20711,'OSMP':21006,'OTCP':407627,'PAZA':81896,'PHOR':81114,'PHST':19717,'PIKK':18654,'PLSM':81241,'PLZL':17123,'PMSB':16908,'PMSBP':16909,'POLY':175924,'PRFN':83121,'PRIM':17850,'PRIN':22806,'PRMB':80818,'PRTK':35247,'PSBR':152320,'QIWI':181610,'RASP':17713,'RBCM':74779,'RDRB':181755,'RGSS':181934,'RKKE':20321,'RLMN':152677,'RLMNP':388313,'RNAV':66644,'RODNP':66693,'ROLO':181316,'ROSB':16866,'ROSN':17273,'ROST':20637,'RSTI':20971,'RSTIP':20972,'RTGZ':152397,'RTKM':7,'RTKMP':15,'RTSB':16783,'RTSBP':16784,'RUAL':414279,'RUALR':74718,'RUGR':66893,'RUSI':81786,'RUSP':20712,'RZSB':16455,'SAGO':445,'SAGOP':70,'SARE':11,'SAREP':24,'SBER':3,'SBERP':23,'SELG':81360,'SELGP':82610,'SELL':21166,'SIBG':436091,'SIBN':2,'SKYC':83122,'SNGS':4,'SNGSP':13,'STSB':20087,'STSBP':20088,'SVAV':16080,'SYNG':19651,'SZPR':22401,'TAER':80593,'TANL':81914,'TANLP':81915,'TASB':16265,'TASBP':16266,'TATN':825,'TATNP':826,'TGKA':18382,'TGKB':17597,'TGKBP':18189,'TGKD':18310,'TGKDP':18391,'TGKN':18176,'TGKO':81899,'TNSE':420644,'TORS':16797,'TORSP':16798,'TRCN':74561,'TRMK':18441,'TRNFP':1012,'TTLK':18371,'TUCH':74746,'TUZA':20716,'UCSS':175781,'UKUZ':20717,'UNAC':22843,'UNKL':82493,'UPRO':18584,'URFD':75124,'URKA':19623,'URKZ':82611,'USBN':81953,'UTAR':15522,'UTII':81040,'UTSY':419504,'UWGN':414560,'VDSB':16352,'VGSB':16456,'VGSBP':16457,'VJGZ':81954,'VJGZP':81955,'VLHZ':17257,'VRAO':20958,'VRAOP':20959,'VRSB':16546,'VRSBP':16547,'VSMO':15965,'VSYD':83251,'VSYDP':83252,'VTBR':19043,'VTGK':19632,'VTRS':82886,'VZRZ':17068,'VZRZP':17067,'WTCM':19095,'WTCMP':19096,'YAKG':81917,'YKEN':81766,'YKENP':81769,'YNDX':388383,'YRSB':16342,'YRSBP':16343,'ZHIV':181674,'ZILL':81918,'ZMZN':556,'ZMZNP':603,'ZVEZ':82001}\n",
    "\n",
    "# Функция запрашивает котировки с сервера экспорта данных Финама по инструменту для заданного таймфрейма за последние\n",
    "# period_days дней и возвращает соответствующий датафрейм\n",
    "def GetCandles (ticker, time_frame, start_d):\n",
    "    period=periods[time_frame] # Выбор из: 'tick': 1, 'min': 2, '5min': 3, '10min': 4, '15min': 5, '30min': 6, 'hour': 7, 'daily': 8, 'week': 9, 'month': 10\n",
    "    market = 0  # можно не задавать. Это рынок, на котором торгуется бумага. Для акций работает с любой цифрой. Другие рынки не проверял.\n",
    "    # Текущий момент времени\n",
    "    end_date = datetime.today()\n",
    "    print(\"end_date \" + str(end_date))\n",
    "    # Время period_days дней назад\n",
    "    start_date = ToDate(start_d) #end_date - timedelta(days = period_days)\n",
    "    # Все параметры упаковываем в единую структуру. Здесь есть дополнительные параметры, кроме тех, которые заданы в шапке. См. комментарии внизу:\n",
    "    params = urlencode([\n",
    "     ('market', market), # на каком рынке торгуется бумага\n",
    "     ('em', symbols[ticker]), # вытягиваем цифровой символ, который соответствует бумаге.\n",
    "     ('code', ticker), # тикер нашей акции\n",
    "     ('df', start_date.day), # Начальная дата, номер дня (1-31)\n",
    "     ('mf', start_date.month - 1), # Начальная дата, номер месяца (0-11)\n",
    "     ('yf', start_date.year), # Начальная дата, год\n",
    "     ('from', start_date), # Начальная дата полностью\n",
    "     ('dt', end_date.day), # Конечная дата, номер дня\n",
    "     ('mt', end_date.month - 1), # Конечная дата, номер месяца\n",
    "     ('yt', end_date.year), # Конечная дата, год\n",
    "     ('to', end_date), # Конечная дата\n",
    "     ('p', period), # Таймфрейм\n",
    "     ('f', ticker), # Имя сформированного файла\n",
    "     ('e', \".csv\"), # Расширение сформированного файла\n",
    "     ('cn', ticker), # ещё раз тикер акции\n",
    "     ('dtf', 1), # В каком формате брать даты. Выбор из 5 возможных. См. страницу https://www.finam.ru/profile/moex-akcii/sberbank/export/\n",
    "     ('MSOR', 0), # Время свечи (0 - open; 1 - close)\n",
    "     ('mstime', \"on\"), # Московское время\n",
    "     ('mstimever', 1), # Коррекция часового пояса\n",
    "     ('sep', 1), # Разделитель полей (1 - запятая, 2 - точка, 3 - точка с запятой, 4 - табуляция, 5 - пробел)\n",
    "     ('sep2', 1), # Разделитель разрядов\n",
    "     ('datf', 1), # Формат записи в файл. Выбор из 6 возможных.\n",
    "     ('at', 1)]) # Нужны ли заголовки столбцов\n",
    "    url = FINAM_URL + ticker + \".csv?\" + params # собственно URL сформированного запроса\n",
    "    # Создаем датафрейм candles с котировками\n",
    "    print(url)\n",
    "    # candles = pd.read_csv(url)\n",
    "\n",
    "    req = urllib.request.Request(url, data=None, headers={\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36'})\n",
    "\n",
    "    candles = pd.read_csv(urllib.request.urlopen(req))\n",
    "    # candles.to_csv(\"Data/\" + ticker + \".csv\")\n",
    "\n",
    "    # Добавляем в датафрейм столбец 'DT', который будет содержать время каждой свечи в формате datetime.\n",
    "    # Формируем его из столбцов '<DATE>'и '<TIME>', их в последствие можете удалить\n",
    "    candles['DT'] = list(map(lambda d, t: ToDatetime(d, t), candles['<DATE>'], candles['<TIME>']))\n",
    "    # Возвращает Датафрейм Пандас со свечами, соответствующими запросу\n",
    "    return candles\n",
    "\n",
    "\n",
    "# Преобразует число (или строку) вида 20201030 и строку вида '12:15:00' в объект datetime.datetime(2020, 10, 30, 12, 15)\n",
    "def ToDatetime (date_num, time_hhmmss):\n",
    "    return datetime.strptime(str(date_num) + time_hhmmss, '%Y%m%d%H:%M:%S')\n",
    "\n",
    "\n",
    "# Преобразует строку (или число) вида \"20201102\" в дату (формат datetime)\n",
    "def ToDate (date_yyyymmdd):\n",
    "    return datetime.strptime(str(date_yyyymmdd), '%Y%m%d').date()\n",
    "\n",
    "# вызов micex=GetCandles ('MICEX', «15min», \"20000101\") вернет в переменную micex датафрейм, содержащий 15-ти\n",
    "# минутные свечи индекса ММВБ с начала даты \"20000101\" до текущего момента времени.\n",
    "# candles = GetCandles (\"SBER\", \"daily\", \"20000101\")\n",
    "#\n",
    "# print(candles.tail)\n",
    "# candles.to_csv(\"SBER.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# univariate multi-step encoder-decoder cnn-lstm\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, LSTM,RepeatVector,TimeDistributed\n",
    "from tensorflow.keras.layers import Conv1D,MaxPooling1D\n",
    "from tensorflow import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parser(x):\n",
    "\treturn datetime.strptime(x, '%d.%m.%Y')\n",
    " \n",
    "\n",
    "def split_dataset(data):\n",
    "\tprint(\"data len {}\".format(len(data)))\n",
    "\tshift = len(data) - step_forecast*(len(data)//step_forecast)\n",
    "\tprint(\"start row {}\".format(shift))\n",
    "\n",
    "\ttrain = data[(shift):(-(rows_to_test)-step_forecast)]\n",
    "\tstd_train = data[(shift):(-(rows_to_test)-step_forecast)]\n",
    "\ttest = data[(-(rows_to_test)-step_forecast):-step_forecast]\n",
    "\tall_history = data[(shift):]\n",
    "\tstd_history = data[(shift):]\n",
    "\n",
    "\t# train = array(split(train, len(train)/step_forecast))\n",
    "\t# std_train = array(split(std_train, len(std_train)/step_forecast))\n",
    "\t# test = array(split(test, len(test)/step_forecast))\n",
    "\t# all_history = array(split(all_history,len(all_history)/step_forecast))\n",
    "\t# std_history = array(split(std_history,len(std_history)/step_forecast))\n",
    " \n",
    "\treturn std_train, train, test , std_history , all_history\n",
    "\n",
    "\n",
    "# evaluate one or more weekly forecasts against expected values\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "\tscores = list()\n",
    "\tlist_actual = list()\n",
    "\t# calculate an RMSE score for each day\n",
    "\tfor i in range(actual.shape[1]):\n",
    "\t\t# calculate mse\n",
    "\t\tmse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "\t\t# calculate rmse\n",
    "\t\trmse = sqrt(mse)\n",
    "\t\t# store\n",
    "\t\tscores.append(rmse)\n",
    "\n",
    "\t\tlist_predict = predicted[:, i]  # [round(x) for x in predicted[:, i]]\n",
    "\t\t# print(\"Текущее:{}  Прогнозное:{} rmse:{}\".format(actual[:, i], list_predict, round(rmse)))\n",
    "\t# calculate overall RMSE\n",
    "\ts = 0\n",
    "\tfor row in range(actual.shape[0]):\n",
    "\t\tfor col in range(actual.shape[1]):\n",
    "\t\t\ts += (actual[row, col] - predicted[row, col])**2\n",
    "\tscore = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "\treturn score, scores\n",
    "\n",
    "# summarize scores\n",
    "def summarize_scores(name, score, scores):\n",
    "\ts_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "\tprint('%s: [%.3f] %s' % (name, score, s_scores))\n",
    "\n",
    "# convert history into inputs and outputs\n",
    "def to_supervised_x(train_1, n_input, n_out):\n",
    "\t# flatten data\n",
    "\tdata = train_1.reshape((train_1.shape[0]*train_1.shape[1], train_1.shape[2]))\n",
    "\tX, y = list(), list()\n",
    "\tin_start = 0\n",
    "\t# step over the entire history one time step at a time\n",
    "\tfor _ in range(len(data)):\n",
    "\t\t# define the end of the input sequence\n",
    "\t\tin_end = in_start + n_input\n",
    "\t\tout_end = in_end + n_out\n",
    "\t\t# ensure we have enough data for this instance\n",
    "\t\tif out_end <= len(data):\n",
    "\t\t\tX.append(data[in_start:in_end, :])\n",
    "\t\t\ty.append(data[in_end:out_end, 0])\n",
    "\t\t# move along one time step\n",
    "\t\tin_start += 1\n",
    "\treturn array(X)\n",
    "def to_supervised_y(train_2, n_input, n_out):\n",
    "\t# flatten data\n",
    "\tdata = train_2.reshape((train_2.shape[0]*train_2.shape[1], train_2.shape[2]))\n",
    "\tX, y = list(), list()\n",
    "\tin_start = 0\n",
    "\t# step over the entire history one time step at a time\n",
    "\tfor _ in range(len(data)):\n",
    "\t\t# define the end of the input sequence\n",
    "\t\tin_end = in_start + n_input\n",
    "\t\tout_end = in_end + n_out\n",
    "\t\t# ensure we have enough data for this instance\n",
    "\t\tif out_end <= len(data):\n",
    "\t\t\tX.append(data[in_start:in_end, :])\n",
    "\t\t\ty.append(data[in_end:out_end, 0])\n",
    "\t\t# move along one time step\n",
    "\t\tin_start += 1\n",
    "\treturn array(y)\n",
    "\n",
    "# train the model\n",
    "def build_model(train_x,train_y, train, n_input):\n",
    "\t# prepare data\n",
    "\t# train_x, train_y = to_supervised(train, n_input, step_forecast)\n",
    "\n",
    "\t# train_y = to_supervised_y(train, n_input, step_forecast)\n",
    "\t# train_x = to_supervised_x(std_train, n_input, step_forecast)\n",
    "\n",
    "\t# print('train_x')\n",
    "\t# print(train_x)\n",
    "\n",
    "\t# print('train_y')\n",
    "\t# print(train_y)\n",
    "\n",
    "\t# define parameters\n",
    "\n",
    "\tn_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "\t# reshape output into [samples, timesteps, features]\n",
    "\ttrain_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv1D(filters=128, kernel_size=3, activation='elu', input_shape=(n_timesteps,n_features)))\n",
    "\tmodel.add(Conv1D(filters=64, kernel_size=3, activation='elu'))\n",
    "\tmodel.add(MaxPooling1D(pool_size=2))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(RepeatVector(n_outputs))\n",
    "\tmodel.add(LSTM(1024, activation='elu', return_sequences=True))\n",
    "\tmodel.add(TimeDistributed(Dense(544, activation='elu')))\n",
    "\tmodel.add(TimeDistributed(Dense(544, activation='elu')))\n",
    "\tmodel.add(TimeDistributed(Dense(544, activation='elu')))\n",
    "\tmodel.add(TimeDistributed(Dense(1)))\n",
    "\tmodel.compile(loss='mse', optimizer='adam')\n",
    "\t# fit network\n",
    "\n",
    "\t\n",
    "\treturn model\n",
    "\n",
    "# make a forecast\n",
    "def forecast(model, history, n_input):\n",
    "\t# flatten data\n",
    "\tdata = array(history)\n",
    "\tdata = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "\t# retrieve last observations for input data\n",
    "\tinput_x = data[-n_input:, :]\n",
    "\tinput_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n",
    "\t# forecast the next week\n",
    "\tyhat = model.predict(input_x, verbose=0)\n",
    "\t# we only want the vector forecast\n",
    "\tyhat = yhat[0]\n",
    "\n",
    "\tinput_x = input_x[0]\n",
    "\t# yhat_list = [round(x) for x in yhat]\n",
    "\t# print(\"Вход (input_x):{} Прогноз (yhat):{}\".format(input_x[0], yhat))\n",
    "\n",
    "\treturn yhat\n",
    "\n",
    "# evaluate a single model\n",
    "def evaluate_model(train, test, n_input):\n",
    "\t# fit model\n",
    "\tmodel = build_model(train, n_input)\n",
    "\t# history is a list of weekly data\n",
    "\thistory = [x for x in train]\n",
    "\t# walk-forward validation over each week\n",
    "\tpredictions = list()\n",
    "\tfor i in range(len(test)):\n",
    "\t\t# predict the week\n",
    "\t\tyhat_sequence = forecast(model, history, n_input)\n",
    "\t\t# store the predictions\n",
    "\t\tpredictions.append(yhat_sequence)\n",
    "\t\t# get real observation and add to history for predicting the next week\n",
    "\t\thistory.append(test[i, :])\n",
    "\t# evaluate predictions days for each week\n",
    "\tpredictions = array(predictions)\n",
    "\tscore, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "\treturn score, scores, model\n",
    "\n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "\tdiff = list()\n",
    "\tfor i in range(interval, len(dataset)):\n",
    "\t\tvalue = dataset[i] - dataset[i - interval]\n",
    "\t\tdiff.append(value)\n",
    "\treturn diff\n",
    "\n",
    "# invert differenced forecast\n",
    "def inverse_difference(last_ob, value):\n",
    "\treturn value + last_ob\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parser_finam(x):\n",
    "\treturn datetime.strptime(x, '%Y%m%d')\n",
    " \n",
    " # dataset = read_csv(data_path + 'Finam/' + 'SBER.txt', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser,sep=\";\", decimal=',')\n",
    "# dataset = read_csv(data_path + 'Finam/' + 'SBER.txt', header=0, parse_dates=[2],  squeeze=True, date_parser=parser_finam,sep=\";\", decimal='.')\n",
    "\n",
    "dataset = GetCandles (seccode, period, \"20000101\")\n",
    "\n",
    "dataset = dataset.drop(['<TICKER>','<PER>','<TIME>','<VOL>'], 1)\n",
    "dataset.rename(columns={'<DATE>':'DATE','<OPEN>':'OPEN', '<HIGH>':'HIGH', '<LOW>':'LOW', '<CLOSE>':'CLOSE'}, inplace=True)\n",
    "dataset = dataset[['DATE','CLOSE']]\n",
    "dataset.set_index('DATE', inplace=True)\n",
    "# dataset = dataset[['CLOSE']]\n",
    "\n",
    "if seccode == \"SBER\":\n",
    "\tdataset.loc[:'2007-07-17'] /=1000 # Делим на 1000 все цены до указанного периода. По сберу была переоценка в лотах \n",
    "\n",
    "\n",
    "# dataset.loc['2007-07-16':'2007-07-20'].values.tolist()\n",
    "\n",
    "print(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if seccode == \"SBER\":\n",
    "  dataset = dataset.loc['2004-01-01':]\n",
    "print(dataset.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data1diff = dataset.diff(periods=1).dropna()\n",
    "print(data1diff)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "В коде выше функция diff() вычисляет разность исходного ряда с рядом с заданным смещением периода. Период смещения передается как параметр period. Т.к. в разности первое значение получиться неопределенным, то нам надо избавиться от него для этого и используется метод dropna().\n",
    "Проверим получившийся ряд на стационарность:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://habr.com/ru/post/207160/\n",
    "test = sm.tsa.adfuller(data1diff['CLOSE'])\n",
    "# test = sm.tsa.adfuller(dataset['CLOSE'])\n",
    "print('adf: ', test[0])\n",
    "print('p-value: ', test[1])\n",
    "print('Critical values: ', test[4])\n",
    "if test[0]> test[4]['5%']: \n",
    "    print('есть единичные корни, ряд не стационарен')\n",
    "else:\n",
    "    print('единичных корней нет, ряд стационарен')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "step_forecast =1\n",
    "n_input = 6 # пакет прошлых данных, назад для прогноза, т.е. 6 все входные данне за прошлый период, что бы получить прогноз\n",
    "rows_to_test = 12 # количесвтов месяцев для теста модели, кратно шагам прогноза (step_forecast), смотри след струку\n",
    "rows_to_test = rows_to_test//step_forecast*step_forecast\n",
    "print(\"rows_to_test\",rows_to_test)\n",
    "\n",
    "# split into train and test\n",
    "std_train , train, test, std_history ,all_history = split_dataset(dataset.values)\n",
    "# std_train , train, test, std_history ,all_history = split_dataset(data1diff.values)\n",
    "# evaluate model and get scores\n",
    "\n",
    "# print(all_history[-1:])\n",
    "\n",
    "# Стандартизация \n",
    "# std_train = train\n",
    "# mean = std_train.mean(axis=0) # Среднее значение\n",
    "# std = std_train.std(axis=0) # Стандартное отклонение\n",
    "\n",
    "# std_train -= mean\n",
    "# std_train /= std\n",
    "\n",
    "# std_history -= mean\n",
    "# std_history /= std\n",
    "\n",
    "# Стандартизация через sklearn \n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# x_scaler = StandardScaler().fit(std_train)\n",
    "# std_train = x_scaler.transform(std_train)\n",
    "# x_scaler = StandardScaler().fit(std_history)\n",
    "# std_history = x_scaler.transform(std_history)\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# x_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# x_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "# std_train = x_scaler.fit_transform(std_train)\n",
    "# std_history = x_scaler.fit_transform(std_history)\n",
    "# train = x_scaler.fit_transform(std_train)\n",
    "# history = x_scaler.fit_transform(std_history)\n",
    "\n",
    "# std_train = x_scaler.inverse_transform(std_train)\n",
    "\n",
    "\n",
    "# Normalize data (length of 1)\n",
    "# from sklearn.preprocessing import Normalizer\n",
    "# x_scaler = Normalizer().fit(std_train)\n",
    "# std_train = x_scaler.transform(std_train)\n",
    "# # x_scaler = Normalizer().fit(std_history)\n",
    "# std_history = x_scaler.transform(std_history)\n",
    "\n",
    "\n",
    "train = array(split(train, len(train)/step_forecast))\n",
    "std_train = array(split(std_train, len(std_train)/step_forecast))\n",
    "test = array(split(test, len(test)/step_forecast))\n",
    "all_history = array(split(all_history,len(all_history)/step_forecast))\n",
    "std_history = array(split(std_history,len(std_history)/step_forecast))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# score, scores, model = evaluate_model(train, test, n_input)\n",
    "\n",
    "# fit model\n",
    "train_y = to_supervised_y(train, n_input, step_forecast)\n",
    "train_x = to_supervised_x(std_train, n_input, step_forecast)\n",
    "model = build_model(train_x,train_y, train, n_input)\n",
    "# print(train_x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Занижаем скорость обучения при снижении качества\n",
    "reduce_ = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "# Ранний выход при переобучении\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "# Сохраняет лучшую модель (рекомендуется переобучать новую модель на найденных гиперпараметрах)\n",
    "mc = keras.callbacks.ModelCheckpoint(path_best_model, monitor='val_loss',  mode='auto', save_best_only=True)\n",
    "# mc = keras.callbacks.ModelCheckpoint(data_path + 'best.h5', monitor='val_acc', mode='auto', save_best_only=True)\n",
    "\n",
    "# tuner.search(x_train,                  # Данные для обучения\n",
    "# \t\t\t\t\t\ty_train,                  # Правильные ответы\n",
    "# \t\t\t\t\t\tbatch_size=100,           # Размер мини-выборки\n",
    "# \t\t\t\t\t\tepochs=150,                # Количество эпох обучения \n",
    "# \t\t\t\t\t\tvalidation_split=0.2,     # Часть данных, которая будет использоваться для проверки\n",
    "# \t\t\t\t\t\tcallbacks=[reduce_,es],\n",
    "# \t\t\t\t\t\tverbose=2,\n",
    "# \t\t\t\t\t\t)\n",
    "verbose, epochs, batch_size = 1, 150, 3 # обычно ставлю 150 эпох\n",
    "# verbose, epochs, batch_size = 1, 10, 20\n",
    "if os.path.exists(path_best_model):\n",
    "\tprint(\"Модель уже просчитана, загружаем модель\")\n",
    "\tmodel = keras.models.load_model(path_best_model)\n",
    "\n",
    "else:\n",
    "\tresults = model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size,\n",
    "\t\t\t\tvalidation_split=0.2,     # Часть данных, которая будет использоваться для проверки\n",
    "\t\t\t\t# validation_data=(x_test, y_test),\n",
    "\t\t\t\tcallbacks=[mc],\n",
    "\t\t\t  verbose=verbose)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# best_model.save(model_path+'stfk_mod1_google_350dpi.h5')\n",
    "# model = keras.models.load_model(data_path + 'best.h5')\n",
    "\n",
    "history = [x for x in std_history]\n",
    "# \n",
    "predictions = list()\n",
    "std_data = array(history)\n",
    "std_data = std_data.reshape((std_data.shape[0]*std_data.shape[1], std_data.shape[2]))\n",
    "\n",
    "# rows_to_test = 36\n",
    "# Смотрим тест\n",
    "for i in range(len(test)):\n",
    "  # predict the week\n",
    "  input_x = std_data[(-n_input-len(test)+i):-len(test)+i, :]\n",
    "  input_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n",
    "\n",
    "  yhat = model.predict(input_x, verbose=0)\n",
    "  # yhat = x_scaler.inverse_transform(yhat)\n",
    "\n",
    "  yhat = yhat[0]\n",
    "  # yhat_sequence = forecast_new(model, history, n_input)\n",
    "  # store the predictions\n",
    "  predictions.append(yhat)\n",
    "  # get real observation and add to history for predicting the next week\n",
    "  # history.append(test[i, :])\n",
    "# evaluate predictions days for each week\n",
    "\n",
    "# predictions = x_scaler.inverse_transform(predictions)\n",
    "predictions = array(predictions)\n",
    "score, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "\n",
    "# summarize scores\n",
    "summarize_scores('lstm', score, scores)\n",
    "predictions_data = predictions.reshape((predictions.shape[0], 1))\n",
    "df_pred = pd.DataFrame(data=predictions_data,columns=[\"predict\"])\n",
    "test_data = test[:, :, 0].reshape((test[:, :, 0].shape[0], 1))\n",
    "df_test = pd.DataFrame(data=test_data,columns=[\"test\"])\n",
    "df = pd.concat([df_test,df_pred],axis=1)\n",
    "print(df)\n",
    "df.to_excel(data_path + today + '_' + seccode + '_' + \"_prognoz.xls\")\n",
    "df[['test', 'predict']].plot(figsize=(12, 8))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Прогноз на будущее (находим последнюю известную строку)\n",
    "history_all = [x for x in all_history]\n",
    "data_all = array(history_all)\n",
    "data_all = data_all.reshape((data_all.shape[0]*data_all.shape[1], data_all.shape[2]))\n",
    "fact = data_all[-1:,0:1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_x = std_data[(-n_input):, :]\n",
    "input_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n",
    "\n",
    "yhat = model.predict(input_x, verbose=0)\n",
    "yhat = yhat[0]\n",
    "\n",
    "# print(\"Вход (input_x):{};Факт{}; Прогноз (yhat):{}\".format(input_x,fact, yhat))\n",
    "print(\"Факт вчера{}; Прогноз на сегодня(yhat):{}\".format(fact, yhat))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_x = std_data[(-n_input-1):-1, :]\n",
    "input_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n",
    "\n",
    "yhat = model.predict(input_x, verbose=0)\n",
    "yhat = yhat[0]\n",
    "# print(\"Вход (input_x):{};Факт{}; Прогноз (yhat):{}\".format(input_x,fact, yhat))\n",
    "print(\"Факт{}; Прогноз (yhat):{}\".format(fact, yhat))\n",
    "\n",
    "# plot scores\n",
    "# days = ['sun', 'mon', 'tue', 'wed', 'thr', 'fri', 'sat']\n",
    "# pyplot.plot(days, scores, marker='o', label='lstm')\n",
    "# pyplot.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# sys.exit()\n",
    "\n",
    "# best_model.save(model_path+'stfk_mod1_google_350dpi.h5')\n",
    "model = keras.models.load_model(path_best_model)\n",
    "\n",
    "history = [x for x in std_history]\n",
    "# \n",
    "predictions = list()\n",
    "std_data = array(history)\n",
    "std_data = std_data.reshape((std_data.shape[0]*std_data.shape[1], std_data.shape[2]))\n",
    "\n",
    "# rows_to_test = 36\n",
    "# Смотрим тест\n",
    "for i in range(len(test)):\n",
    "  # predict the week\n",
    "  input_x = std_data[(-n_input-len(test)+i):-len(test)+i, :]\n",
    "  input_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n",
    "\n",
    "  yhat = model.predict(input_x, verbose=0)\n",
    "  # yhat = x_scaler.inverse_transform(yhat)\n",
    "\n",
    "  yhat = yhat[0]\n",
    "  # yhat_sequence = forecast_new(model, history, n_input)\n",
    "  # store the predictions\n",
    "  predictions.append(yhat)\n",
    "  # get real observation and add to history for predicting the next week\n",
    "  # history.append(test[i, :])\n",
    "# evaluate predictions days for each week\n",
    "\n",
    "# predictions = x_scaler.inverse_transform(predictions)\n",
    "predictions = array(predictions)\n",
    "score, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "\n",
    "# summarize scores\n",
    "summarize_scores('lstm', score, scores)\n",
    "predictions_data = predictions.reshape((predictions.shape[0], 1))\n",
    "df_pred = pd.DataFrame(data=predictions_data,columns=[\"predict\"])\n",
    "test_data = test[:, :, 0].reshape((test[:, :, 0].shape[0], 1))\n",
    "df_test = pd.DataFrame(data=test_data,columns=[\"test\"])\n",
    "df = pd.concat([df_test,df_pred],axis=1)\n",
    "print(df)\n",
    "df.to_excel(data_path + today + '_' + seccode + \"_prognoz.xls\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[['test', 'predict']].plot(figsize=(12, 8))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Прогноз на будущее (последняя известная строка)\n",
    "history = [x for x in all_history]\n",
    "data = array(history)\n",
    "data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "fact = data[-1:,0:1]\n",
    "\n",
    "input_x = std_data[(-n_input-1):-1, :]\n",
    "input_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n",
    "\n",
    "yhat = model.predict(input_x, verbose=0)\n",
    "yhat = yhat[0]\n",
    "# print(\"Вход (input_x):{};Факт{}; Прогноз (yhat):{}\".format(input_x,fact, yhat))\n",
    "print(\"Факт{}; Прогноз (yhat):{}\".format(fact, yhat))\n",
    "\n",
    "# plot scores\n",
    "# days = ['sun', 'mon', 'tue', 'wed', 'thr', 'fri', 'sat']\n",
    "# pyplot.plot(days, scores, marker='o', label='lstm')\n",
    "# pyplot.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_x = std_data[(-n_input):, :]\n",
    "input_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n",
    "\n",
    "yhat = model.predict(input_x, verbose=0)\n",
    "yhat = yhat[0]\n",
    "# print(\"Вход (input_x):{};Факт{}; Прогноз (yhat):{}\".format(input_x,fact, yhat))\n",
    "print(\"Факт вчера{}; Прогноз на сегодня(yhat):{}\".format(fact, yhat))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Low: Факт вчера[[189.1]]; Прогноз на сегодня(yhat):[[194.83665]]\n",
    "\n",
    "Close: Факт вчера[[192.64]]; Прогноз на сегодня(yhat):[[191.94373]]\n",
    "\n",
    "High: Факт вчера[[195.]]; Прогноз на сегодня(yhat):[[194.56512]] - весьма точно\n",
    "\n",
    "Close Без Open: Факт вчера[[192.64]]; Прогноз на сегодня(yhat):[[204.97925]] - точно если сделать (-10)\n",
    "\n",
    "High Без Open: Факт вчера[[195.]]; Прогноз на сегодня(yhat):[[197.63686]] \n",
    "lstm: [7.108] 7.1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# best_model.save(model_path+'stfk_mod1_google_350dpi.h5')\n",
    "# model = keras.models.load_model(data_path + 'best.h5')\n",
    "\n",
    "history = [x for x in std_history]\n",
    "# \n",
    "predictions = list()\n",
    "std_data = array(history)\n",
    "std_data = std_data.reshape((std_data.shape[0]*std_data.shape[1], std_data.shape[2]))\n",
    "\n",
    "# rows_to_test = 36\n",
    "# Смотрим тест\n",
    "\n",
    "for i in range(len(train)):\n",
    "  # predict the week\n",
    "  input_x = std_data[(-n_input-len(train)+i):-len(train)+i, :]\n",
    "  input_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n",
    "\n",
    "  yhat = model.predict(input_x, verbose=0)\n",
    "  # yhat = x_scaler.inverse_transform(yhat)\n",
    "\n",
    "  yhat = yhat[0]\n",
    "  # yhat_sequence = forecast_new(model, history, n_input)\n",
    "  # store the predictions\n",
    "  predictions.append(yhat)\n",
    "  # get real observation and add to history for predicting the next week\n",
    "  # history.append(test[i, :])\n",
    "# evaluate predictions days for each week\n",
    "\n",
    "# predictions = x_scaler.inverse_transform(predictions)\n",
    "predictions = array(predictions)\n",
    "score, scores = evaluate_forecasts(train[:, :, 0], predictions)\n",
    "\n",
    "# summarize scores\n",
    "summarize_scores('lstm', score, scores)\n",
    "\n",
    "predictions_data = predictions.reshape((predictions.shape[0], 1))\n",
    "df_pred = pd.DataFrame(data=predictions_data,columns=[\"predict\"])\n",
    "test_data = train[:, :, 0].reshape((train[:, :, 0].shape[0], 1))\n",
    "df_test = pd.DataFrame(data=test_data,columns=[\"test\"])\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.concat([df_test,df_pred],axis=1)\n",
    "df = df[-100:]\n",
    "df[['test', 'predict']].plot(figsize=(12, 8))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}