{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 445,
   "outputs": [],
   "source": [
    "# !pip3 install tensorflow pandas numpy matplotlib yahoo_fin sklearn fredapi openpyxl"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from fredapi import Fred\n",
    "\n",
    "fred = Fred(api_key='39fa3bd07f8f55540a93e075a5f97cc1')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "outputs": [],
   "source": [
    "series_ids = [\n",
    "    'CHNCPIALLMINMEI','USACPIALLMINMEI','BRACPIALLMINMEI','INDCPIALLMINMEI','BRAPROINDMISMEI'\n",
    "    ,'USAPROINDMISMEI','PCUOMFGOMFG','RUSCPIALLMINMEI','PIEATI02RUM661N','RUSPROMANMISMEI'\n",
    "    ,'UNRATENSA','GS10','M2NS','INTDSRUSM193N','TOTALNSA','MABMM301USM189S','ALTSALES','RSXFSN'\n",
    "    ,'HTRUCKSSA','M2REAL','M1NS','BUSLOANS','MCOILWTICO','DAUPNSA','FRGSHPUSM649NCIS','PCU33443344'\n",
    "    ,'AISRSA' ,'M1REAL','TRUCKD11','RAILFRTCARLOADS','MNFCTRIRNSA','DAUTOSAAR'\n",
    "    ,'LTOTALNSA','MVMTD027MNFRBDAL','USEPUINDXM','CEU4348400001','CEU1021100001'\n",
    "    ,'IPG3361T3S','TRESEGUSM052N','LAUTONSA','WPU114','RSGASSN','CMRMT','NATURALGAS'\n",
    "    ,'EMVOVERALLEMV','PCU483111483111','WPU101706','GASREGCOVM','WPU11','PCU48214821'\n",
    "    ,'WPU1413','MRTSIR441USN','U36SNO','PCU21112111','IPB53122N','IPB54100N','EXUSEU'\n",
    "    ,'PCU4841214841212','GASDESM','EXCHUS','IPN213111N','PCU21212121','WPU012'\n",
    "    # ,'M1109BUSM293NNBR','PCU484484','IC131','JTU2300JOL','WPU801104','WPU3011','PCU33613361'\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed, so we can get the same results after rerunning several times\n",
    "np.random.seed(314)\n",
    "tf.random.set_seed(314)\n",
    "random.seed(314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# Window size or the sequence length\n",
    "N_STEPS = 1\n",
    "# Lookup step, 1 is the next day\n",
    "LOOKUP_STEP = 1\n",
    "\n",
    "# whether to scale feature columns & output price as well\n",
    "SCALE = True\n",
    "scale_str = f\"sc-{int(SCALE)}\"\n",
    "# whether to shuffle the dataset\n",
    "SHUFFLE = True\n",
    "shuffle_str = f\"sh-{int(SHUFFLE)}\"\n",
    "# whether to split the training/testing set by date\n",
    "SPLIT_BY_DATE = False\n",
    "split_by_date_str = f\"sbd-{int(SPLIT_BY_DATE)}\"\n",
    "# test ratio size, 0.2 is 20%\n",
    "TEST_SIZE = 0.2\n",
    "# features to use\n",
    "# FEATURE_COLUMNS = [\"close\", \"volume\", \"open\", \"high\", \"low\",\"ema100\"]\n",
    "FEATURE_COLUMNS = [\"close\", \"volume\", \"open\", \"high\", \"low\",\"ma7\",\"ma21\"\n",
    "                      # ,\"ma100\",\"ma50\"\n",
    "                      ,\"26ema\",\"12ema\",\"MACD\",\"ema\",\"momentum\" ] + series_ids\n",
    "\n",
    "# date now\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "### model parameters\n",
    "\n",
    "N_LAYERS = 2\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "# 256 LSTM neurons\n",
    "UNITS = 256\n",
    "# 40% dropout\n",
    "DROPOUT = 0.4\n",
    "# whether to use bidirectional RNNs\n",
    "BIDIRECTIONAL = False\n",
    "\n",
    "### training parameters\n",
    "\n",
    "# mean absolute error loss\n",
    "# LOSS = \"mae\"\n",
    "# huber loss\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 700\n",
    "\n",
    "# Amazon stock market\n",
    "ticker = \"^GSPC\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}\")\n",
    "# model name to save, making it as unique as possible based on parameters\n",
    "model_name = f\"{date_now}_{ticker}-{shuffle_str}-{scale_str}-{split_by_date_str}-\\\n",
    "{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
    "if BIDIRECTIONAL:\n",
    "    model_name += \"-b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_technical_indicators(dataset):\n",
    "    # Create 7 and 21 days Moving Average\n",
    "    dataset['ma7'] = dataset['close'].rolling(window=7).mean()\n",
    "    dataset['ma21'] = dataset['close'].rolling(window=21).mean()\n",
    "    # dataset['ma100'] = dataset['close'].rolling(window=100).mean()\n",
    "    # dataset['ma50'] = dataset['close'].rolling(window=50).mean()\n",
    "\n",
    "    # Create MACD\n",
    "\t# candles['ema20'] = pd.Series.ewm(candles['<CLOSE>'], span=20).mean()\n",
    "    dataset['26ema'] = pd.Series.ewm(dataset['close'], span=26).mean()\n",
    "    dataset['12ema'] = pd.Series.ewm(dataset['close'], span=12).mean()\n",
    "    dataset['MACD'] = (dataset['12ema']-dataset['26ema'])\n",
    "\n",
    "    # Create Bollinger Bands\n",
    "    # dataset['20sd'] = pd.stats.moments.rolling_std(dataset['GS'],20)\n",
    "    # dataset['upper_band'] = dataset['ma21'] + (dataset['20sd']*2)\n",
    "    # dataset['lower_band'] = dataset['ma21'] - (dataset['20sd']*2)\n",
    "\n",
    "    # Create Exponential moving average\n",
    "    dataset['ema'] = dataset['close'].ewm(com=0.5).mean()\n",
    "\n",
    "    # Create Momentum\n",
    "    dataset['momentum'] = dataset['close']-1\n",
    "\n",
    "    dataset.replace('', np.nan, inplace=True)\n",
    "    dataset = dataset.dropna()\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def shuffle_in_unison(a, b):\n",
    "    # shuffle two arrays in the same way\n",
    "    state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(b)\n",
    "\n",
    "\n",
    "def load_data(ticker, n_steps=10, scale=True, shuffle=True, lookup_step=1, split_by_date=True,\n",
    "                test_size=0.2, feature_columns=['close', 'volume', 'open', 'high', 'low']):\n",
    "    \"\"\"\n",
    "    Loads data from Yahoo Finance source, as well as scaling, shuffling, normalizing and splitting.\n",
    "    Params:\n",
    "        ticker (str/pd.DataFrame): the ticker you want to load, examples include AAPL, TESL, etc.\n",
    "        n_steps (int): the historical sequence length (i.e window size) used to predict, default is 50\n",
    "        scale (bool): whether to scale prices from 0 to 1, default is True\n",
    "        shuffle (bool): whether to shuffle the dataset (both training & testing), default is True\n",
    "        lookup_step (int): the future lookup step to predict, default is 1 (e.g next day)\n",
    "        split_by_date (bool): whether we split the dataset into training/testing by date, setting it \n",
    "            to False will split datasets in a random way\n",
    "        test_size (float): ratio for test data, default is 0.2 (20% testing data)\n",
    "        feature_columns (list): the list of features to use to feed into the model, default is everything grabbed from yahoo_fin\n",
    "    \"\"\"\n",
    "    # see if ticker is already a loaded stock from yahoo finance\n",
    "    if isinstance(ticker, str):\n",
    "        # load it from yahoo_fin library\n",
    "        # df = si.get_data(ticker)\n",
    "        df = si.get_data(ticker,start_date = '2000-01-01', end_date = None, index_as_date = True, interval= \"1mo\")\n",
    "        # get_data(ticker, start_date = None, end_date = None, index_as_date = True, interval = “1d”)\n",
    "    elif isinstance(ticker, pd.DataFrame):\n",
    "        # already loaded, use it directly\n",
    "        df = ticker\n",
    "    else:\n",
    "        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instances\")\n",
    "\n",
    "\n",
    "    df = get_technical_indicators(df)\n",
    "    df_fred = get_fred()\n",
    "    df = pd.concat([df_fred , df], axis=1)\n",
    "\n",
    "    df.replace('', np.nan, inplace=True)\n",
    "    df = df.dropna()\n",
    "\n",
    "    # print(df.tail)\n",
    "    # this will contain all the elements we want to return from this function\n",
    "    result = {}\n",
    "    # we will also return the original dataframe itself\n",
    "    result['df'] = df.copy()\n",
    "\n",
    "    # make sure that the passed feature_columns exist in the dataframe\n",
    "    for col in feature_columns:\n",
    "        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n",
    "\n",
    "    # add date as a column\n",
    "    if \"date\" not in df.columns:\n",
    "        df[\"date\"] = df.index\n",
    "\n",
    "    if scale:\n",
    "        column_scaler = {}\n",
    "        # scale the data (prices) from 0 to 1\n",
    "        for column in feature_columns:\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
    "            column_scaler[column] = scaler\n",
    "\n",
    "        # add the MinMaxScaler instances to the result returned\n",
    "        result[\"column_scaler\"] = column_scaler\n",
    "\n",
    "    # add the target column (label) by shifting by `lookup_step`\n",
    "    df['future'] = df['close'].shift(-lookup_step)\n",
    "\n",
    "    # last `lookup_step` columns contains NaN in future column\n",
    "    # get them before droping NaNs\n",
    "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
    "    \n",
    "    # drop NaNs\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    sequence_data = []\n",
    "    sequences = deque(maxlen=n_steps)\n",
    "\n",
    "    for entry, target in zip(df[feature_columns + [\"date\"]].values, df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == n_steps:\n",
    "            sequence_data.append([np.array(sequences), target])\n",
    "\n",
    "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
    "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 60 (that is 50+10) length\n",
    "    # this last_sequence will be used to predict future stock prices that are not available in the dataset\n",
    "    last_sequence = list([s[:len(feature_columns)] for s in sequences]) + list(last_sequence)\n",
    "    last_sequence = np.array(last_sequence).astype(np.float32)\n",
    "    # add to result\n",
    "    result['last_sequence'] = last_sequence\n",
    "    \n",
    "    # construct the X's and y's\n",
    "    X, y = [], []\n",
    "    for seq, target in sequence_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "\n",
    "    # convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    if split_by_date:\n",
    "        # split the dataset into training & testing sets by date (not randomly splitting)\n",
    "        train_samples = int((1 - test_size) * len(X))\n",
    "        result[\"X_train\"] = X[:train_samples]\n",
    "        result[\"y_train\"] = y[:train_samples]\n",
    "        result[\"X_test\"]  = X[train_samples:]\n",
    "        result[\"y_test\"]  = y[train_samples:]\n",
    "        if shuffle:\n",
    "            # shuffle the datasets for training (if shuffle parameter is set)\n",
    "            shuffle_in_unison(result[\"X_train\"], result[\"y_train\"])\n",
    "            shuffle_in_unison(result[\"X_test\"], result[\"y_test\"])\n",
    "    else:    \n",
    "        # split the dataset randomly\n",
    "        result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, \n",
    "                                                                                test_size=test_size, shuffle=shuffle)\n",
    "\n",
    "    # get the list of test set dates\n",
    "    dates = result[\"X_test\"][:, -1, -1]\n",
    "    # retrieve test features from the original dataframe\n",
    "    result[\"test_df\"] = result[\"df\"].loc[dates]\n",
    "    # remove duplicated dates in the testing dataframe\n",
    "    result[\"test_df\"] = result[\"test_df\"][~result[\"test_df\"].index.duplicated(keep='first')]\n",
    "    # remove dates from the training/testing sets & convert to float32\n",
    "    result[\"X_train\"] = result[\"X_train\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    result[\"X_test\"] = result[\"X_test\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_fred():\n",
    "    # series_ids = ['PCPI06037', 'PCPI06075', 'PCPI24510']\n",
    "    # ticker_fred = 'SP500'\n",
    "    # dataset_fred = fred.get_series(ticker_fred, observation_start=\"1/1/2000\")\n",
    "    # print(dataset_fred)\n",
    "    # dataset_fred = pd.DataFrame({'date':dataset_fred.index.tolist(),ticker_fred:dataset_fred.tolist()})\n",
    "    # dataset_fred.set_index('date', inplace=True)\n",
    "    #\n",
    "    # dataset = pd.concat([dataset, dataset_fred],axis=1)\n",
    "    data_ = {}\n",
    "    for series_id in series_ids:\n",
    "        data_[series_id] = fred.get_series(series_id, observation_start='1/1/2000')\n",
    "    dataset_ = pd.concat(data_, axis=1)\n",
    "    dataset_.replace('', np.nan, inplace=True)\n",
    "    dataset_=dataset_.fillna(dataset_[-10:].mean())\n",
    "    # dataset = pd.concat([dataset , df], axis=1)\n",
    "    # print(df)\n",
    "\n",
    "    return dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_s = []\n",
    "# data_s = pd.DataFrame\n",
    "# data_s = get_fred('SP500')\n",
    "# ticker_fred = 'SP500'\n",
    "# dataset_fred_ = fred.get_series(ticker_fred)\n",
    "# print(dataset_fred_)\n",
    "# ddf = get_fred()\n",
    "# ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "outputs": [],
   "source": [
    "# ddf.to_excel(\"fred.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "outputs": [],
   "source": [
    "# df_yaho = si.get_data(ticker,start_date = '2000-01-01', end_date = None, index_as_date = True, interval= \"1mo\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "outputs": [],
   "source": [
    "# df_yaho"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "outputs": [],
   "source": [
    "# df_sum = pd.concat([df_yaho,ddf],axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "outputs": [],
   "source": [
    "# df_sum\n",
    "# df_sum.to_excel(\"view.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "outputs": [],
   "source": [
    "# df_sum.replace('', np.nan, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "outputs": [],
   "source": [
    "# df_sum.fillna(method='ffill')\n",
    "# df_sum_out=df_sum.fillna(df_sum[-10:].mean())\n",
    "# df_sum = df_sum.dropna()\n",
    "# data_s.to_excel(\"test.xlsx\")\n",
    "# data_s.to_excel(\"test.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "outputs": [],
   "source": [
    "# df_sum_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "outputs": [],
   "source": [
    "# df_sum_out.to_excel(\"view2.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "outputs": [],
   "source": [
    "def create_model(sequence_length, n_features, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True), batch_input_shape=(None, sequence_length, n_features)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True, batch_input_shape=(None, sequence_length, n_features)))\n",
    "        elif i == n_layers - 1:\n",
    "            # last layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# create these folders if they does not exist\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "\n",
    "# load the data\n",
    "data = load_data(ticker, N_STEPS, scale=SCALE, split_by_date=SPLIT_BY_DATE, \n",
    "                shuffle=SHUFFLE, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, \n",
    "                feature_columns=FEATURE_COLUMNS)\n",
    "\n",
    "# save the dataframe\n",
    "# data[\"df\"].to_csv(ticker_data_filename + \".csv\")\n",
    "data[\"df\"].to_excel(\"view.xlsx\")\n",
    "# data.to_csv(ticker_data_filename + \"_2.csv\")\n",
    "\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, len(FEATURE_COLUMNS), loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "outputs": [],
   "source": [
    "# data[\"df\"] = data[\"df\"].dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "outputs": [],
   "source": [
    "# data[\"df\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "outputs": [],
   "source": [
    "data[\"df\"].to_excel(\"view_start.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n",
      "1/3 [=========>....................] - ETA: 9s - loss: 0.0491 - mean_absolute_error: 0.2425\n",
      "Epoch 00001: val_loss improved from inf to 0.01700, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 6s 600ms/step - loss: 0.0417 - mean_absolute_error: 0.2054 - val_loss: 0.0170 - val_mean_absolute_error: 0.1274\n",
      "Epoch 2/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0143 - mean_absolute_error: 0.1172\n",
      "Epoch 00002: val_loss improved from 0.01700 to 0.01532, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0147 - mean_absolute_error: 0.1352 - val_loss: 0.0153 - val_mean_absolute_error: 0.1579\n",
      "Epoch 3/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0183 - mean_absolute_error: 0.1717\n",
      "Epoch 00003: val_loss improved from 0.01532 to 0.00663, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0148 - mean_absolute_error: 0.1531 - val_loss: 0.0066 - val_mean_absolute_error: 0.0961\n",
      "Epoch 4/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0073 - mean_absolute_error: 0.0998\n",
      "Epoch 00004: val_loss improved from 0.00663 to 0.00554, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0068 - mean_absolute_error: 0.0888 - val_loss: 0.0055 - val_mean_absolute_error: 0.0762\n",
      "Epoch 5/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0064 - mean_absolute_error: 0.0764\n",
      "Epoch 00005: val_loss improved from 0.00554 to 0.00396, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0057 - mean_absolute_error: 0.0763 - val_loss: 0.0040 - val_mean_absolute_error: 0.0715\n",
      "Epoch 6/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0037 - mean_absolute_error: 0.0688\n",
      "Epoch 00006: val_loss improved from 0.00396 to 0.00171, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.0031 - mean_absolute_error: 0.0635 - val_loss: 0.0017 - val_mean_absolute_error: 0.0524\n",
      "Epoch 7/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0028 - mean_absolute_error: 0.0621\n",
      "Epoch 00007: val_loss did not improve from 0.00171\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.0031 - mean_absolute_error: 0.0653 - val_loss: 0.0026 - val_mean_absolute_error: 0.0644\n",
      "Epoch 8/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0025 - mean_absolute_error: 0.0572\n",
      "Epoch 00008: val_loss improved from 0.00171 to 0.00136, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.0024 - mean_absolute_error: 0.0573 - val_loss: 0.0014 - val_mean_absolute_error: 0.0446\n",
      "Epoch 9/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0016 - mean_absolute_error: 0.0457\n",
      "Epoch 00009: val_loss did not improve from 0.00136\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.0019 - mean_absolute_error: 0.0498 - val_loss: 0.0015 - val_mean_absolute_error: 0.0459\n",
      "Epoch 10/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0015 - mean_absolute_error: 0.0435\n",
      "Epoch 00010: val_loss improved from 0.00136 to 0.00071, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0016 - mean_absolute_error: 0.0443 - val_loss: 7.1195e-04 - val_mean_absolute_error: 0.0321\n",
      "Epoch 11/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0023 - mean_absolute_error: 0.0445\n",
      "Epoch 00011: val_loss did not improve from 0.00071\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.0017 - mean_absolute_error: 0.0398 - val_loss: 7.2350e-04 - val_mean_absolute_error: 0.0279\n",
      "Epoch 12/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0014 - mean_absolute_error: 0.0381\n",
      "Epoch 00012: val_loss improved from 0.00071 to 0.00065, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0014 - mean_absolute_error: 0.0388 - val_loss: 6.4557e-04 - val_mean_absolute_error: 0.0272\n",
      "Epoch 13/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0017 - mean_absolute_error: 0.0414\n",
      "Epoch 00013: val_loss improved from 0.00065 to 0.00052, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0018 - mean_absolute_error: 0.0410 - val_loss: 5.1521e-04 - val_mean_absolute_error: 0.0231\n",
      "Epoch 14/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0014 - mean_absolute_error: 0.0374\n",
      "Epoch 00014: val_loss improved from 0.00052 to 0.00050, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0012 - mean_absolute_error: 0.0351 - val_loss: 5.0112e-04 - val_mean_absolute_error: 0.0235\n",
      "Epoch 15/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0020 - mean_absolute_error: 0.0417\n",
      "Epoch 00015: val_loss improved from 0.00050 to 0.00043, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0013 - mean_absolute_error: 0.0345 - val_loss: 4.2693e-04 - val_mean_absolute_error: 0.0230\n",
      "Epoch 16/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0012 - mean_absolute_error: 0.0353\n",
      "Epoch 00016: val_loss improved from 0.00043 to 0.00042, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 9.4301e-04 - mean_absolute_error: 0.0328 - val_loss: 4.2221e-04 - val_mean_absolute_error: 0.0228\n",
      "Epoch 17/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0014 - mean_absolute_error: 0.0371\n",
      "Epoch 00017: val_loss did not improve from 0.00042\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.0011 - mean_absolute_error: 0.0336 - val_loss: 5.2625e-04 - val_mean_absolute_error: 0.0242\n",
      "Epoch 18/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.9019e-04 - mean_absolute_error: 0.0324\n",
      "Epoch 00018: val_loss improved from 0.00042 to 0.00040, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.0012 - mean_absolute_error: 0.0321 - val_loss: 4.0383e-04 - val_mean_absolute_error: 0.0224\n",
      "Epoch 19/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.6346e-04 - mean_absolute_error: 0.0303\n",
      "Epoch 00019: val_loss did not improve from 0.00040\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 8.1780e-04 - mean_absolute_error: 0.0315 - val_loss: 5.0131e-04 - val_mean_absolute_error: 0.0241\n",
      "Epoch 20/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0017 - mean_absolute_error: 0.0396\n",
      "Epoch 00020: val_loss improved from 0.00040 to 0.00039, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0011 - mean_absolute_error: 0.0331 - val_loss: 3.9058e-04 - val_mean_absolute_error: 0.0217\n",
      "Epoch 21/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0012 - mean_absolute_error: 0.0368\n",
      "Epoch 00021: val_loss did not improve from 0.00039\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.0011 - mean_absolute_error: 0.0345 - val_loss: 4.5997e-04 - val_mean_absolute_error: 0.0232\n",
      "Epoch 22/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - mean_absolute_error: 0.0338\n",
      "Epoch 00022: val_loss improved from 0.00039 to 0.00038, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 8.6741e-04 - mean_absolute_error: 0.0306 - val_loss: 3.8339e-04 - val_mean_absolute_error: 0.0217\n",
      "Epoch 23/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.8145e-04 - mean_absolute_error: 0.0281\n",
      "Epoch 00023: val_loss did not improve from 0.00038\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 9.1671e-04 - mean_absolute_error: 0.0314 - val_loss: 4.4041e-04 - val_mean_absolute_error: 0.0231\n",
      "Epoch 24/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.1452e-04 - mean_absolute_error: 0.0280\n",
      "Epoch 00024: val_loss improved from 0.00038 to 0.00038, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 7.7661e-04 - mean_absolute_error: 0.0295 - val_loss: 3.7626e-04 - val_mean_absolute_error: 0.0214\n",
      "Epoch 25/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - mean_absolute_error: 0.0342\n",
      "Epoch 00025: val_loss did not improve from 0.00038\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.0011 - mean_absolute_error: 0.0332 - val_loss: 4.8667e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 26/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0014 - mean_absolute_error: 0.0356\n",
      "Epoch 00026: val_loss improved from 0.00038 to 0.00037, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 9.0613e-04 - mean_absolute_error: 0.0299 - val_loss: 3.7169e-04 - val_mean_absolute_error: 0.0215\n",
      "Epoch 27/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0013 - mean_absolute_error: 0.0345\n",
      "Epoch 00027: val_loss did not improve from 0.00037\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 9.7494e-04 - mean_absolute_error: 0.0318 - val_loss: 4.0351e-04 - val_mean_absolute_error: 0.0223\n",
      "Epoch 28/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.9699e-04 - mean_absolute_error: 0.0309\n",
      "Epoch 00028: val_loss improved from 0.00037 to 0.00036, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.0012 - mean_absolute_error: 0.0335 - val_loss: 3.5644e-04 - val_mean_absolute_error: 0.0211\n",
      "Epoch 29/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3073e-04 - mean_absolute_error: 0.0298\n",
      "Epoch 00029: val_loss did not improve from 0.00036\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 8.6096e-04 - mean_absolute_error: 0.0300 - val_loss: 4.4824e-04 - val_mean_absolute_error: 0.0227\n",
      "Epoch 30/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0010 - mean_absolute_error: 0.0340\n",
      "Epoch 00030: val_loss did not improve from 0.00036\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 8.8600e-04 - mean_absolute_error: 0.0306 - val_loss: 3.5902e-04 - val_mean_absolute_error: 0.0214\n",
      "Epoch 31/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.5321e-04 - mean_absolute_error: 0.0347\n",
      "Epoch 00031: val_loss did not improve from 0.00036\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 9.7657e-04 - mean_absolute_error: 0.0332 - val_loss: 3.9421e-04 - val_mean_absolute_error: 0.0220\n",
      "Epoch 32/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - mean_absolute_error: 0.0318\n",
      "Epoch 00032: val_loss improved from 0.00036 to 0.00034, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 8.8963e-04 - mean_absolute_error: 0.0299 - val_loss: 3.4059e-04 - val_mean_absolute_error: 0.0205\n",
      "Epoch 33/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.7767e-04 - mean_absolute_error: 0.0301\n",
      "Epoch 00033: val_loss did not improve from 0.00034\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 8.8584e-04 - mean_absolute_error: 0.0294 - val_loss: 5.2907e-04 - val_mean_absolute_error: 0.0244\n",
      "Epoch 34/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0013 - mean_absolute_error: 0.0393\n",
      "Epoch 00034: val_loss improved from 0.00034 to 0.00030, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 9.4485e-04 - mean_absolute_error: 0.0330 - val_loss: 3.0050e-04 - val_mean_absolute_error: 0.0196\n",
      "Epoch 35/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.7930e-04 - mean_absolute_error: 0.0269\n",
      "Epoch 00035: val_loss did not improve from 0.00030\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 8.3896e-04 - mean_absolute_error: 0.0288 - val_loss: 3.5277e-04 - val_mean_absolute_error: 0.0203\n",
      "Epoch 36/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - mean_absolute_error: 0.0346\n",
      "Epoch 00036: val_loss did not improve from 0.00030\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.0012 - mean_absolute_error: 0.0322 - val_loss: 3.0557e-04 - val_mean_absolute_error: 0.0192\n",
      "Epoch 37/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.5440e-04 - mean_absolute_error: 0.0287\n",
      "Epoch 00037: val_loss did not improve from 0.00030\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 7.4463e-04 - mean_absolute_error: 0.0280 - val_loss: 3.7869e-04 - val_mean_absolute_error: 0.0204\n",
      "Epoch 38/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.8513e-04 - mean_absolute_error: 0.0308\n",
      "Epoch 00038: val_loss did not improve from 0.00030\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 8.0885e-04 - mean_absolute_error: 0.0283 - val_loss: 3.6828e-04 - val_mean_absolute_error: 0.0206\n",
      "Epoch 39/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0012 - mean_absolute_error: 0.0319\n",
      "Epoch 00039: val_loss did not improve from 0.00030\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 9.8874e-04 - mean_absolute_error: 0.0315 - val_loss: 3.0987e-04 - val_mean_absolute_error: 0.0193\n",
      "Epoch 40/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.8060e-04 - mean_absolute_error: 0.0251\n",
      "Epoch 00040: val_loss did not improve from 0.00030\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 7.3095e-04 - mean_absolute_error: 0.0269 - val_loss: 3.1172e-04 - val_mean_absolute_error: 0.0202\n",
      "Epoch 41/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.1614e-04 - mean_absolute_error: 0.0257\n",
      "Epoch 00041: val_loss did not improve from 0.00030\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 6.6597e-04 - mean_absolute_error: 0.0267 - val_loss: 3.1571e-04 - val_mean_absolute_error: 0.0201\n",
      "Epoch 42/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.4942e-04 - mean_absolute_error: 0.0283\n",
      "Epoch 00042: val_loss did not improve from 0.00030\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 9.8621e-04 - mean_absolute_error: 0.0297 - val_loss: 3.1474e-04 - val_mean_absolute_error: 0.0192\n",
      "Epoch 43/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.0114e-04 - mean_absolute_error: 0.0280\n",
      "Epoch 00043: val_loss improved from 0.00030 to 0.00029, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 8.4473e-04 - mean_absolute_error: 0.0284 - val_loss: 2.8541e-04 - val_mean_absolute_error: 0.0190\n",
      "Epoch 44/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.0833e-04 - mean_absolute_error: 0.0243\n",
      "Epoch 00044: val_loss did not improve from 0.00029\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.0011 - mean_absolute_error: 0.0307 - val_loss: 2.9056e-04 - val_mean_absolute_error: 0.0188\n",
      "Epoch 45/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.5512e-04 - mean_absolute_error: 0.0285\n",
      "Epoch 00045: val_loss improved from 0.00029 to 0.00028, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 7.1851e-04 - mean_absolute_error: 0.0266 - val_loss: 2.8278e-04 - val_mean_absolute_error: 0.0188\n",
      "Epoch 46/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - mean_absolute_error: 0.0282\n",
      "Epoch 00046: val_loss did not improve from 0.00028\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 7.3579e-04 - mean_absolute_error: 0.0259 - val_loss: 2.9443e-04 - val_mean_absolute_error: 0.0189\n",
      "Epoch 47/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.1932e-04 - mean_absolute_error: 0.0273\n",
      "Epoch 00047: val_loss improved from 0.00028 to 0.00028, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 9.2830e-04 - mean_absolute_error: 0.0287 - val_loss: 2.8133e-04 - val_mean_absolute_error: 0.0186\n",
      "Epoch 48/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.9845e-04 - mean_absolute_error: 0.0278\n",
      "Epoch 00048: val_loss did not improve from 0.00028\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 6.2163e-04 - mean_absolute_error: 0.0261 - val_loss: 2.9975e-04 - val_mean_absolute_error: 0.0187\n",
      "Epoch 49/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.9870e-04 - mean_absolute_error: 0.0285\n",
      "Epoch 00049: val_loss did not improve from 0.00028\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 9.0107e-04 - mean_absolute_error: 0.0289 - val_loss: 3.9492e-04 - val_mean_absolute_error: 0.0214\n",
      "Epoch 50/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.9849e-04 - mean_absolute_error: 0.0276\n",
      "Epoch 00050: val_loss did not improve from 0.00028\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 8.9947e-04 - mean_absolute_error: 0.0288 - val_loss: 3.2526e-04 - val_mean_absolute_error: 0.0197\n",
      "Epoch 51/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - mean_absolute_error: 0.0302\n",
      "Epoch 00051: val_loss did not improve from 0.00028\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 9.5237e-04 - mean_absolute_error: 0.0292 - val_loss: 4.0890e-04 - val_mean_absolute_error: 0.0206\n",
      "Epoch 52/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.9664e-04 - mean_absolute_error: 0.0264\n",
      "Epoch 00052: val_loss did not improve from 0.00028\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 8.0252e-04 - mean_absolute_error: 0.0278 - val_loss: 3.2346e-04 - val_mean_absolute_error: 0.0197\n",
      "Epoch 53/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.6557e-04 - mean_absolute_error: 0.0271\n",
      "Epoch 00053: val_loss did not improve from 0.00028\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 9.4657e-04 - mean_absolute_error: 0.0293 - val_loss: 3.1669e-04 - val_mean_absolute_error: 0.0196\n",
      "Epoch 54/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.9346e-04 - mean_absolute_error: 0.0255\n",
      "Epoch 00054: val_loss did not improve from 0.00028\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 9.1534e-04 - mean_absolute_error: 0.0281 - val_loss: 3.0284e-04 - val_mean_absolute_error: 0.0189\n",
      "Epoch 55/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.3569e-04 - mean_absolute_error: 0.0236\n",
      "Epoch 00055: val_loss did not improve from 0.00028\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 7.4640e-04 - mean_absolute_error: 0.0260 - val_loss: 3.0764e-04 - val_mean_absolute_error: 0.0190\n",
      "Epoch 56/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.6333e-04 - mean_absolute_error: 0.0222\n",
      "Epoch 00056: val_loss did not improve from 0.00028\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 5.5340e-04 - mean_absolute_error: 0.0232 - val_loss: 3.0841e-04 - val_mean_absolute_error: 0.0190\n",
      "Epoch 57/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.1160e-04 - mean_absolute_error: 0.0258\n",
      "Epoch 00057: val_loss did not improve from 0.00028\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 7.6887e-04 - mean_absolute_error: 0.0277 - val_loss: 2.9698e-04 - val_mean_absolute_error: 0.0189\n",
      "Epoch 58/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.4900e-04 - mean_absolute_error: 0.0293\n",
      "Epoch 00058: val_loss did not improve from 0.00028\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 6.8097e-04 - mean_absolute_error: 0.0272 - val_loss: 3.6184e-04 - val_mean_absolute_error: 0.0202\n",
      "Epoch 59/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0010 - mean_absolute_error: 0.0309\n",
      "Epoch 00059: val_loss did not improve from 0.00028\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 8.2885e-04 - mean_absolute_error: 0.0285 - val_loss: 2.8177e-04 - val_mean_absolute_error: 0.0184\n",
      "Epoch 60/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.7366e-04 - mean_absolute_error: 0.0258\n",
      "Epoch 00060: val_loss did not improve from 0.00028\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 6.1462e-04 - mean_absolute_error: 0.0264 - val_loss: 3.8579e-04 - val_mean_absolute_error: 0.0212\n",
      "Epoch 61/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.7509e-04 - mean_absolute_error: 0.0311\n",
      "Epoch 00061: val_loss improved from 0.00028 to 0.00027, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 8.5791e-04 - mean_absolute_error: 0.0296 - val_loss: 2.6924e-04 - val_mean_absolute_error: 0.0179\n",
      "Epoch 62/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.8971e-04 - mean_absolute_error: 0.0249\n",
      "Epoch 00062: val_loss did not improve from 0.00027\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 7.3987e-04 - mean_absolute_error: 0.0263 - val_loss: 3.3519e-04 - val_mean_absolute_error: 0.0205\n",
      "Epoch 63/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.6283e-04 - mean_absolute_error: 0.0268\n",
      "Epoch 00063: val_loss did not improve from 0.00027\n",
      "3/3 [==============================]\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b - 0s 39ms/step - loss: 6.8863e-04 - mean_absolute_error: 0.0265 - val_loss: 2.7103e-04 - val_mean_absolute_error: 0.0182\n",
      "Epoch 64/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.8976e-04 - mean_absolute_error: 0.0249\n",
      "Epoch 00064: val_loss did not improve from 0.00027\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 7.6808e-04 - mean_absolute_error: 0.0265 - val_loss: 2.8709e-04 - val_mean_absolute_error: 0.0182\n",
      "Epoch 65/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.3394e-04 - mean_absolute_error: 0.0233\n",
      "Epoch 00065: val_loss did not improve from 0.00027\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 5.7229e-04 - mean_absolute_error: 0.0231 - val_loss: 2.7651e-04 - val_mean_absolute_error: 0.0181\n",
      "Epoch 66/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.1168e-04 - mean_absolute_error: 0.0216\n",
      "Epoch 00066: val_loss did not improve from 0.00027\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 7.0623e-04 - mean_absolute_error: 0.0255 - val_loss: 3.0071e-04 - val_mean_absolute_error: 0.0186\n",
      "Epoch 67/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.0753e-04 - mean_absolute_error: 0.0244\n",
      "Epoch 00067: val_loss improved from 0.00027 to 0.00025, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 6.6176e-04 - mean_absolute_error: 0.0252 - val_loss: 2.5083e-04 - val_mean_absolute_error: 0.0177\n",
      "Epoch 68/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.7017e-04 - mean_absolute_error: 0.0273\n",
      "Epoch 00068: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 6.5286e-04 - mean_absolute_error: 0.0272 - val_loss: 3.0863e-04 - val_mean_absolute_error: 0.0185\n",
      "Epoch 69/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.4494e-04 - mean_absolute_error: 0.0234\n",
      "Epoch 00069: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 6.9646e-04 - mean_absolute_error: 0.0258 - val_loss: 3.6799e-04 - val_mean_absolute_error: 0.0207\n",
      "Epoch 70/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.9203e-04 - mean_absolute_error: 0.0295\n",
      "Epoch 00070: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 8.6487e-04 - mean_absolute_error: 0.0283 - val_loss: 2.5365e-04 - val_mean_absolute_error: 0.0177\n",
      "Epoch 71/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.3142e-04 - mean_absolute_error: 0.0239\n",
      "Epoch 00071: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 6.9616e-04 - mean_absolute_error: 0.0267 - val_loss: 7.4426e-04 - val_mean_absolute_error: 0.0277\n",
      "Epoch 72/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0010 - mean_absolute_error: 0.0336\n",
      "Epoch 00072: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 8.7820e-04 - mean_absolute_error: 0.0304 - val_loss: 3.4414e-04 - val_mean_absolute_error: 0.0209\n",
      "Epoch 73/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.6010e-04 - mean_absolute_error: 0.0244\n",
      "Epoch 00073: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0010 - mean_absolute_error: 0.0334 - val_loss: 6.6892e-04 - val_mean_absolute_error: 0.0293\n",
      "Epoch 74/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0016 - mean_absolute_error: 0.0350\n",
      "Epoch 00074: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0011 - mean_absolute_error: 0.0306 - val_loss: 5.5920e-04 - val_mean_absolute_error: 0.0252\n",
      "Epoch 75/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.2711e-04 - mean_absolute_error: 0.0269\n",
      "Epoch 00075: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 8.7002e-04 - mean_absolute_error: 0.0310 - val_loss: 2.7267e-04 - val_mean_absolute_error: 0.0187\n",
      "Epoch 76/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.3597e-04 - mean_absolute_error: 0.0209\n",
      "Epoch 00076: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 6.5133e-04 - mean_absolute_error: 0.0256 - val_loss: 6.9026e-04 - val_mean_absolute_error: 0.0262\n",
      "Epoch 77/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3904e-04 - mean_absolute_error: 0.0288\n",
      "Epoch 00077: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 5.8266e-04 - mean_absolute_error: 0.0253 - val_loss: 2.7491e-04 - val_mean_absolute_error: 0.0186\n",
      "Epoch 78/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.4501e-04 - mean_absolute_error: 0.0272\n",
      "Epoch 00078: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 7.6228e-04 - mean_absolute_error: 0.0271 - val_loss: 3.5517e-04 - val_mean_absolute_error: 0.0205\n",
      "Epoch 79/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.2625e-04 - mean_absolute_error: 0.0296\n",
      "Epoch 00079: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 8.4599e-04 - mean_absolute_error: 0.0285 - val_loss: 3.8558e-04 - val_mean_absolute_error: 0.0210\n",
      "Epoch 80/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.1761e-04 - mean_absolute_error: 0.0268\n",
      "Epoch 00080: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 6.8814e-04 - mean_absolute_error: 0.0256 - val_loss: 2.8064e-04 - val_mean_absolute_error: 0.0186\n",
      "Epoch 81/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.7749e-04 - mean_absolute_error: 0.0220\n",
      "Epoch 00081: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 8.4123e-04 - mean_absolute_error: 0.0267 - val_loss: 2.5808e-04 - val_mean_absolute_error: 0.0174\n",
      "Epoch 82/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.2145e-04 - mean_absolute_error: 0.0198\n",
      "Epoch 00082: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 5.6402e-04 - mean_absolute_error: 0.0236 - val_loss: 3.0509e-04 - val_mean_absolute_error: 0.0185\n",
      "Epoch 83/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.7340e-04 - mean_absolute_error: 0.0222\n",
      "Epoch 00083: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 7.6971e-04 - mean_absolute_error: 0.0264 - val_loss: 2.7068e-04 - val_mean_absolute_error: 0.0186\n",
      "Epoch 84/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.9496e-04 - mean_absolute_error: 0.0232\n",
      "Epoch 00084: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 6.5795e-04 - mean_absolute_error: 0.0263 - val_loss: 2.6252e-04 - val_mean_absolute_error: 0.0177\n",
      "Epoch 85/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.0291e-04 - mean_absolute_error: 0.0239\n",
      "Epoch 00085: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 6.0977e-04 - mean_absolute_error: 0.0241 - val_loss: 3.6742e-04 - val_mean_absolute_error: 0.0194\n",
      "Epoch 86/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.5632e-04 - mean_absolute_error: 0.0201\n",
      "Epoch 00086: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 6.4222e-04 - mean_absolute_error: 0.0242 - val_loss: 2.5164e-04 - val_mean_absolute_error: 0.0177\n",
      "Epoch 87/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.1363e-04 - mean_absolute_error: 0.0242\n",
      "Epoch 00087: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 5.8922e-04 - mean_absolute_error: 0.0238 - val_loss: 2.5748e-04 - val_mean_absolute_error: 0.0175\n",
      "Epoch 88/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.4331e-04 - mean_absolute_error: 0.0253\n",
      "Epoch 00088: val_loss improved from 0.00025 to 0.00025, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 5.9230e-04 - mean_absolute_error: 0.0238 - val_loss: 2.4826e-04 - val_mean_absolute_error: 0.0171\n",
      "Epoch 89/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2669e-04 - mean_absolute_error: 0.0207\n",
      "Epoch 00089: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 5.6981e-04 - mean_absolute_error: 0.0236 - val_loss: 2.7671e-04 - val_mean_absolute_error: 0.0181\n",
      "Epoch 90/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.3062e-04 - mean_absolute_error: 0.0249\n",
      "Epoch 00090: val_loss improved from 0.00025 to 0.00024, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 5.5169e-04 - mean_absolute_error: 0.0236 - val_loss: 2.3942e-04 - val_mean_absolute_error: 0.0170\n",
      "Epoch 91/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.0861e-04 - mean_absolute_error: 0.0274\n",
      "Epoch 00091: val_loss did not improve from 0.00024\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 5.6631e-04 - mean_absolute_error: 0.0246 - val_loss: 2.4959e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 92/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3051e-04 - mean_absolute_error: 0.0267\n",
      "Epoch 00092: val_loss did not improve from 0.00024\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 6.2591e-04 - mean_absolute_error: 0.0249 - val_loss: 3.8504e-04 - val_mean_absolute_error: 0.0194\n",
      "Epoch 93/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.0502e-04 - mean_absolute_error: 0.0273\n",
      "Epoch 00093: val_loss did not improve from 0.00024\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 5.9866e-04 - mean_absolute_error: 0.0252 - val_loss: 2.4728e-04 - val_mean_absolute_error: 0.0172\n",
      "Epoch 94/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.9463e-04 - mean_absolute_error: 0.0230\n",
      "Epoch 00094: val_loss improved from 0.00024 to 0.00023, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 6.6336e-04 - mean_absolute_error: 0.0251 - val_loss: 2.3398e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 95/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0013 - mean_absolute_error: 0.0311\n",
      "Epoch 00095: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 9.3804e-04 - mean_absolute_error: 0.0261 - val_loss: 3.3051e-04 - val_mean_absolute_error: 0.0185\n",
      "Epoch 96/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0012 - mean_absolute_error: 0.0317\n",
      "Epoch 00096: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 8.6273e-04 - mean_absolute_error: 0.0278 - val_loss: 2.9666e-04 - val_mean_absolute_error: 0.0176\n",
      "Epoch 97/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.5109e-04 - mean_absolute_error: 0.0261\n",
      "Epoch 00097: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 5.7741e-04 - mean_absolute_error: 0.0241 - val_loss: 2.9235e-04 - val_mean_absolute_error: 0.0188\n",
      "Epoch 98/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.7953e-04 - mean_absolute_error: 0.0231\n",
      "Epoch 00098: val_loss improved from 0.00023 to 0.00023, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 6.2142e-04 - mean_absolute_error: 0.0248 - val_loss: 2.3026e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 99/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.0406e-04 - mean_absolute_error: 0.0214\n",
      "Epoch 00099: val_loss improved from 0.00023 to 0.00023, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 5.8281e-04 - mean_absolute_error: 0.0248 - val_loss: 2.2796e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 100/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.4867e-04 - mean_absolute_error: 0.0268\n",
      "Epoch 00100: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 5.3748e-04 - mean_absolute_error: 0.0234 - val_loss: 3.6405e-04 - val_mean_absolute_error: 0.0186\n",
      "Epoch 101/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0016 - mean_absolute_error: 0.0333\n",
      "Epoch 00101: val_loss improved from 0.00023 to 0.00023, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 9.1777e-04 - mean_absolute_error: 0.0266 - val_loss: 2.2704e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 102/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.1362e-04 - mean_absolute_error: 0.0219\n",
      "Epoch 00102: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 4.3367e-04 - mean_absolute_error: 0.0220 - val_loss: 2.3709e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 103/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.5120e-04 - mean_absolute_error: 0.0224\n",
      "Epoch 00103: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 6.9755e-04 - mean_absolute_error: 0.0247 - val_loss: 2.5003e-04 - val_mean_absolute_error: 0.0174\n",
      "Epoch 104/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.7599e-04 - mean_absolute_error: 0.0250\n",
      "Epoch 00104: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 8.6382e-04 - mean_absolute_error: 0.0248 - val_loss: 2.9884e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 105/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1276e-04 - mean_absolute_error: 0.0193\n",
      "Epoch 00105: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 4.7036e-04 - mean_absolute_error: 0.0219 - val_loss: 2.8047e-04 - val_mean_absolute_error: 0.0173\n",
      "Epoch 106/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.9975e-04 - mean_absolute_error: 0.0239\n",
      "Epoch 00106: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 6.0174e-04 - mean_absolute_error: 0.0228 - val_loss: 4.6305e-04 - val_mean_absolute_error: 0.0224\n",
      "Epoch 107/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.0695e-04 - mean_absolute_error: 0.0279\n",
      "Epoch 00107: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 7.3462e-04 - mean_absolute_error: 0.0262 - val_loss: 2.8603e-04 - val_mean_absolute_error: 0.0195\n",
      "Epoch 108/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.8838e-04 - mean_absolute_error: 0.0281\n",
      "Epoch 00108: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 8.3351e-04 - mean_absolute_error: 0.0311 - val_loss: 3.9859e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 109/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.2464e-04 - mean_absolute_error: 0.0284\n",
      "Epoch 00109: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 7.2180e-04 - mean_absolute_error: 0.0272 - val_loss: 5.3487e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 110/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3713e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 00110: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 6.2305e-04 - mean_absolute_error: 0.0256 - val_loss: 2.5062e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 111/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.0525e-04 - mean_absolute_error: 0.0277\n",
      "Epoch 00111: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 6.2047e-04 - mean_absolute_error: 0.0236 - val_loss: 3.4471e-04 - val_mean_absolute_error: 0.0182\n",
      "Epoch 112/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.1466e-04 - mean_absolute_error: 0.0235\n",
      "Epoch 00112: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 6.2933e-04 - mean_absolute_error: 0.0237 - val_loss: 3.6330e-04 - val_mean_absolute_error: 0.0206\n",
      "Epoch 113/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.4051e-04 - mean_absolute_error: 0.0258\n",
      "Epoch 00113: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 6.6851e-04 - mean_absolute_error: 0.0247 - val_loss: 2.6888e-04 - val_mean_absolute_error: 0.0175\n",
      "Epoch 114/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.9950e-04 - mean_absolute_error: 0.0199\n",
      "Epoch 00114: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 6.2180e-04 - mean_absolute_error: 0.0245 - val_loss: 2.4659e-04 - val_mean_absolute_error: 0.0173\n",
      "Epoch 115/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.0053e-04 - mean_absolute_error: 0.0246\n",
      "Epoch 00115: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 5.6280e-04 - mean_absolute_error: 0.0248 - val_loss: 2.6240e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 116/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - mean_absolute_error: 0.0281\n",
      "Epoch 00116: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 7.2194e-04 - mean_absolute_error: 0.0242 - val_loss: 2.3143e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 117/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.2901e-04 - mean_absolute_error: 0.0224\n",
      "Epoch 00117: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 5.5524e-04 - mean_absolute_error: 0.0240 - val_loss: 2.5272e-04 - val_mean_absolute_error: 0.0174\n",
      "Epoch 118/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.1492e-04 - mean_absolute_error: 0.0198\n",
      "Epoch 00118: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 6.2668e-04 - mean_absolute_error: 0.0254 - val_loss: 3.9224e-04 - val_mean_absolute_error: 0.0211\n",
      "Epoch 119/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0012 - mean_absolute_error: 0.0321\n",
      "Epoch 00119: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 8.4191e-04 - mean_absolute_error: 0.0291 - val_loss: 3.1247e-04 - val_mean_absolute_error: 0.0198\n",
      "Epoch 120/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.4926e-04 - mean_absolute_error: 0.0265\n",
      "Epoch 00120: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 7.0039e-04 - mean_absolute_error: 0.0255 - val_loss: 3.3547e-04 - val_mean_absolute_error: 0.0180\n",
      "Epoch 121/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.1566e-04 - mean_absolute_error: 0.0228\n",
      "Epoch 00121: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 5.8974e-04 - mean_absolute_error: 0.0234 - val_loss: 2.2705e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 122/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.6384e-04 - mean_absolute_error: 0.0264\n",
      "Epoch 00122: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 6.7698e-04 - mean_absolute_error: 0.0260 - val_loss: 2.8252e-04 - val_mean_absolute_error: 0.0178\n",
      "Epoch 123/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - mean_absolute_error: 0.0306\n",
      "Epoch 00123: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 7.1534e-04 - mean_absolute_error: 0.0251 - val_loss: 2.8205e-04 - val_mean_absolute_error: 0.0184\n",
      "Epoch 124/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.8766e-04 - mean_absolute_error: 0.0215\n",
      "Epoch 00124: val_loss improved from 0.00023 to 0.00022, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 6.1271e-04 - mean_absolute_error: 0.0230 - val_loss: 2.1718e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 125/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.7607e-04 - mean_absolute_error: 0.0248\n",
      "Epoch 00125: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 7.5678e-04 - mean_absolute_error: 0.0271 - val_loss: 2.1963e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 126/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.5229e-04 - mean_absolute_error: 0.0207\n",
      "Epoch 00126: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 5.9087e-04 - mean_absolute_error: 0.0224 - val_loss: 2.9930e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 127/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.2172e-04 - mean_absolute_error: 0.0242\n",
      "Epoch 00127: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 5.3767e-04 - mean_absolute_error: 0.0227 - val_loss: 2.5887e-04 - val_mean_absolute_error: 0.0174\n",
      "Epoch 128/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.0526e-04 - mean_absolute_error: 0.0252\n",
      "Epoch 00128: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 6.2425e-04 - mean_absolute_error: 0.0249 - val_loss: 2.5794e-04 - val_mean_absolute_error: 0.0175\n",
      "Epoch 129/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6762e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 00129: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 6.3463e-04 - mean_absolute_error: 0.0242 - val_loss: 2.7788e-04 - val_mean_absolute_error: 0.0182\n",
      "Epoch 130/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.9817e-04 - mean_absolute_error: 0.0244\n",
      "Epoch 00130: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 5.3138e-04 - mean_absolute_error: 0.0242 - val_loss: 2.3549e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 131/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.7343e-04 - mean_absolute_error: 0.0215\n",
      "Epoch 00131: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.4235e-04 - mean_absolute_error: 0.0212 - val_loss: 2.5730e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 132/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0293e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 00132: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 5.4716e-04 - mean_absolute_error: 0.0222 - val_loss: 2.4199e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 133/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.4519e-04 - mean_absolute_error: 0.0227\n",
      "Epoch 00133: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 4.8927e-04 - mean_absolute_error: 0.0222 - val_loss: 2.3391e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 134/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.0115e-04 - mean_absolute_error: 0.0223\n",
      "Epoch 00134: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 4.4929e-04 - mean_absolute_error: 0.0215 - val_loss: 2.5186e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 135/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9252e-04 - mean_absolute_error: 0.0201\n",
      "Epoch 00135: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 5.5999e-04 - mean_absolute_error: 0.0227 - val_loss: 3.0590e-04 - val_mean_absolute_error: 0.0194\n",
      "Epoch 136/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9140e-04 - mean_absolute_error: 0.0222\n",
      "Epoch 00136: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 4.7886e-04 - mean_absolute_error: 0.0231 - val_loss: 2.4510e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 137/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.3352e-04 - mean_absolute_error: 0.0244\n",
      "Epoch 00137: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 5.4960e-04 - mean_absolute_error: 0.0238 - val_loss: 2.8681e-04 - val_mean_absolute_error: 0.0194\n",
      "Epoch 138/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.7175e-04 - mean_absolute_error: 0.0263\n",
      "Epoch 00138: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 5.1132e-04 - mean_absolute_error: 0.0240 - val_loss: 5.1536e-04 - val_mean_absolute_error: 0.0223\n",
      "Epoch 139/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.6412e-04 - mean_absolute_error: 0.0256\n",
      "Epoch 00139: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 7.0304e-04 - mean_absolute_error: 0.0248 - val_loss: 2.8568e-04 - val_mean_absolute_error: 0.0190\n",
      "Epoch 140/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.3621e-04 - mean_absolute_error: 0.0204\n",
      "Epoch 00140: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 5.6583e-04 - mean_absolute_error: 0.0249 - val_loss: 2.3629e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 141/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.3941e-04 - mean_absolute_error: 0.0242\n",
      "Epoch 00141: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 6.5656e-04 - mean_absolute_error: 0.0259 - val_loss: 4.1456e-04 - val_mean_absolute_error: 0.0222\n",
      "Epoch 142/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.5611e-04 - mean_absolute_error: 0.0239\n",
      "Epoch 00142: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 5.7398e-04 - mean_absolute_error: 0.0236 - val_loss: 4.6376e-04 - val_mean_absolute_error: 0.0204\n",
      "Epoch 143/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.8916e-04 - mean_absolute_error: 0.0262\n",
      "Epoch 00143: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 7.0961e-04 - mean_absolute_error: 0.0252 - val_loss: 3.4288e-04 - val_mean_absolute_error: 0.0197\n",
      "Epoch 144/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6992e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 00144: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 8.8876e-04 - mean_absolute_error: 0.0275 - val_loss: 2.4908e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 145/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.4157e-04 - mean_absolute_error: 0.0230\n",
      "Epoch 00145: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 8.5633e-04 - mean_absolute_error: 0.0280 - val_loss: 2.7853e-04 - val_mean_absolute_error: 0.0186\n",
      "Epoch 146/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.7402e-04 - mean_absolute_error: 0.0211\n",
      "Epoch 00146: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 4.9372e-04 - mean_absolute_error: 0.0228 - val_loss: 5.9408e-04 - val_mean_absolute_error: 0.0258\n",
      "Epoch 147/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.5743e-04 - mean_absolute_error: 0.0248\n",
      "Epoch 00147: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 5.4765e-04 - mean_absolute_error: 0.0226 - val_loss: 2.2965e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 148/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.1127e-04 - mean_absolute_error: 0.0221\n",
      "Epoch 00148: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 4.9166e-04 - mean_absolute_error: 0.0228 - val_loss: 2.5504e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 149/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.1859e-04 - mean_absolute_error: 0.0269\n",
      "Epoch 00149: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 5.3374e-04 - mean_absolute_error: 0.0219 - val_loss: 3.6466e-04 - val_mean_absolute_error: 0.0192\n",
      "Epoch 150/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.8610e-04 - mean_absolute_error: 0.0245\n",
      "Epoch 00150: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 5.1115e-04 - mean_absolute_error: 0.0227 - val_loss: 2.6464e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 151/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.4407e-04 - mean_absolute_error: 0.0210\n",
      "Epoch 00151: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 5.7876e-04 - mean_absolute_error: 0.0228 - val_loss: 2.5508e-04 - val_mean_absolute_error: 0.0177\n",
      "Epoch 152/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.0442e-04 - mean_absolute_error: 0.0216\n",
      "Epoch 00152: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 5.3967e-04 - mean_absolute_error: 0.0237 - val_loss: 2.5484e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 153/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.9551e-04 - mean_absolute_error: 0.0267\n",
      "Epoch 00153: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 6.1464e-04 - mean_absolute_error: 0.0239 - val_loss: 2.4715e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 154/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.2602e-04 - mean_absolute_error: 0.0190\n",
      "Epoch 00154: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 4.1228e-04 - mean_absolute_error: 0.0199 - val_loss: 2.4166e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 155/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.6887e-04 - mean_absolute_error: 0.0232\n",
      "Epoch 00155: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 5.9836e-04 - mean_absolute_error: 0.0232 - val_loss: 2.5182e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 156/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.1761e-04 - mean_absolute_error: 0.0244\n",
      "Epoch 00156: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 5.6722e-04 - mean_absolute_error: 0.0243 - val_loss: 4.2134e-04 - val_mean_absolute_error: 0.0221\n",
      "Epoch 157/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.0946e-04 - mean_absolute_error: 0.0253\n",
      "Epoch 00157: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 5.6517e-04 - mean_absolute_error: 0.0234 - val_loss: 3.2934e-04 - val_mean_absolute_error: 0.0195\n",
      "Epoch 158/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.9547e-04 - mean_absolute_error: 0.0242\n",
      "Epoch 00158: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 6.1719e-04 - mean_absolute_error: 0.0254 - val_loss: 2.8192e-04 - val_mean_absolute_error: 0.0171\n",
      "Epoch 159/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.0586e-04 - mean_absolute_error: 0.0221\n",
      "Epoch 00159: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 5.9159e-04 - mean_absolute_error: 0.0235 - val_loss: 2.5128e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 160/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4334e-04 - mean_absolute_error: 0.0208\n",
      "Epoch 00160: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 4.5330e-04 - mean_absolute_error: 0.0202 - val_loss: 2.4088e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 161/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.2832e-04 - mean_absolute_error: 0.0203\n",
      "Epoch 00161: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 5.0100e-04 - mean_absolute_error: 0.0219 - val_loss: 2.7725e-04 - val_mean_absolute_error: 0.0173\n",
      "Epoch 162/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3546e-04 - mean_absolute_error: 0.0258\n",
      "Epoch 00162: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 6.0239e-04 - mean_absolute_error: 0.0228 - val_loss: 2.4999e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 163/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.8835e-04 - mean_absolute_error: 0.0218\n",
      "Epoch 00163: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 5.7314e-04 - mean_absolute_error: 0.0229 - val_loss: 2.6073e-04 - val_mean_absolute_error: 0.0171\n",
      "Epoch 164/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7183e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 00164: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.7623e-04 - mean_absolute_error: 0.0196 - val_loss: 2.7220e-04 - val_mean_absolute_error: 0.0170\n",
      "Epoch 165/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.4085e-04 - mean_absolute_error: 0.0258\n",
      "Epoch 00165: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 5.6429e-04 - mean_absolute_error: 0.0227 - val_loss: 2.6268e-04 - val_mean_absolute_error: 0.0181\n",
      "Epoch 166/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.4155e-04 - mean_absolute_error: 0.0254\n",
      "Epoch 00166: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 5.5272e-04 - mean_absolute_error: 0.0229 - val_loss: 2.2846e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 167/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.8124e-04 - mean_absolute_error: 0.0193\n",
      "Epoch 00167: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 5.5670e-04 - mean_absolute_error: 0.0213 - val_loss: 3.2085e-04 - val_mean_absolute_error: 0.0186\n",
      "Epoch 168/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.1215e-04 - mean_absolute_error: 0.0234\n",
      "Epoch 00168: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 6.5881e-04 - mean_absolute_error: 0.0240 - val_loss: 4.0667e-04 - val_mean_absolute_error: 0.0188\n",
      "Epoch 169/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.1383e-04 - mean_absolute_error: 0.0257\n",
      "Epoch 00169: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 6.1419e-04 - mean_absolute_error: 0.0243 - val_loss: 3.5785e-04 - val_mean_absolute_error: 0.0211\n",
      "Epoch 170/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.1005e-04 - mean_absolute_error: 0.0221\n",
      "Epoch 00170: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 8.6496e-04 - mean_absolute_error: 0.0277 - val_loss: 2.2393e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 171/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.7194e-04 - mean_absolute_error: 0.0258\n",
      "Epoch 00171: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 5.9778e-04 - mean_absolute_error: 0.0246 - val_loss: 3.2195e-04 - val_mean_absolute_error: 0.0197\n",
      "Epoch 172/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.8947e-04 - mean_absolute_error: 0.0292\n",
      "Epoch 00172: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 4.6132e-04 - mean_absolute_error: 0.0218 - val_loss: 3.6346e-04 - val_mean_absolute_error: 0.0200\n",
      "Epoch 173/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.7226e-04 - mean_absolute_error: 0.0267\n",
      "Epoch 00173: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 6.1047e-04 - mean_absolute_error: 0.0245 - val_loss: 2.8214e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 174/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.7108e-04 - mean_absolute_error: 0.0210\n",
      "Epoch 00174: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 5.5138e-04 - mean_absolute_error: 0.0227 - val_loss: 2.3510e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 175/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.9940e-04 - mean_absolute_error: 0.0231\n",
      "Epoch 00175: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 4.9202e-04 - mean_absolute_error: 0.0232 - val_loss: 2.9500e-04 - val_mean_absolute_error: 0.0175\n",
      "Epoch 176/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.6067e-04 - mean_absolute_error: 0.0237\n",
      "Epoch 00176: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 5.6334e-04 - mean_absolute_error: 0.0226 - val_loss: 3.0782e-04 - val_mean_absolute_error: 0.0191\n",
      "Epoch 177/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.9522e-04 - mean_absolute_error: 0.0236\n",
      "Epoch 00177: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 4.6099e-04 - mean_absolute_error: 0.0229 - val_loss: 2.7584e-04 - val_mean_absolute_error: 0.0170\n",
      "Epoch 178/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.5202e-04 - mean_absolute_error: 0.0218\n",
      "Epoch 00178: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 5.3160e-04 - mean_absolute_error: 0.0224 - val_loss: 2.2422e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 179/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.2346e-04 - mean_absolute_error: 0.0231\n",
      "Epoch 00179: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 5.0030e-04 - mean_absolute_error: 0.0201 - val_loss: 2.2507e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 180/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.6412e-04 - mean_absolute_error: 0.0197\n",
      "Epoch 00180: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 4.9652e-04 - mean_absolute_error: 0.0212 - val_loss: 2.4931e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 181/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.3192e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 00181: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.8809e-04 - mean_absolute_error: 0.0220 - val_loss: 2.4760e-04 - val_mean_absolute_error: 0.0156\n",
      "Epoch 182/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0278e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 00182: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 5.4950e-04 - mean_absolute_error: 0.0222 - val_loss: 2.2302e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 183/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.0298e-04 - mean_absolute_error: 0.0273\n",
      "Epoch 00183: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 5.7018e-04 - mean_absolute_error: 0.0241 - val_loss: 2.3944e-04 - val_mean_absolute_error: 0.0175\n",
      "Epoch 184/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0546e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 00184: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 5.3286e-04 - mean_absolute_error: 0.0222 - val_loss: 2.8467e-04 - val_mean_absolute_error: 0.0187\n",
      "Epoch 185/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9319e-04 - mean_absolute_error: 0.0210\n",
      "Epoch 00185: val_loss improved from 0.00022 to 0.00020, saving model to results\\2021-11-27_^GSPC-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-1-step-1-layers-2-units-256.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 3.6329e-04 - mean_absolute_error: 0.0202 - val_loss: 2.0437e-04 - val_mean_absolute_error: 0.0153\n",
      "Epoch 186/700\n",
      "3/3 [==============================] - ETA: 0s - loss: 4.7617e-04 - mean_absolute_error: 0.0213\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 00186: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 4.7617e-04 - mean_absolute_error: 0.0213 - val_loss: 2.1213e-04 - val_mean_absolute_error: 0.0153\n",
      "Epoch 187/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.4046e-04 - mean_absolute_error: 0.0219\n",
      "Epoch 00187: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 4.5562e-04 - mean_absolute_error: 0.0201 - val_loss: 2.3848e-04 - val_mean_absolute_error: 0.0154\n",
      "Epoch 188/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.6642e-04 - mean_absolute_error: 0.0214\n",
      "Epoch 00188: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 4.8395e-04 - mean_absolute_error: 0.0212 - val_loss: 2.2904e-04 - val_mean_absolute_error: 0.0153\n",
      "Epoch 189/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.2050e-04 - mean_absolute_error: 0.0243\n",
      "Epoch 00189: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 4.7585e-04 - mean_absolute_error: 0.0209 - val_loss: 2.2099e-04 - val_mean_absolute_error: 0.0152\n",
      "Epoch 190/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.9771e-04 - mean_absolute_error: 0.0214\n",
      "Epoch 00190: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 5.5296e-04 - mean_absolute_error: 0.0218 - val_loss: 2.3329e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 191/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.5802e-04 - mean_absolute_error: 0.0240\n",
      "Epoch 00191: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 5.5939e-04 - mean_absolute_error: 0.0224 - val_loss: 2.8251e-04 - val_mean_absolute_error: 0.0184\n",
      "Epoch 192/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.5032e-04 - mean_absolute_error: 0.0204\n",
      "Epoch 00192: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 5.5484e-04 - mean_absolute_error: 0.0226 - val_loss: 2.6244e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 193/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.0740e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 00193: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 4.6354e-04 - mean_absolute_error: 0.0203 - val_loss: 2.1971e-04 - val_mean_absolute_error: 0.0173\n",
      "Epoch 194/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9744e-04 - mean_absolute_error: 0.0211\n",
      "Epoch 00194: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 4.2871e-04 - mean_absolute_error: 0.0223 - val_loss: 2.2987e-04 - val_mean_absolute_error: 0.0152\n",
      "Epoch 195/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.5639e-04 - mean_absolute_error: 0.0214\n",
      "Epoch 00195: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 5.3352e-04 - mean_absolute_error: 0.0219 - val_loss: 2.1037e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 196/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.0338e-04 - mean_absolute_error: 0.0191\n",
      "Epoch 00196: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 4.0085e-04 - mean_absolute_error: 0.0195 - val_loss: 2.4852e-04 - val_mean_absolute_error: 0.0154\n",
      "Epoch 197/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4423e-04 - mean_absolute_error: 0.0200\n",
      "Epoch 00197: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 5.1194e-04 - mean_absolute_error: 0.0219 - val_loss: 2.4669e-04 - val_mean_absolute_error: 0.0154\n",
      "Epoch 198/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.4386e-04 - mean_absolute_error: 0.0253\n",
      "Epoch 00198: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 5.5745e-04 - mean_absolute_error: 0.0227 - val_loss: 3.0215e-04 - val_mean_absolute_error: 0.0182\n",
      "Epoch 199/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.9673e-04 - mean_absolute_error: 0.0220\n",
      "Epoch 00199: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 4.4599e-04 - mean_absolute_error: 0.0207 - val_loss: 2.1531e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 200/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.0713e-04 - mean_absolute_error: 0.0224\n",
      "Epoch 00200: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 4.9859e-04 - mean_absolute_error: 0.0230 - val_loss: 2.7233e-04 - val_mean_absolute_error: 0.0175\n",
      "Epoch 201/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.5654e-04 - mean_absolute_error: 0.0243\n",
      "Epoch 00201: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 4.9850e-04 - mean_absolute_error: 0.0213 - val_loss: 3.0256e-04 - val_mean_absolute_error: 0.0193\n",
      "Epoch 202/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.6998e-04 - mean_absolute_error: 0.0260\n",
      "Epoch 00202: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 5.6017e-04 - mean_absolute_error: 0.0237 - val_loss: 2.3541e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 203/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.6863e-04 - mean_absolute_error: 0.0268\n",
      "Epoch 00203: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 4.4283e-04 - mean_absolute_error: 0.0206 - val_loss: 2.3471e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 204/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.6336e-04 - mean_absolute_error: 0.0249\n",
      "Epoch 00204: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 4.4508e-04 - mean_absolute_error: 0.0208 - val_loss: 3.0779e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 205/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.0500e-04 - mean_absolute_error: 0.0195\n",
      "Epoch 00205: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 5.2701e-04 - mean_absolute_error: 0.0223 - val_loss: 2.3350e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 206/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.8840e-04 - mean_absolute_error: 0.0235\n",
      "Epoch 00206: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 4.6883e-04 - mean_absolute_error: 0.0208 - val_loss: 2.3650e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 207/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.4128e-04 - mean_absolute_error: 0.0270\n",
      "Epoch 00207: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 7.4154e-04 - mean_absolute_error: 0.0256 - val_loss: 3.1112e-04 - val_mean_absolute_error: 0.0193\n",
      "Epoch 208/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.0340e-04 - mean_absolute_error: 0.0234\n",
      "Epoch 00208: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 4.5727e-04 - mean_absolute_error: 0.0216 - val_loss: 2.2127e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 209/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.8244e-04 - mean_absolute_error: 0.0209\n",
      "Epoch 00209: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.7350e-04 - mean_absolute_error: 0.0196 - val_loss: 2.6064e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 210/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.9368e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 00210: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.0684e-04 - mean_absolute_error: 0.0203 - val_loss: 2.1941e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 211/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.9513e-04 - mean_absolute_error: 0.0236\n",
      "Epoch 00211: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.3500e-04 - mean_absolute_error: 0.0206 - val_loss: 2.9914e-04 - val_mean_absolute_error: 0.0178\n",
      "Epoch 212/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.6750e-04 - mean_absolute_error: 0.0212\n",
      "Epoch 00212: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 4.9851e-04 - mean_absolute_error: 0.0216 - val_loss: 2.1163e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 213/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0790e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 00213: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 5.6661e-04 - mean_absolute_error: 0.0222 - val_loss: 2.4344e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 214/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.9646e-04 - mean_absolute_error: 0.0202\n",
      "Epoch 00214: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 5.1357e-04 - mean_absolute_error: 0.0211 - val_loss: 2.6577e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 215/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8735e-04 - mean_absolute_error: 0.0187\n",
      "Epoch 00215: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.9645e-04 - mean_absolute_error: 0.0197 - val_loss: 2.2456e-04 - val_mean_absolute_error: 0.0170\n",
      "Epoch 216/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.2923e-04 - mean_absolute_error: 0.0209\n",
      "Epoch 00216: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.9613e-04 - mean_absolute_error: 0.0212 - val_loss: 2.7782e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 217/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2224e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 00217: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 4.6085e-04 - mean_absolute_error: 0.0203 - val_loss: 2.9041e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 218/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.5065e-04 - mean_absolute_error: 0.0256\n",
      "Epoch 00218: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 5.5900e-04 - mean_absolute_error: 0.0226 - val_loss: 3.1038e-04 - val_mean_absolute_error: 0.0199\n",
      "Epoch 219/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.2186e-04 - mean_absolute_error: 0.0220\n",
      "Epoch 00219: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 4.6794e-04 - mean_absolute_error: 0.0228 - val_loss: 2.6610e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 220/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.3756e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 00220: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 6.5779e-04 - mean_absolute_error: 0.0246 - val_loss: 2.9874e-04 - val_mean_absolute_error: 0.0214\n",
      "Epoch 221/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.5642e-04 - mean_absolute_error: 0.0261\n",
      "Epoch 00221: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 6.1296e-04 - mean_absolute_error: 0.0232 - val_loss: 2.6340e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 222/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9870e-04 - mean_absolute_error: 0.0190\n",
      "Epoch 00222: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.9367e-04 - mean_absolute_error: 0.0194 - val_loss: 2.2466e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 223/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.7912e-04 - mean_absolute_error: 0.0236\n",
      "Epoch 00223: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 5.2256e-04 - mean_absolute_error: 0.0219 - val_loss: 2.2201e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 224/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.1087e-04 - mean_absolute_error: 0.0208\n",
      "Epoch 00224: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 4.3837e-04 - mean_absolute_error: 0.0202 - val_loss: 2.2080e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 225/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2528e-04 - mean_absolute_error: 0.0196\n",
      "Epoch 00225: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 4.3015e-04 - mean_absolute_error: 0.0213 - val_loss: 3.3839e-04 - val_mean_absolute_error: 0.0186\n",
      "Epoch 226/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.1035e-04 - mean_absolute_error: 0.0200\n",
      "Epoch 00226: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 5.0023e-04 - mean_absolute_error: 0.0208 - val_loss: 2.1314e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 227/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.4001e-04 - mean_absolute_error: 0.0223\n",
      "Epoch 00227: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 5.0905e-04 - mean_absolute_error: 0.0222 - val_loss: 2.1474e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 228/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.7929e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 00228: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 5.2726e-04 - mean_absolute_error: 0.0213 - val_loss: 2.9262e-04 - val_mean_absolute_error: 0.0178\n",
      "Epoch 229/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.4067e-04 - mean_absolute_error: 0.0219\n",
      "Epoch 00229: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 5.2369e-04 - mean_absolute_error: 0.0213 - val_loss: 2.3153e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 230/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4549e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 00230: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.2661e-04 - mean_absolute_error: 0.0193 - val_loss: 2.4528e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 231/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.1621e-04 - mean_absolute_error: 0.0208\n",
      "Epoch 00231: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 5.8503e-04 - mean_absolute_error: 0.0216 - val_loss: 2.3090e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 232/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4377e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 00232: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 4.1857e-04 - mean_absolute_error: 0.0192 - val_loss: 2.4324e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 233/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.1414e-04 - mean_absolute_error: 0.0210\n",
      "Epoch 00233: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 5.2062e-04 - mean_absolute_error: 0.0217 - val_loss: 2.8024e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 234/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.3167e-04 - mean_absolute_error: 0.0202\n",
      "Epoch 00234: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.4875e-04 - mean_absolute_error: 0.0212 - val_loss: 2.4711e-04 - val_mean_absolute_error: 0.0180\n",
      "Epoch 235/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1548e-04 - mean_absolute_error: 0.0204\n",
      "Epoch 00235: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.5258e-04 - mean_absolute_error: 0.0210 - val_loss: 2.7151e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 236/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1695e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 00236: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 5.6761e-04 - mean_absolute_error: 0.0205 - val_loss: 2.3592e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 237/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.5676e-04 - mean_absolute_error: 0.0223\n",
      "Epoch 00237: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 5.5882e-04 - mean_absolute_error: 0.0215 - val_loss: 2.3319e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 238/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.0517e-04 - mean_absolute_error: 0.0209\n",
      "Epoch 00238: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.7672e-04 - mean_absolute_error: 0.0201 - val_loss: 2.8244e-04 - val_mean_absolute_error: 0.0178\n",
      "Epoch 239/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.3403e-04 - mean_absolute_error: 0.0199\n",
      "Epoch 00239: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 3.5091e-04 - mean_absolute_error: 0.0186 - val_loss: 2.2784e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 240/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.6156e-04 - mean_absolute_error: 0.0206\n",
      "Epoch 00240: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 4.0214e-04 - mean_absolute_error: 0.0195 - val_loss: 2.5122e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 241/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.8140e-04 - mean_absolute_error: 0.0211\n",
      "Epoch 00241: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 5.0335e-04 - mean_absolute_error: 0.0216 - val_loss: 2.2161e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 242/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.4852e-04 - mean_absolute_error: 0.0215\n",
      "Epoch 00242: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 4.1601e-04 - mean_absolute_error: 0.0200 - val_loss: 2.2153e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 243/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.0323e-04 - mean_absolute_error: 0.0225\n",
      "Epoch 00243: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 4.9286e-04 - mean_absolute_error: 0.0221 - val_loss: 3.3961e-04 - val_mean_absolute_error: 0.0199\n",
      "Epoch 244/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.2164e-04 - mean_absolute_error: 0.0221\n",
      "Epoch 00244: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 4.1892e-04 - mean_absolute_error: 0.0204 - val_loss: 2.2041e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 245/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.3826e-04 - mean_absolute_error: 0.0216\n",
      "Epoch 00245: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 4.9801e-04 - mean_absolute_error: 0.0200 - val_loss: 2.5003e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 246/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5210e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 00246: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 4.1016e-04 - mean_absolute_error: 0.0187 - val_loss: 2.6531e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 247/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.2019e-04 - mean_absolute_error: 0.0239\n",
      "Epoch 00247: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 4.1458e-04 - mean_absolute_error: 0.0214 - val_loss: 2.2063e-04 - val_mean_absolute_error: 0.0156\n",
      "Epoch 248/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.1583e-04 - mean_absolute_error: 0.0206\n",
      "Epoch 00248: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 4.8604e-04 - mean_absolute_error: 0.0213 - val_loss: 3.3750e-04 - val_mean_absolute_error: 0.0194\n",
      "Epoch 249/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.9415e-04 - mean_absolute_error: 0.0195\n",
      "Epoch 00249: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 4.4809e-04 - mean_absolute_error: 0.0201 - val_loss: 3.1010e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 250/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.8144e-04 - mean_absolute_error: 0.0242\n",
      "Epoch 00250: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 5.5791e-04 - mean_absolute_error: 0.0227 - val_loss: 4.8325e-04 - val_mean_absolute_error: 0.0249\n",
      "Epoch 251/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.0813e-04 - mean_absolute_error: 0.0260\n",
      "Epoch 00251: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 5.4361e-04 - mean_absolute_error: 0.0255 - val_loss: 2.7506e-04 - val_mean_absolute_error: 0.0173\n",
      "Epoch 252/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.9204e-04 - mean_absolute_error: 0.0251\n",
      "Epoch 00252: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 6.1492e-04 - mean_absolute_error: 0.0263 - val_loss: 3.1344e-04 - val_mean_absolute_error: 0.0215\n",
      "Epoch 253/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.5509e-04 - mean_absolute_error: 0.0219\n",
      "Epoch 00253: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.9304e-04 - mean_absolute_error: 0.0230 - val_loss: 5.7158e-04 - val_mean_absolute_error: 0.0261\n",
      "Epoch 254/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.7208e-04 - mean_absolute_error: 0.0234\n",
      "Epoch 00254: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 5.1313e-04 - mean_absolute_error: 0.0226 - val_loss: 2.5926e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 255/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3435e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 00255: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 5.4712e-04 - mean_absolute_error: 0.0233 - val_loss: 2.4641e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 256/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.1944e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 00256: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 5.1017e-04 - mean_absolute_error: 0.0212 - val_loss: 3.0143e-04 - val_mean_absolute_error: 0.0179\n",
      "Epoch 257/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.5745e-04 - mean_absolute_error: 0.0192\n",
      "Epoch 00257: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.7808e-04 - mean_absolute_error: 0.0194 - val_loss: 2.6121e-04 - val_mean_absolute_error: 0.0176\n",
      "Epoch 258/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9366e-04 - mean_absolute_error: 0.0213\n",
      "Epoch 00258: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 4.0126e-04 - mean_absolute_error: 0.0206 - val_loss: 3.4708e-04 - val_mean_absolute_error: 0.0187\n",
      "Epoch 259/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.9496e-04 - mean_absolute_error: 0.0204\n",
      "Epoch 00259: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 5.5493e-04 - mean_absolute_error: 0.0221 - val_loss: 2.2970e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 260/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.9195e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 00260: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 3.8697e-04 - mean_absolute_error: 0.0201 - val_loss: 2.4045e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 261/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.1007e-04 - mean_absolute_error: 0.0226\n",
      "Epoch 00261: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 4.6784e-04 - mean_absolute_error: 0.0203 - val_loss: 3.4505e-04 - val_mean_absolute_error: 0.0195\n",
      "Epoch 262/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.6160e-04 - mean_absolute_error: 0.0204\n",
      "Epoch 00262: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 4.5006e-04 - mean_absolute_error: 0.0209 - val_loss: 2.2812e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 263/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.2145e-04 - mean_absolute_error: 0.0197\n",
      "Epoch 00263: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 4.6341e-04 - mean_absolute_error: 0.0202 - val_loss: 2.3876e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 264/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.8511e-04 - mean_absolute_error: 0.0202\n",
      "Epoch 00264: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 3.5574e-04 - mean_absolute_error: 0.0184 - val_loss: 2.9889e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 265/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.6408e-04 - mean_absolute_error: 0.0234\n",
      "Epoch 00265: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.1670e-04 - mean_absolute_error: 0.0200 - val_loss: 2.2693e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 266/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.2906e-04 - mean_absolute_error: 0.0208\n",
      "Epoch 00266: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 3.8259e-04 - mean_absolute_error: 0.0202 - val_loss: 2.5264e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 267/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2201e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 00267: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 3.5537e-04 - mean_absolute_error: 0.0177 - val_loss: 2.5135e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 268/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2483e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 00268: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 3.4423e-04 - mean_absolute_error: 0.0186 - val_loss: 2.4085e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 269/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.3246e-04 - mean_absolute_error: 0.0209\n",
      "Epoch 00269: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 5.3297e-04 - mean_absolute_error: 0.0218 - val_loss: 2.4599e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 270/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.8805e-04 - mean_absolute_error: 0.0214\n",
      "Epoch 00270: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 4.9080e-04 - mean_absolute_error: 0.0216 - val_loss: 4.3675e-04 - val_mean_absolute_error: 0.0223\n",
      "Epoch 271/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.9859e-04 - mean_absolute_error: 0.0238\n",
      "Epoch 00271: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 4.7693e-04 - mean_absolute_error: 0.0209 - val_loss: 2.5376e-04 - val_mean_absolute_error: 0.0181\n",
      "Epoch 272/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9213e-04 - mean_absolute_error: 0.0210\n",
      "Epoch 00272: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 5.4507e-04 - mean_absolute_error: 0.0234 - val_loss: 2.4881e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 273/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2188e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 00273: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 4.5879e-04 - mean_absolute_error: 0.0202 - val_loss: 2.5917e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 274/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7616e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 00274: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 4.4058e-04 - mean_absolute_error: 0.0207 - val_loss: 2.6857e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 275/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.7100e-04 - mean_absolute_error: 0.0196\n",
      "Epoch 00275: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.5574e-04 - mean_absolute_error: 0.0194 - val_loss: 3.2785e-04 - val_mean_absolute_error: 0.0200\n",
      "Epoch 276/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.9718e-04 - mean_absolute_error: 0.0251\n",
      "Epoch 00276: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.6435e-04 - mean_absolute_error: 0.0210 - val_loss: 2.2028e-04 - val_mean_absolute_error: 0.0156\n",
      "Epoch 277/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.4212e-04 - mean_absolute_error: 0.0222\n",
      "Epoch 00277: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 5.1916e-04 - mean_absolute_error: 0.0225 - val_loss: 2.2560e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 278/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9831e-04 - mean_absolute_error: 0.0204\n",
      "Epoch 00278: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 4.5950e-04 - mean_absolute_error: 0.0197 - val_loss: 4.5939e-04 - val_mean_absolute_error: 0.0219\n",
      "Epoch 279/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.5008e-04 - mean_absolute_error: 0.0238\n",
      "Epoch 00279: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 4.4083e-04 - mean_absolute_error: 0.0209 - val_loss: 2.4236e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 280/700\n",
      "3/3 [==============================] - ETA: 0s - loss: 4.6204e-04 - mean_absolute_error: 0.0205\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 00280: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 4.6204e-04 - mean_absolute_error: 0.0205 - val_loss: 2.4102e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 281/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9541e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 00281: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 4.3099e-04 - mean_absolute_error: 0.0194 - val_loss: 2.7249e-04 - val_mean_absolute_error: 0.0172\n",
      "Epoch 282/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.9325e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 00282: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.7157e-04 - mean_absolute_error: 0.0197 - val_loss: 2.2444e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 283/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2905e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 00283: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 3.5960e-04 - mean_absolute_error: 0.0182 - val_loss: 2.1686e-04 - val_mean_absolute_error: 0.0156\n",
      "Epoch 284/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.6988e-04 - mean_absolute_error: 0.0246\n",
      "Epoch 00284: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 5.1218e-04 - mean_absolute_error: 0.0219 - val_loss: 2.7104e-04 - val_mean_absolute_error: 0.0173\n",
      "Epoch 285/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8815e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 00285: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 4.4323e-04 - mean_absolute_error: 0.0202 - val_loss: 2.1607e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 286/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.9746e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 00286: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 4.7656e-04 - mean_absolute_error: 0.0213 - val_loss: 2.5569e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 287/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.5119e-04 - mean_absolute_error: 0.0209\n",
      "Epoch 00287: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 4.3926e-04 - mean_absolute_error: 0.0202 - val_loss: 2.5176e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 288/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2165e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 00288: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 3.8353e-04 - mean_absolute_error: 0.0191 - val_loss: 2.0722e-04 - val_mean_absolute_error: 0.0153\n",
      "Epoch 289/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.5013e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 00289: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.4946e-04 - mean_absolute_error: 0.0188 - val_loss: 2.2200e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 290/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.3856e-04 - mean_absolute_error: 0.0227\n",
      "Epoch 00290: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 4.7482e-04 - mean_absolute_error: 0.0204 - val_loss: 2.2174e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 291/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9150e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 00291: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 4.2296e-04 - mean_absolute_error: 0.0190 - val_loss: 2.1032e-04 - val_mean_absolute_error: 0.0154\n",
      "Epoch 292/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1176e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 00292: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 3.3686e-04 - mean_absolute_error: 0.0184 - val_loss: 2.1526e-04 - val_mean_absolute_error: 0.0154\n",
      "Epoch 293/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.9011e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 00293: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.8837e-04 - mean_absolute_error: 0.0202 - val_loss: 2.7449e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 294/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.3364e-04 - mean_absolute_error: 0.0202\n",
      "Epoch 00294: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 5.9340e-04 - mean_absolute_error: 0.0213 - val_loss: 2.3521e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 295/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.0354e-04 - mean_absolute_error: 0.0223\n",
      "Epoch 00295: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.4532e-04 - mean_absolute_error: 0.0208 - val_loss: 3.1936e-04 - val_mean_absolute_error: 0.0189\n",
      "Epoch 296/700\n",
      "3/3 [==============================] - ETA: 0s - loss: 4.1510e-04 - mean_absolute_error: 0.0206\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 00296: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 4.1510e-04 - mean_absolute_error: 0.0206 - val_loss: 2.4793e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 297/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.5757e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 00297: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 4.5181e-04 - mean_absolute_error: 0.0208 - val_loss: 2.4777e-04 - val_mean_absolute_error: 0.0173\n",
      "Epoch 298/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.1292e-04 - mean_absolute_error: 0.0233\n",
      "Epoch 00298: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 3.3544e-04 - mean_absolute_error: 0.0189 - val_loss: 3.7876e-04 - val_mean_absolute_error: 0.0201\n",
      "Epoch 299/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.3816e-04 - mean_absolute_error: 0.0199\n",
      "Epoch 00299: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 3.7078e-04 - mean_absolute_error: 0.0194 - val_loss: 2.1891e-04 - val_mean_absolute_error: 0.0156\n",
      "Epoch 300/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.9803e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 00300: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 5.4422e-04 - mean_absolute_error: 0.0210 - val_loss: 2.1562e-04 - val_mean_absolute_error: 0.0154\n",
      "Epoch 301/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.3935e-04 - mean_absolute_error: 0.0206\n",
      "Epoch 00301: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.9656e-04 - mean_absolute_error: 0.0196 - val_loss: 2.3839e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 302/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.8502e-04 - mean_absolute_error: 0.0210\n",
      "Epoch 00302: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 4.6227e-04 - mean_absolute_error: 0.0203 - val_loss: 2.5383e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 303/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.3078e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 00303: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 3.6775e-04 - mean_absolute_error: 0.0190 - val_loss: 2.4592e-04 - val_mean_absolute_error: 0.0170\n",
      "Epoch 304/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0115e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 00304: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.6415e-04 - mean_absolute_error: 0.0198 - val_loss: 2.3531e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 305/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.3447e-04 - mean_absolute_error: 0.0212\n",
      "Epoch 00305: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 5.0918e-04 - mean_absolute_error: 0.0213 - val_loss: 2.9650e-04 - val_mean_absolute_error: 0.0186\n",
      "Epoch 306/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.4710e-04 - mean_absolute_error: 0.0241\n",
      "Epoch 00306: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 4.6712e-04 - mean_absolute_error: 0.0207 - val_loss: 3.2198e-04 - val_mean_absolute_error: 0.0193\n",
      "Epoch 307/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.5510e-04 - mean_absolute_error: 0.0217\n",
      "Epoch 00307: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 4.1698e-04 - mean_absolute_error: 0.0215 - val_loss: 2.1738e-04 - val_mean_absolute_error: 0.0152\n",
      "Epoch 308/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7940e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 00308: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 4.8252e-04 - mean_absolute_error: 0.0224 - val_loss: 2.1970e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 309/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.7570e-04 - mean_absolute_error: 0.0193\n",
      "Epoch 00309: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 4.0481e-04 - mean_absolute_error: 0.0187 - val_loss: 3.4168e-04 - val_mean_absolute_error: 0.0171\n",
      "Epoch 310/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8583e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 00310: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 3.9015e-04 - mean_absolute_error: 0.0192 - val_loss: 2.3960e-04 - val_mean_absolute_error: 0.0172\n",
      "Epoch 311/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7779e-04 - mean_absolute_error: 0.0190\n",
      "Epoch 00311: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 3.8903e-04 - mean_absolute_error: 0.0208 - val_loss: 2.6101e-04 - val_mean_absolute_error: 0.0180\n",
      "Epoch 312/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.8809e-04 - mean_absolute_error: 0.0196\n",
      "Epoch 00312: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 4.7884e-04 - mean_absolute_error: 0.0210 - val_loss: 2.5487e-04 - val_mean_absolute_error: 0.0181\n",
      "Epoch 313/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4308e-04 - mean_absolute_error: 0.0204\n",
      "Epoch 00313: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 3.7678e-04 - mean_absolute_error: 0.0200 - val_loss: 2.1755e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 314/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.7916e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 00314: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 4.2026e-04 - mean_absolute_error: 0.0199 - val_loss: 3.5073e-04 - val_mean_absolute_error: 0.0176\n",
      "Epoch 315/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.8713e-04 - mean_absolute_error: 0.0219\n",
      "Epoch 00315: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.4919e-04 - mean_absolute_error: 0.0195 - val_loss: 2.3372e-04 - val_mean_absolute_error: 0.0175\n",
      "Epoch 316/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.8440e-04 - mean_absolute_error: 0.0217\n",
      "Epoch 00316: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 4.4167e-04 - mean_absolute_error: 0.0202 - val_loss: 2.9675e-04 - val_mean_absolute_error: 0.0182\n",
      "Epoch 317/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0506e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 00317: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.8311e-04 - mean_absolute_error: 0.0197 - val_loss: 2.8032e-04 - val_mean_absolute_error: 0.0177\n",
      "Epoch 318/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4275e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 00318: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 2.9554e-04 - mean_absolute_error: 0.0174 - val_loss: 2.2443e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 319/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4235e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 00319: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 4.1672e-04 - mean_absolute_error: 0.0207 - val_loss: 2.2654e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 320/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.6035e-04 - mean_absolute_error: 0.0212\n",
      "Epoch 00320: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 3.3373e-04 - mean_absolute_error: 0.0178 - val_loss: 2.3101e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 321/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8800e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 00321: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 4.2219e-04 - mean_absolute_error: 0.0201 - val_loss: 2.2349e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 322/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.0596e-04 - mean_absolute_error: 0.0220\n",
      "Epoch 00322: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 5.4038e-04 - mean_absolute_error: 0.0216 - val_loss: 2.1735e-04 - val_mean_absolute_error: 0.0156\n",
      "Epoch 323/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8146e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 00323: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.7676e-04 - mean_absolute_error: 0.0192 - val_loss: 2.2565e-04 - val_mean_absolute_error: 0.0156\n",
      "Epoch 324/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.2166e-04 - mean_absolute_error: 0.0213\n",
      "Epoch 00324: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.7555e-04 - mean_absolute_error: 0.0194 - val_loss: 2.3193e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 325/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1627e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 00325: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 4.0780e-04 - mean_absolute_error: 0.0189 - val_loss: 2.3565e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 326/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.2519e-04 - mean_absolute_error: 0.0196\n",
      "Epoch 00326: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.9158e-04 - mean_absolute_error: 0.0186 - val_loss: 2.2825e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 327/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.0469e-04 - mean_absolute_error: 0.0205\n",
      "Epoch 00327: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 3.6687e-04 - mean_absolute_error: 0.0182 - val_loss: 2.2455e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 328/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7981e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 00328: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.7428e-04 - mean_absolute_error: 0.0199 - val_loss: 2.6417e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 329/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.7981e-04 - mean_absolute_error: 0.0195\n",
      "Epoch 00329: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 3.7543e-04 - mean_absolute_error: 0.0190 - val_loss: 2.3249e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 330/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4616e-04 - mean_absolute_error: 0.0187\n",
      "Epoch 00330: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.3477e-04 - mean_absolute_error: 0.0198 - val_loss: 2.1804e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 331/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4630e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 00331: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.5857e-04 - mean_absolute_error: 0.0183 - val_loss: 3.1128e-04 - val_mean_absolute_error: 0.0193\n",
      "Epoch 332/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.1558e-04 - mean_absolute_error: 0.0216\n",
      "Epoch 00332: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.8465e-04 - mean_absolute_error: 0.0199 - val_loss: 2.5107e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 333/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.3070e-04 - mean_absolute_error: 0.0205\n",
      "Epoch 00333: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.8767e-04 - mean_absolute_error: 0.0192 - val_loss: 2.2377e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 334/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.0567e-04 - mean_absolute_error: 0.0226\n",
      "Epoch 00334: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 4.3890e-04 - mean_absolute_error: 0.0199 - val_loss: 3.0669e-04 - val_mean_absolute_error: 0.0174\n",
      "Epoch 335/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.9068e-04 - mean_absolute_error: 0.0221\n",
      "Epoch 00335: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 3.8149e-04 - mean_absolute_error: 0.0193 - val_loss: 2.1938e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 336/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2071e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 00336: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 4.5642e-04 - mean_absolute_error: 0.0198 - val_loss: 2.3251e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 337/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6425e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 00337: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.4369e-04 - mean_absolute_error: 0.0179 - val_loss: 2.6034e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 338/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6757e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 00338: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 4.1107e-04 - mean_absolute_error: 0.0200 - val_loss: 2.1683e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 339/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.2878e-04 - mean_absolute_error: 0.0195\n",
      "Epoch 00339: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 4.1968e-04 - mean_absolute_error: 0.0206 - val_loss: 2.3549e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 340/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4270e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 00340: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.4524e-04 - mean_absolute_error: 0.0188 - val_loss: 2.5183e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 341/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.8505e-04 - mean_absolute_error: 0.0210\n",
      "Epoch 00341: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 3.5428e-04 - mean_absolute_error: 0.0188 - val_loss: 2.6242e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 342/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4723e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 00342: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 3.8281e-04 - mean_absolute_error: 0.0189 - val_loss: 3.0108e-04 - val_mean_absolute_error: 0.0174\n",
      "Epoch 343/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.0512e-04 - mean_absolute_error: 0.0216\n",
      "Epoch 00343: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 4.9397e-04 - mean_absolute_error: 0.0202 - val_loss: 2.7878e-04 - val_mean_absolute_error: 0.0175\n",
      "Epoch 344/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.9428e-04 - mean_absolute_error: 0.0221\n",
      "Epoch 00344: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 4.3394e-04 - mean_absolute_error: 0.0215 - val_loss: 2.3196e-04 - val_mean_absolute_error: 0.0171\n",
      "Epoch 345/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.2249e-04 - mean_absolute_error: 0.0207\n",
      "Epoch 00345: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.4533e-04 - mean_absolute_error: 0.0195 - val_loss: 3.8752e-04 - val_mean_absolute_error: 0.0222\n",
      "Epoch 346/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.1337e-04 - mean_absolute_error: 0.0209\n",
      "Epoch 00346: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 3.5475e-04 - mean_absolute_error: 0.0191 - val_loss: 2.8802e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 347/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.3176e-04 - mean_absolute_error: 0.0209\n",
      "Epoch 00347: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 3.8558e-04 - mean_absolute_error: 0.0205 - val_loss: 2.3795e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 348/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.5062e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 00348: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 3.8245e-04 - mean_absolute_error: 0.0183 - val_loss: 2.2339e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 349/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.5899e-04 - mean_absolute_error: 0.0191\n",
      "Epoch 00349: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 3.5749e-04 - mean_absolute_error: 0.0190 - val_loss: 2.1709e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 350/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.4975e-04 - mean_absolute_error: 0.0198\n",
      "Epoch 00350: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 4.9724e-04 - mean_absolute_error: 0.0209 - val_loss: 2.4784e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 351/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.5650e-04 - mean_absolute_error: 0.0221\n",
      "Epoch 00351: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.8248e-04 - mean_absolute_error: 0.0198 - val_loss: 2.1911e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 352/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.2462e-04 - mean_absolute_error: 0.0224\n",
      "Epoch 00352: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 4.6616e-04 - mean_absolute_error: 0.0205 - val_loss: 2.3255e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 353/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.1760e-04 - mean_absolute_error: 0.0197\n",
      "Epoch 00353: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 4.1413e-04 - mean_absolute_error: 0.0198 - val_loss: 3.3542e-04 - val_mean_absolute_error: 0.0205\n",
      "Epoch 354/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.1106e-04 - mean_absolute_error: 0.0228\n",
      "Epoch 00354: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.7242e-04 - mean_absolute_error: 0.0193 - val_loss: 2.4177e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 355/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.0320e-04 - mean_absolute_error: 0.0243\n",
      "Epoch 00355: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.8814e-04 - mean_absolute_error: 0.0199 - val_loss: 2.7186e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 356/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.0670e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 00356: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 4.0503e-04 - mean_absolute_error: 0.0177 - val_loss: 2.2712e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 357/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.3808e-04 - mean_absolute_error: 0.0217\n",
      "Epoch 00357: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 4.4337e-04 - mean_absolute_error: 0.0195 - val_loss: 2.1538e-04 - val_mean_absolute_error: 0.0156\n",
      "Epoch 358/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.5515e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 00358: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 3.5466e-04 - mean_absolute_error: 0.0187 - val_loss: 2.7239e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 359/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.5166e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 00359: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.5640e-04 - mean_absolute_error: 0.0202 - val_loss: 2.0892e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 360/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.8839e-04 - mean_absolute_error: 0.0196\n",
      "Epoch 00360: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 3.6141e-04 - mean_absolute_error: 0.0198 - val_loss: 3.3769e-04 - val_mean_absolute_error: 0.0183\n",
      "Epoch 361/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0127e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 00361: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 4.7082e-04 - mean_absolute_error: 0.0208 - val_loss: 3.0189e-04 - val_mean_absolute_error: 0.0200\n",
      "Epoch 362/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.1064e-04 - mean_absolute_error: 0.0256\n",
      "Epoch 00362: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 4.9362e-04 - mean_absolute_error: 0.0236 - val_loss: 2.8092e-04 - val_mean_absolute_error: 0.0182\n",
      "Epoch 363/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.7682e-04 - mean_absolute_error: 0.0230\n",
      "Epoch 00363: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 3.6960e-04 - mean_absolute_error: 0.0202 - val_loss: 2.7087e-04 - val_mean_absolute_error: 0.0181\n",
      "Epoch 364/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.3901e-04 - mean_absolute_error: 0.0195\n",
      "Epoch 00364: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 5.6580e-04 - mean_absolute_error: 0.0215 - val_loss: 2.4527e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 365/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.6679e-04 - mean_absolute_error: 0.0190\n",
      "Epoch 00365: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 4.1802e-04 - mean_absolute_error: 0.0197 - val_loss: 3.8998e-04 - val_mean_absolute_error: 0.0189\n",
      "Epoch 366/700\n",
      "3/3 [==============================] - ETA: 0s - loss: 3.6912e-04 - mean_absolute_error: 0.0200\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 00366: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 3.6912e-04 - mean_absolute_error: 0.0200 - val_loss: 2.3544e-04 - val_mean_absolute_error: 0.0170\n",
      "Epoch 367/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.8305e-04 - mean_absolute_error: 0.0227\n",
      "Epoch 00367: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 4.0302e-04 - mean_absolute_error: 0.0195 - val_loss: 3.1160e-04 - val_mean_absolute_error: 0.0195\n",
      "Epoch 368/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2352e-04 - mean_absolute_error: 0.0192\n",
      "Epoch 00368: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 4.6235e-04 - mean_absolute_error: 0.0202 - val_loss: 2.1216e-04 - val_mean_absolute_error: 0.0153\n",
      "Epoch 369/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.5209e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 00369: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 4.6053e-04 - mean_absolute_error: 0.0204 - val_loss: 2.4243e-04 - val_mean_absolute_error: 0.0156\n",
      "Epoch 370/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.4293e-04 - mean_absolute_error: 0.0215\n",
      "Epoch 00370: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 5.2235e-04 - mean_absolute_error: 0.0211 - val_loss: 2.0715e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 371/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1189e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 00371: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 3.0514e-04 - mean_absolute_error: 0.0181 - val_loss: 2.2696e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 372/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1695e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 00372: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.7314e-04 - mean_absolute_error: 0.0184 - val_loss: 2.4121e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 373/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.4503e-04 - mean_absolute_error: 0.0207\n",
      "Epoch 00373: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.7916e-04 - mean_absolute_error: 0.0184 - val_loss: 2.3819e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 374/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2628e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 00374: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 5.3857e-04 - mean_absolute_error: 0.0210 - val_loss: 3.5982e-04 - val_mean_absolute_error: 0.0210\n",
      "Epoch 375/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.0360e-04 - mean_absolute_error: 0.0199\n",
      "Epoch 00375: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 4.9110e-04 - mean_absolute_error: 0.0219 - val_loss: 2.2119e-04 - val_mean_absolute_error: 0.0154\n",
      "Epoch 376/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.9267e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 00376: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 5.0063e-04 - mean_absolute_error: 0.0205 - val_loss: 2.2786e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 377/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9524e-04 - mean_absolute_error: 0.0202\n",
      "Epoch 00377: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 3.1605e-04 - mean_absolute_error: 0.0179 - val_loss: 5.1235e-04 - val_mean_absolute_error: 0.0221\n",
      "Epoch 378/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.4409e-04 - mean_absolute_error: 0.0229\n",
      "Epoch 00378: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 4.3736e-04 - mean_absolute_error: 0.0195 - val_loss: 2.6477e-04 - val_mean_absolute_error: 0.0181\n",
      "Epoch 379/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.0294e-04 - mean_absolute_error: 0.0235\n",
      "Epoch 00379: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 5.7575e-04 - mean_absolute_error: 0.0239 - val_loss: 2.5361e-04 - val_mean_absolute_error: 0.0174\n",
      "Epoch 380/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2185e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 00380: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 3.6696e-04 - mean_absolute_error: 0.0204 - val_loss: 3.1777e-04 - val_mean_absolute_error: 0.0199\n",
      "Epoch 381/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4345e-04 - mean_absolute_error: 0.0195\n",
      "Epoch 00381: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 3.2317e-04 - mean_absolute_error: 0.0181 - val_loss: 2.3466e-04 - val_mean_absolute_error: 0.0171\n",
      "Epoch 382/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4050e-04 - mean_absolute_error: 0.0198\n",
      "Epoch 00382: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.5537e-04 - mean_absolute_error: 0.0204 - val_loss: 2.5706e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 383/700\n",
      "3/3 [==============================] - ETA: 0s - loss: 3.3275e-04 - mean_absolute_error: 0.0180\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 00383: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 3.3275e-04 - mean_absolute_error: 0.0180 - val_loss: 2.7445e-04 - val_mean_absolute_error: 0.0171\n",
      "Epoch 384/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6605e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 00384: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.7395e-04 - mean_absolute_error: 0.0197 - val_loss: 2.3990e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 385/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.1694e-04 - mean_absolute_error: 0.0200\n",
      "Epoch 00385: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.5793e-04 - mean_absolute_error: 0.0193 - val_loss: 2.9282e-04 - val_mean_absolute_error: 0.0183\n",
      "Epoch 386/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1278e-04 - mean_absolute_error: 0.0187\n",
      "Epoch 00386: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 3.8146e-04 - mean_absolute_error: 0.0192 - val_loss: 2.2765e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 387/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.6450e-04 - mean_absolute_error: 0.0198\n",
      "Epoch 00387: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 3.9410e-04 - mean_absolute_error: 0.0186 - val_loss: 4.4919e-04 - val_mean_absolute_error: 0.0197\n",
      "Epoch 388/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.6647e-04 - mean_absolute_error: 0.0192\n",
      "Epoch 00388: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 5.0953e-04 - mean_absolute_error: 0.0217 - val_loss: 3.1904e-04 - val_mean_absolute_error: 0.0203\n",
      "Epoch 389/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.8218e-04 - mean_absolute_error: 0.0221\n",
      "Epoch 00389: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 5.2939e-04 - mean_absolute_error: 0.0239 - val_loss: 4.4669e-04 - val_mean_absolute_error: 0.0248\n",
      "Epoch 390/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.4824e-04 - mean_absolute_error: 0.0240\n",
      "Epoch 00390: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 5.8834e-04 - mean_absolute_error: 0.0255 - val_loss: 2.3826e-04 - val_mean_absolute_error: 0.0174\n",
      "Epoch 391/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.9810e-04 - mean_absolute_error: 0.0234\n",
      "Epoch 00391: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 4.3291e-04 - mean_absolute_error: 0.0200 - val_loss: 2.3110e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 392/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.9287e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 00392: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 4.1230e-04 - mean_absolute_error: 0.0197 - val_loss: 2.9501e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 393/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8078e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 00393: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 3.8399e-04 - mean_absolute_error: 0.0189 - val_loss: 2.2621e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 394/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.2680e-04 - mean_absolute_error: 0.0207\n",
      "Epoch 00394: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 4.6835e-04 - mean_absolute_error: 0.0212 - val_loss: 3.1721e-04 - val_mean_absolute_error: 0.0197\n",
      "Epoch 395/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.5761e-04 - mean_absolute_error: 0.0199\n",
      "Epoch 00395: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 3.9657e-04 - mean_absolute_error: 0.0217 - val_loss: 2.6376e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 396/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1504e-04 - mean_absolute_error: 0.0187\n",
      "Epoch 00396: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 4.3336e-04 - mean_absolute_error: 0.0214 - val_loss: 2.7527e-04 - val_mean_absolute_error: 0.0194\n",
      "Epoch 397/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0703e-04 - mean_absolute_error: 0.0198\n",
      "Epoch 00397: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 4.2445e-04 - mean_absolute_error: 0.0192 - val_loss: 4.2634e-04 - val_mean_absolute_error: 0.0209\n",
      "Epoch 398/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.3555e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 00398: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 4.2309e-04 - mean_absolute_error: 0.0202 - val_loss: 2.2371e-04 - val_mean_absolute_error: 0.0156\n",
      "Epoch 399/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.8652e-04 - mean_absolute_error: 0.0230\n",
      "Epoch 00399: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 6.3778e-04 - mean_absolute_error: 0.0246 - val_loss: 2.3410e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 400/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.7879e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 00400: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 3.5377e-04 - mean_absolute_error: 0.0178 - val_loss: 3.9798e-04 - val_mean_absolute_error: 0.0225\n",
      "Epoch 401/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.0390e-04 - mean_absolute_error: 0.0264\n",
      "Epoch 00401: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 4.9287e-04 - mean_absolute_error: 0.0228 - val_loss: 2.5745e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 402/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0348e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 00402: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.5962e-04 - mean_absolute_error: 0.0205 - val_loss: 3.6025e-04 - val_mean_absolute_error: 0.0194\n",
      "Epoch 403/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9825e-04 - mean_absolute_error: 0.0202\n",
      "Epoch 00403: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 5.1213e-04 - mean_absolute_error: 0.0198 - val_loss: 2.3401e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 404/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.8705e-04 - mean_absolute_error: 0.0209\n",
      "Epoch 00404: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.4244e-04 - mean_absolute_error: 0.0193 - val_loss: 2.2817e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 405/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.7003e-04 - mean_absolute_error: 0.0187\n",
      "Epoch 00405: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 3.0853e-04 - mean_absolute_error: 0.0176 - val_loss: 2.5723e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 406/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.0196e-04 - mean_absolute_error: 0.0191\n",
      "Epoch 00406: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 4.2030e-04 - mean_absolute_error: 0.0189 - val_loss: 2.3345e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 407/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.3052e-04 - mean_absolute_error: 0.0220\n",
      "Epoch 00407: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 4.5960e-04 - mean_absolute_error: 0.0196 - val_loss: 2.4610e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 408/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.7287e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 00408: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 5.0678e-04 - mean_absolute_error: 0.0213 - val_loss: 2.2314e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 409/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.5924e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 00409: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.7699e-04 - mean_absolute_error: 0.0187 - val_loss: 2.4451e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 410/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.6577e-04 - mean_absolute_error: 0.0195\n",
      "Epoch 00410: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 3.3131e-04 - mean_absolute_error: 0.0181 - val_loss: 2.4079e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 411/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3879e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 00411: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 4.7080e-04 - mean_absolute_error: 0.0189 - val_loss: 2.2163e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 412/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.0241e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 00412: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.5389e-04 - mean_absolute_error: 0.0183 - val_loss: 3.3008e-04 - val_mean_absolute_error: 0.0171\n",
      "Epoch 413/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2124e-04 - mean_absolute_error: 0.0192\n",
      "Epoch 00413: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 5.0358e-04 - mean_absolute_error: 0.0210 - val_loss: 2.1626e-04 - val_mean_absolute_error: 0.0154\n",
      "Epoch 414/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0720e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 00414: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 4.3951e-04 - mean_absolute_error: 0.0198 - val_loss: 2.2131e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 415/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.1378e-04 - mean_absolute_error: 0.0146\n",
      "Epoch 00415: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 3.6346e-04 - mean_absolute_error: 0.0184 - val_loss: 2.9109e-04 - val_mean_absolute_error: 0.0182\n",
      "Epoch 416/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3542e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 00416: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 2.9147e-04 - mean_absolute_error: 0.0176 - val_loss: 2.1984e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 417/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7464e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 00417: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 4.7836e-04 - mean_absolute_error: 0.0198 - val_loss: 2.2701e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 418/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.4395e-04 - mean_absolute_error: 0.0208\n",
      "Epoch 00418: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 3.2852e-04 - mean_absolute_error: 0.0172 - val_loss: 2.3604e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 419/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.3001e-04 - mean_absolute_error: 0.0191\n",
      "Epoch 00419: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 3.4165e-04 - mean_absolute_error: 0.0177 - val_loss: 2.8849e-04 - val_mean_absolute_error: 0.0186\n",
      "Epoch 420/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2147e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 00420: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 3.9919e-04 - mean_absolute_error: 0.0193 - val_loss: 2.3737e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 421/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.1546e-04 - mean_absolute_error: 0.0241\n",
      "Epoch 00421: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 4.2501e-04 - mean_absolute_error: 0.0201 - val_loss: 2.3805e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 422/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.0066e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 00422: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 4.7913e-04 - mean_absolute_error: 0.0196 - val_loss: 2.3442e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 423/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.6359e-04 - mean_absolute_error: 0.0194\n",
      "Epoch 00423: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 3.7805e-04 - mean_absolute_error: 0.0177 - val_loss: 2.3918e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 424/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.9263e-04 - mean_absolute_error: 0.0202\n",
      "Epoch 00424: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 4.0888e-04 - mean_absolute_error: 0.0186 - val_loss: 2.4106e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 425/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4395e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 00425: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 3.5226e-04 - mean_absolute_error: 0.0175 - val_loss: 2.6830e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 426/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5072e-04 - mean_absolute_error: 0.0152\n",
      "Epoch 00426: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 4.2957e-04 - mean_absolute_error: 0.0187 - val_loss: 2.3641e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 427/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4378e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 00427: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.6804e-04 - mean_absolute_error: 0.0182 - val_loss: 2.3167e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 428/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.5726e-04 - mean_absolute_error: 0.0215\n",
      "Epoch 00428: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 4.5359e-04 - mean_absolute_error: 0.0203 - val_loss: 2.4808e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 429/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0152e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 00429: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 3.7800e-04 - mean_absolute_error: 0.0189 - val_loss: 2.8988e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 430/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.3391e-04 - mean_absolute_error: 0.0207\n",
      "Epoch 00430: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 4.0551e-04 - mean_absolute_error: 0.0198 - val_loss: 2.2294e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 431/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.5053e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 00431: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.2017e-04 - mean_absolute_error: 0.0185 - val_loss: 2.3842e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 432/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.2138e-04 - mean_absolute_error: 0.0205\n",
      "Epoch 00432: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.9734e-04 - mean_absolute_error: 0.0187 - val_loss: 2.4422e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 433/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4953e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 00433: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.9850e-04 - mean_absolute_error: 0.0186 - val_loss: 2.2344e-04 - val_mean_absolute_error: 0.0156\n",
      "Epoch 434/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.3183e-04 - mean_absolute_error: 0.0215\n",
      "Epoch 00434: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.3045e-04 - mean_absolute_error: 0.0183 - val_loss: 2.7468e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 435/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.6606e-04 - mean_absolute_error: 0.0201\n",
      "Epoch 00435: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.6902e-04 - mean_absolute_error: 0.0194 - val_loss: 2.1803e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 436/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.8340e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 00436: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 2.9862e-04 - mean_absolute_error: 0.0167 - val_loss: 2.3256e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 437/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8820e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 00437: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.5717e-04 - mean_absolute_error: 0.0178 - val_loss: 3.3392e-04 - val_mean_absolute_error: 0.0201\n",
      "Epoch 438/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.3209e-04 - mean_absolute_error: 0.0216\n",
      "Epoch 00438: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 4.0367e-04 - mean_absolute_error: 0.0194 - val_loss: 2.4184e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 439/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.8001e-04 - mean_absolute_error: 0.0207\n",
      "Epoch 00439: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 2.8534e-04 - mean_absolute_error: 0.0180 - val_loss: 2.6611e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 440/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3827e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00440: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 2.4894e-04 - mean_absolute_error: 0.0165 - val_loss: 2.5694e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 441/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.6992e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 00441: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.4423e-04 - mean_absolute_error: 0.0185 - val_loss: 2.3019e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 442/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4904e-04 - mean_absolute_error: 0.0198\n",
      "Epoch 00442: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 2.7831e-04 - mean_absolute_error: 0.0173 - val_loss: 2.9141e-04 - val_mean_absolute_error: 0.0178\n",
      "Epoch 443/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.1305e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 00443: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 3.1329e-04 - mean_absolute_error: 0.0168 - val_loss: 2.3482e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 444/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1010e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 00444: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 4.7050e-04 - mean_absolute_error: 0.0203 - val_loss: 2.4093e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 445/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.3830e-04 - mean_absolute_error: 0.0194\n",
      "Epoch 00445: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 3.3919e-04 - mean_absolute_error: 0.0184 - val_loss: 2.7461e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 446/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.9182e-04 - mean_absolute_error: 0.0151\n",
      "Epoch 00446: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 2.8531e-04 - mean_absolute_error: 0.0161 - val_loss: 2.2641e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 447/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.9481e-04 - mean_absolute_error: 0.0225\n",
      "Epoch 00447: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 4.0028e-04 - mean_absolute_error: 0.0189 - val_loss: 3.0993e-04 - val_mean_absolute_error: 0.0189\n",
      "Epoch 448/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6127e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 00448: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 3.6803e-04 - mean_absolute_error: 0.0183 - val_loss: 2.4498e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 449/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3364e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 00449: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 4.1832e-04 - mean_absolute_error: 0.0193 - val_loss: 2.2440e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 450/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.3989e-04 - mean_absolute_error: 0.0217\n",
      "Epoch 00450: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.7216e-04 - mean_absolute_error: 0.0206 - val_loss: 2.6435e-04 - val_mean_absolute_error: 0.0174\n",
      "Epoch 451/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5013e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00451: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.5644e-04 - mean_absolute_error: 0.0188 - val_loss: 2.7533e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 452/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8607e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 00452: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.7319e-04 - mean_absolute_error: 0.0187 - val_loss: 2.7979e-04 - val_mean_absolute_error: 0.0173\n",
      "Epoch 453/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7710e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 00453: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 4.2410e-04 - mean_absolute_error: 0.0195 - val_loss: 3.7080e-04 - val_mean_absolute_error: 0.0183\n",
      "Epoch 454/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.5210e-04 - mean_absolute_error: 0.0214\n",
      "Epoch 00454: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 5.6927e-04 - mean_absolute_error: 0.0232 - val_loss: 2.4755e-04 - val_mean_absolute_error: 0.0181\n",
      "Epoch 455/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.0300e-04 - mean_absolute_error: 0.0207\n",
      "Epoch 00455: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 4.2551e-04 - mean_absolute_error: 0.0211 - val_loss: 4.0001e-04 - val_mean_absolute_error: 0.0228\n",
      "Epoch 456/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1429e-04 - mean_absolute_error: 0.0193\n",
      "Epoch 00456: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.2446e-04 - mean_absolute_error: 0.0208 - val_loss: 2.7799e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 457/700\n",
      "3/3 [==============================] - ETA: 0s - loss: 3.4931e-04 - mean_absolute_error: 0.0187\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 00457: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 3.4931e-04 - mean_absolute_error: 0.0187 - val_loss: 2.6454e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 458/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2938e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 00458: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.9000e-04 - mean_absolute_error: 0.0187 - val_loss: 2.5356e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 459/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.9318e-04 - mean_absolute_error: 0.0232\n",
      "Epoch 00459: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 4.7413e-04 - mean_absolute_error: 0.0222 - val_loss: 2.1951e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 460/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.7356e-04 - mean_absolute_error: 0.0221\n",
      "Epoch 00460: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.4541e-04 - mean_absolute_error: 0.0190 - val_loss: 3.9063e-04 - val_mean_absolute_error: 0.0223\n",
      "Epoch 461/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.4262e-04 - mean_absolute_error: 0.0234\n",
      "Epoch 00461: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.8231e-04 - mean_absolute_error: 0.0199 - val_loss: 2.6753e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 462/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.3320e-04 - mean_absolute_error: 0.0236\n",
      "Epoch 00462: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.3083e-04 - mean_absolute_error: 0.0207 - val_loss: 3.6353e-04 - val_mean_absolute_error: 0.0188\n",
      "Epoch 463/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0686e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 00463: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.7096e-04 - mean_absolute_error: 0.0192 - val_loss: 2.4813e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 464/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.0279e-04 - mean_absolute_error: 0.0257\n",
      "Epoch 00464: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 4.6861e-04 - mean_absolute_error: 0.0210 - val_loss: 2.4604e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 465/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3605e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00465: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.6331e-04 - mean_absolute_error: 0.0189 - val_loss: 2.8679e-04 - val_mean_absolute_error: 0.0180\n",
      "Epoch 466/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.6366e-04 - mean_absolute_error: 0.0200\n",
      "Epoch 00466: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.6141e-04 - mean_absolute_error: 0.0189 - val_loss: 2.1583e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 467/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.0290e-04 - mean_absolute_error: 0.0215\n",
      "Epoch 00467: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.0029e-04 - mean_absolute_error: 0.0191 - val_loss: 4.0259e-04 - val_mean_absolute_error: 0.0189\n",
      "Epoch 468/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.7176e-04 - mean_absolute_error: 0.0211\n",
      "Epoch 00468: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 4.7101e-04 - mean_absolute_error: 0.0207 - val_loss: 2.9741e-04 - val_mean_absolute_error: 0.0171\n",
      "Epoch 469/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.2429e-04 - mean_absolute_error: 0.0202\n",
      "Epoch 00469: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 4.3215e-04 - mean_absolute_error: 0.0203 - val_loss: 3.7166e-04 - val_mean_absolute_error: 0.0223\n",
      "Epoch 470/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.4733e-04 - mean_absolute_error: 0.0239\n",
      "Epoch 00470: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 4.0873e-04 - mean_absolute_error: 0.0212 - val_loss: 2.1261e-04 - val_mean_absolute_error: 0.0153\n",
      "Epoch 471/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.5247e-04 - mean_absolute_error: 0.0198\n",
      "Epoch 00471: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.7568e-04 - mean_absolute_error: 0.0187 - val_loss: 3.6588e-04 - val_mean_absolute_error: 0.0179\n",
      "Epoch 472/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.0187e-04 - mean_absolute_error: 0.0199\n",
      "Epoch 00472: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.8077e-04 - mean_absolute_error: 0.0202 - val_loss: 2.4432e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 473/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0454e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 00473: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 4.0143e-04 - mean_absolute_error: 0.0195 - val_loss: 2.3981e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 474/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.7046e-04 - mean_absolute_error: 0.0192\n",
      "Epoch 00474: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 4.5278e-04 - mean_absolute_error: 0.0197 - val_loss: 2.7603e-04 - val_mean_absolute_error: 0.0182\n",
      "Epoch 475/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.6572e-04 - mean_absolute_error: 0.0226\n",
      "Epoch 00475: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 5.4014e-04 - mean_absolute_error: 0.0202 - val_loss: 2.6669e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 476/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0907e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 00476: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 3.7626e-04 - mean_absolute_error: 0.0184 - val_loss: 2.8057e-04 - val_mean_absolute_error: 0.0170\n",
      "Epoch 477/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.4913e-04 - mean_absolute_error: 0.0234\n",
      "Epoch 00477: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 4.4164e-04 - mean_absolute_error: 0.0192 - val_loss: 2.7381e-04 - val_mean_absolute_error: 0.0172\n",
      "Epoch 478/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7144e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 00478: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 4.0647e-04 - mean_absolute_error: 0.0186 - val_loss: 2.4330e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 479/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0786e-04 - mean_absolute_error: 0.0144\n",
      "Epoch 00479: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.3742e-04 - mean_absolute_error: 0.0191 - val_loss: 2.8819e-04 - val_mean_absolute_error: 0.0180\n",
      "Epoch 480/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.4040e-04 - mean_absolute_error: 0.0233\n",
      "Epoch 00480: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 4.3130e-04 - mean_absolute_error: 0.0200 - val_loss: 2.7441e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 481/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3888e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 00481: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 3.7426e-04 - mean_absolute_error: 0.0188 - val_loss: 2.3710e-04 - val_mean_absolute_error: 0.0171\n",
      "Epoch 482/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.8302e-04 - mean_absolute_error: 0.0191\n",
      "Epoch 00482: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 3.1465e-04 - mean_absolute_error: 0.0178 - val_loss: 3.3424e-04 - val_mean_absolute_error: 0.0203\n",
      "Epoch 483/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.1841e-04 - mean_absolute_error: 0.0217\n",
      "Epoch 00483: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.3292e-04 - mean_absolute_error: 0.0177 - val_loss: 2.7271e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 484/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.0406e-04 - mean_absolute_error: 0.0217\n",
      "Epoch 00484: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 4.0854e-04 - mean_absolute_error: 0.0195 - val_loss: 2.5707e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 485/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7040e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 00485: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.5765e-04 - mean_absolute_error: 0.0176 - val_loss: 2.7307e-04 - val_mean_absolute_error: 0.0171\n",
      "Epoch 486/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9799e-04 - mean_absolute_error: 0.0194\n",
      "Epoch 00486: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 4.5303e-04 - mean_absolute_error: 0.0204 - val_loss: 2.1870e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 487/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4810e-04 - mean_absolute_error: 0.0191\n",
      "Epoch 00487: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 3.6508e-04 - mean_absolute_error: 0.0183 - val_loss: 2.4877e-04 - val_mean_absolute_error: 0.0173\n",
      "Epoch 488/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5945e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00488: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 2.7831e-04 - mean_absolute_error: 0.0163 - val_loss: 2.3231e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 489/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4585e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 00489: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 3.6326e-04 - mean_absolute_error: 0.0165 - val_loss: 2.2100e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 490/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.6556e-04 - mean_absolute_error: 0.0205\n",
      "Epoch 00490: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 4.5772e-04 - mean_absolute_error: 0.0195 - val_loss: 2.5217e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 491/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.6873e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 00491: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 3.7303e-04 - mean_absolute_error: 0.0183 - val_loss: 2.1433e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 492/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7428e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 00492: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.9491e-04 - mean_absolute_error: 0.0194 - val_loss: 2.4108e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 493/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1186e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 00493: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 3.1686e-04 - mean_absolute_error: 0.0179 - val_loss: 2.5855e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 494/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.3467e-04 - mean_absolute_error: 0.0208\n",
      "Epoch 00494: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 3.7727e-04 - mean_absolute_error: 0.0191 - val_loss: 2.0922e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 495/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0781e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 00495: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.5975e-04 - mean_absolute_error: 0.0193 - val_loss: 2.2547e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 496/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2539e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 00496: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.4785e-04 - mean_absolute_error: 0.0187 - val_loss: 2.5094e-04 - val_mean_absolute_error: 0.0171\n",
      "Epoch 497/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4163e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 00497: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 4.0680e-04 - mean_absolute_error: 0.0195 - val_loss: 2.3750e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 498/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.1269e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 00498: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 3.4605e-04 - mean_absolute_error: 0.0176 - val_loss: 2.5036e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 499/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.9417e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 00499: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 4.4648e-04 - mean_absolute_error: 0.0204 - val_loss: 2.4554e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 500/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8712e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 00500: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.4964e-04 - mean_absolute_error: 0.0172 - val_loss: 2.4601e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 501/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2350e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 00501: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 3.1194e-04 - mean_absolute_error: 0.0178 - val_loss: 2.7246e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 502/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.7857e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 00502: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.9714e-04 - mean_absolute_error: 0.0183 - val_loss: 2.2324e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 503/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.5747e-04 - mean_absolute_error: 0.0212\n",
      "Epoch 00503: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 3.7262e-04 - mean_absolute_error: 0.0203 - val_loss: 2.1388e-04 - val_mean_absolute_error: 0.0154\n",
      "Epoch 504/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.0380e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 00504: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 3.8794e-04 - mean_absolute_error: 0.0178 - val_loss: 3.0617e-04 - val_mean_absolute_error: 0.0200\n",
      "Epoch 505/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4022e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 00505: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 3.4880e-04 - mean_absolute_error: 0.0189 - val_loss: 2.2028e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 506/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.7811e-04 - mean_absolute_error: 0.0194\n",
      "Epoch 00506: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 4.0834e-04 - mean_absolute_error: 0.0193 - val_loss: 2.3645e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 507/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.6462e-04 - mean_absolute_error: 0.0200\n",
      "Epoch 00507: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.9250e-04 - mean_absolute_error: 0.0194 - val_loss: 2.2963e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 508/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9949e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 00508: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.6374e-04 - mean_absolute_error: 0.0168 - val_loss: 2.3569e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 509/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7447e-04 - mean_absolute_error: 0.0153\n",
      "Epoch 00509: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 2.8547e-04 - mean_absolute_error: 0.0160 - val_loss: 2.5575e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 510/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5846e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 00510: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.0126e-04 - mean_absolute_error: 0.0160 - val_loss: 2.2277e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 511/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8521e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 00511: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 2.7337e-04 - mean_absolute_error: 0.0170 - val_loss: 2.4307e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 512/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4686e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 00512: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 4.0399e-04 - mean_absolute_error: 0.0185 - val_loss: 2.2377e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 513/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.7146e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 00513: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 3.2966e-04 - mean_absolute_error: 0.0177 - val_loss: 2.9282e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 514/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.9600e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 00514: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 3.3818e-04 - mean_absolute_error: 0.0178 - val_loss: 2.0850e-04 - val_mean_absolute_error: 0.0156\n",
      "Epoch 515/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.3983e-04 - mean_absolute_error: 0.0219\n",
      "Epoch 00515: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 4.1581e-04 - mean_absolute_error: 0.0187 - val_loss: 2.3834e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 516/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.7508e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 00516: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 3.8721e-04 - mean_absolute_error: 0.0182 - val_loss: 2.6469e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 517/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.8464e-04 - mean_absolute_error: 0.0193\n",
      "Epoch 00517: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 4.2829e-04 - mean_absolute_error: 0.0198 - val_loss: 2.4378e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 518/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.3357e-04 - mean_absolute_error: 0.0219\n",
      "Epoch 00518: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 4.9666e-04 - mean_absolute_error: 0.0200 - val_loss: 2.6528e-04 - val_mean_absolute_error: 0.0154\n",
      "Epoch 519/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.6976e-04 - mean_absolute_error: 0.0196\n",
      "Epoch 00519: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 4.2512e-04 - mean_absolute_error: 0.0178 - val_loss: 2.4054e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 520/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.1311e-04 - mean_absolute_error: 0.0215\n",
      "Epoch 00520: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 4.3789e-04 - mean_absolute_error: 0.0195 - val_loss: 2.7043e-04 - val_mean_absolute_error: 0.0189\n",
      "Epoch 521/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.8593e-04 - mean_absolute_error: 0.0216\n",
      "Epoch 00521: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.6525e-04 - mean_absolute_error: 0.0192 - val_loss: 2.2025e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 522/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6800e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 00522: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 4.2693e-04 - mean_absolute_error: 0.0177 - val_loss: 2.3277e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 523/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7141e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 00523: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.8370e-04 - mean_absolute_error: 0.0198 - val_loss: 2.4320e-04 - val_mean_absolute_error: 0.0156\n",
      "Epoch 524/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.8440e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 00524: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 4.5187e-04 - mean_absolute_error: 0.0203 - val_loss: 3.3160e-04 - val_mean_absolute_error: 0.0194\n",
      "Epoch 525/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - mean_absolute_error: 0.0285\n",
      "Epoch 00525: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 5.8618e-04 - mean_absolute_error: 0.0216 - val_loss: 2.2794e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 526/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.0596e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 00526: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 3.7379e-04 - mean_absolute_error: 0.0184 - val_loss: 3.0210e-04 - val_mean_absolute_error: 0.0171\n",
      "Epoch 527/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4814e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 00527: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.9324e-04 - mean_absolute_error: 0.0189 - val_loss: 2.0738e-04 - val_mean_absolute_error: 0.0154\n",
      "Epoch 528/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4649e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 00528: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.2580e-04 - mean_absolute_error: 0.0175 - val_loss: 2.0585e-04 - val_mean_absolute_error: 0.0156\n",
      "Epoch 529/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.5367e-04 - mean_absolute_error: 0.0225\n",
      "Epoch 00529: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 3.6802e-04 - mean_absolute_error: 0.0193 - val_loss: 2.3468e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 530/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5571e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 00530: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 3.1223e-04 - mean_absolute_error: 0.0179 - val_loss: 2.0540e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 531/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.8914e-04 - mean_absolute_error: 0.0197\n",
      "Epoch 00531: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.7896e-04 - mean_absolute_error: 0.0189 - val_loss: 2.1966e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 532/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7588e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 00532: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 4.0270e-04 - mean_absolute_error: 0.0185 - val_loss: 2.2529e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 533/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9336e-04 - mean_absolute_error: 0.0193\n",
      "Epoch 00533: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 3.9484e-04 - mean_absolute_error: 0.0188 - val_loss: 2.1857e-04 - val_mean_absolute_error: 0.0154\n",
      "Epoch 534/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8077e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 00534: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 3.0664e-04 - mean_absolute_error: 0.0157 - val_loss: 2.5984e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 535/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.0818e-04 - mean_absolute_error: 0.0225\n",
      "Epoch 00535: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 3.7399e-04 - mean_absolute_error: 0.0195 - val_loss: 2.1205e-04 - val_mean_absolute_error: 0.0153\n",
      "Epoch 536/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1991e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 00536: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.6173e-04 - mean_absolute_error: 0.0182 - val_loss: 2.4600e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 537/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.0241e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 00537: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 3.8612e-04 - mean_absolute_error: 0.0177 - val_loss: 2.1301e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 538/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4259e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 00538: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 3.6057e-04 - mean_absolute_error: 0.0183 - val_loss: 2.2477e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 539/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2706e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 00539: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 3.6483e-04 - mean_absolute_error: 0.0182 - val_loss: 3.2904e-04 - val_mean_absolute_error: 0.0198\n",
      "Epoch 540/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0989e-04 - mean_absolute_error: 0.0191\n",
      "Epoch 00540: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 4.2678e-04 - mean_absolute_error: 0.0185 - val_loss: 2.2701e-04 - val_mean_absolute_error: 0.0176\n",
      "Epoch 541/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4098e-04 - mean_absolute_error: 0.0208\n",
      "Epoch 00541: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 4.4661e-04 - mean_absolute_error: 0.0205 - val_loss: 2.1230e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 542/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7464e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 00542: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 3.0280e-04 - mean_absolute_error: 0.0178 - val_loss: 2.7946e-04 - val_mean_absolute_error: 0.0181\n",
      "Epoch 543/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4190e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 00543: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 3.2498e-04 - mean_absolute_error: 0.0180 - val_loss: 2.0491e-04 - val_mean_absolute_error: 0.0156\n",
      "Epoch 544/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8192e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 00544: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 3.5542e-04 - mean_absolute_error: 0.0184 - val_loss: 3.5152e-04 - val_mean_absolute_error: 0.0177\n",
      "Epoch 545/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.5407e-04 - mean_absolute_error: 0.0218\n",
      "Epoch 00545: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 4.2213e-04 - mean_absolute_error: 0.0189 - val_loss: 2.3190e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 546/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.0508e-04 - mean_absolute_error: 0.0211\n",
      "Epoch 00546: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 3.5909e-04 - mean_absolute_error: 0.0182 - val_loss: 2.6185e-04 - val_mean_absolute_error: 0.0181\n",
      "Epoch 547/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.2905e-04 - mean_absolute_error: 0.0205\n",
      "Epoch 00547: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 3.5691e-04 - mean_absolute_error: 0.0185 - val_loss: 2.4067e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 548/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1720e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 00548: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 3.7761e-04 - mean_absolute_error: 0.0187 - val_loss: 2.3385e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 549/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8323e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 00549: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 2.9019e-04 - mean_absolute_error: 0.0172 - val_loss: 2.6182e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 550/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3169e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 00550: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.0081e-04 - mean_absolute_error: 0.0175 - val_loss: 2.3287e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 551/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.5866e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 00551: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 4.0180e-04 - mean_absolute_error: 0.0180 - val_loss: 2.2056e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 552/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.2611e-04 - mean_absolute_error: 0.0200\n",
      "Epoch 00552: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.0228e-04 - mean_absolute_error: 0.0195 - val_loss: 3.4439e-04 - val_mean_absolute_error: 0.0183\n",
      "Epoch 553/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.7744e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 00553: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 3.2354e-04 - mean_absolute_error: 0.0166 - val_loss: 2.2274e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 554/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6044e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 00554: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.4484e-04 - mean_absolute_error: 0.0187 - val_loss: 2.1841e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 555/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4362e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 00555: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.1852e-04 - mean_absolute_error: 0.0185 - val_loss: 2.6889e-04 - val_mean_absolute_error: 0.0189\n",
      "Epoch 556/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.6632e-04 - mean_absolute_error: 0.0198\n",
      "Epoch 00556: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 4.9753e-04 - mean_absolute_error: 0.0212 - val_loss: 2.7178e-04 - val_mean_absolute_error: 0.0175\n",
      "Epoch 557/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.9067e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 00557: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 4.2230e-04 - mean_absolute_error: 0.0203 - val_loss: 3.0082e-04 - val_mean_absolute_error: 0.0176\n",
      "Epoch 558/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.2599e-04 - mean_absolute_error: 0.0225\n",
      "Epoch 00558: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.7447e-04 - mean_absolute_error: 0.0219 - val_loss: 2.2046e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 559/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.4988e-04 - mean_absolute_error: 0.0211\n",
      "Epoch 00559: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.4381e-04 - mean_absolute_error: 0.0189 - val_loss: 3.7845e-04 - val_mean_absolute_error: 0.0224\n",
      "Epoch 560/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.8977e-04 - mean_absolute_error: 0.0227\n",
      "Epoch 00560: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 3.6992e-04 - mean_absolute_error: 0.0197 - val_loss: 2.3458e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 561/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3472e-04 - mean_absolute_error: 0.0256\n",
      "Epoch 00561: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 4.0421e-04 - mean_absolute_error: 0.0196 - val_loss: 2.5516e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 562/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5851e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 00562: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.0476e-04 - mean_absolute_error: 0.0176 - val_loss: 2.8256e-04 - val_mean_absolute_error: 0.0170\n",
      "Epoch 563/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5301e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 00563: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 2.6682e-04 - mean_absolute_error: 0.0168 - val_loss: 2.2797e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 564/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.4915e-04 - mean_absolute_error: 0.0198\n",
      "Epoch 00564: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 3.2655e-04 - mean_absolute_error: 0.0177 - val_loss: 3.0456e-04 - val_mean_absolute_error: 0.0180\n",
      "Epoch 565/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4339e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 00565: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.1343e-04 - mean_absolute_error: 0.0179 - val_loss: 2.2220e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 566/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.9759e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 00566: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 2.9750e-04 - mean_absolute_error: 0.0168 - val_loss: 2.5905e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 567/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9022e-04 - mean_absolute_error: 0.0132\n",
      "Epoch 00567: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 3.1902e-04 - mean_absolute_error: 0.0176 - val_loss: 2.2782e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 568/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.4553e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 00568: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.2671e-04 - mean_absolute_error: 0.0185 - val_loss: 2.3632e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 569/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.2797e-04 - mean_absolute_error: 0.0210\n",
      "Epoch 00569: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.1156e-04 - mean_absolute_error: 0.0182 - val_loss: 2.5428e-04 - val_mean_absolute_error: 0.0172\n",
      "Epoch 570/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8260e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00570: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.1311e-04 - mean_absolute_error: 0.0161 - val_loss: 2.2290e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 571/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0879e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 00571: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 3.3736e-04 - mean_absolute_error: 0.0178 - val_loss: 2.9176e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 572/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4694e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 00572: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 3.2618e-04 - mean_absolute_error: 0.0178 - val_loss: 2.4156e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 573/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.6212e-04 - mean_absolute_error: 0.0201\n",
      "Epoch 00573: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 3.7849e-04 - mean_absolute_error: 0.0193 - val_loss: 2.4022e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 574/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.8660e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 00574: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 4.0329e-04 - mean_absolute_error: 0.0191 - val_loss: 3.7837e-04 - val_mean_absolute_error: 0.0200\n",
      "Epoch 575/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3965e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 00575: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 4.0503e-04 - mean_absolute_error: 0.0193 - val_loss: 2.6498e-04 - val_mean_absolute_error: 0.0170\n",
      "Epoch 576/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2320e-04 - mean_absolute_error: 0.0195\n",
      "Epoch 00576: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.6375e-04 - mean_absolute_error: 0.0196 - val_loss: 2.2793e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 577/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.4964e-04 - mean_absolute_error: 0.0193\n",
      "Epoch 00577: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 3.8564e-04 - mean_absolute_error: 0.0185 - val_loss: 3.0078e-04 - val_mean_absolute_error: 0.0202\n",
      "Epoch 578/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.6601e-04 - mean_absolute_error: 0.0187\n",
      "Epoch 00578: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.8129e-04 - mean_absolute_error: 0.0198 - val_loss: 2.2807e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 579/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.1628e-04 - mean_absolute_error: 0.0246\n",
      "Epoch 00579: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 4.8232e-04 - mean_absolute_error: 0.0199 - val_loss: 3.5575e-04 - val_mean_absolute_error: 0.0176\n",
      "Epoch 580/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.0396e-04 - mean_absolute_error: 0.0200\n",
      "Epoch 00580: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 3.4604e-04 - mean_absolute_error: 0.0183 - val_loss: 3.3178e-04 - val_mean_absolute_error: 0.0181\n",
      "Epoch 581/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.6363e-04 - mean_absolute_error: 0.0213\n",
      "Epoch 00581: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 3.2618e-04 - mean_absolute_error: 0.0188 - val_loss: 2.2257e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 582/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.5084e-04 - mean_absolute_error: 0.0209\n",
      "Epoch 00582: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 3.1635e-04 - mean_absolute_error: 0.0168 - val_loss: 2.8952e-04 - val_mean_absolute_error: 0.0194\n",
      "Epoch 583/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.9593e-04 - mean_absolute_error: 0.0203\n",
      "Epoch 00583: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.9290e-04 - mean_absolute_error: 0.0192 - val_loss: 2.2829e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 584/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1617e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 00584: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 2.9058e-04 - mean_absolute_error: 0.0176 - val_loss: 2.9372e-04 - val_mean_absolute_error: 0.0170\n",
      "Epoch 585/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4133e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 00585: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 3.5846e-04 - mean_absolute_error: 0.0185 - val_loss: 2.5806e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 586/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.5578e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 00586: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.4714e-04 - mean_absolute_error: 0.0175 - val_loss: 2.2879e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 587/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.5385e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 00587: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 3.4832e-04 - mean_absolute_error: 0.0176 - val_loss: 2.5042e-04 - val_mean_absolute_error: 0.0172\n",
      "Epoch 588/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.0387e-04 - mean_absolute_error: 0.0197\n",
      "Epoch 00588: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 3.5459e-04 - mean_absolute_error: 0.0177 - val_loss: 2.7305e-04 - val_mean_absolute_error: 0.0171\n",
      "Epoch 589/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6202e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 00589: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 2.8602e-04 - mean_absolute_error: 0.0160 - val_loss: 2.5164e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 590/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3828e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 00590: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 3.1583e-04 - mean_absolute_error: 0.0172 - val_loss: 2.2573e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 591/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.3005e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 00591: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 4.1702e-04 - mean_absolute_error: 0.0196 - val_loss: 2.4940e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 592/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.3226e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 00592: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 2.5302e-04 - mean_absolute_error: 0.0159 - val_loss: 2.4764e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 593/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.3042e-04 - mean_absolute_error: 0.0214\n",
      "Epoch 00593: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.2690e-04 - mean_absolute_error: 0.0195 - val_loss: 2.4717e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 594/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2365e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 00594: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.6474e-04 - mean_absolute_error: 0.0185 - val_loss: 2.1503e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 595/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.3667e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 00595: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.5376e-04 - mean_absolute_error: 0.0179 - val_loss: 2.6221e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 596/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0660e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 00596: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 2.7015e-04 - mean_absolute_error: 0.0167 - val_loss: 2.0836e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 597/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3643e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 00597: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.3074e-04 - mean_absolute_error: 0.0172 - val_loss: 2.3844e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 598/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8342e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 00598: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 3.2199e-04 - mean_absolute_error: 0.0175 - val_loss: 2.1822e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 599/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9920e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 00599: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.8096e-04 - mean_absolute_error: 0.0182 - val_loss: 2.5203e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 600/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.1351e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 00600: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.1291e-04 - mean_absolute_error: 0.0199 - val_loss: 2.9728e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 601/700\n",
      "3/3 [==============================] - ETA: 0s - loss: 2.9448e-04 - mean_absolute_error: 0.0179\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 00601: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 2.9448e-04 - mean_absolute_error: 0.0179 - val_loss: 2.2535e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 602/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.0068e-04 - mean_absolute_error: 0.0207\n",
      "Epoch 00602: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 4.0761e-04 - mean_absolute_error: 0.0193 - val_loss: 3.5158e-04 - val_mean_absolute_error: 0.0216\n",
      "Epoch 603/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3023e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 00603: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 5.0599e-04 - mean_absolute_error: 0.0206 - val_loss: 2.5240e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 604/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7727e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 00604: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 3.1456e-04 - mean_absolute_error: 0.0182 - val_loss: 2.6344e-04 - val_mean_absolute_error: 0.0172\n",
      "Epoch 605/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4468e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 00605: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.8244e-04 - mean_absolute_error: 0.0185 - val_loss: 2.3333e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 606/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5297e-04 - mean_absolute_error: 0.0152\n",
      "Epoch 00606: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 3.6571e-04 - mean_absolute_error: 0.0184 - val_loss: 2.4224e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 607/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3208e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 00607: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 3.7740e-04 - mean_absolute_error: 0.0193 - val_loss: 3.5742e-04 - val_mean_absolute_error: 0.0194\n",
      "Epoch 608/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.1687e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 00608: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.2795e-04 - mean_absolute_error: 0.0177 - val_loss: 2.7813e-04 - val_mean_absolute_error: 0.0192\n",
      "Epoch 609/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9770e-04 - mean_absolute_error: 0.0236\n",
      "Epoch 00609: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.5742e-04 - mean_absolute_error: 0.0191 - val_loss: 2.8265e-04 - val_mean_absolute_error: 0.0180\n",
      "Epoch 610/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2905e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 00610: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.2020e-04 - mean_absolute_error: 0.0183 - val_loss: 2.7449e-04 - val_mean_absolute_error: 0.0179\n",
      "Epoch 611/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9695e-04 - mean_absolute_error: 0.0192\n",
      "Epoch 00611: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 3.1287e-04 - mean_absolute_error: 0.0171 - val_loss: 2.6095e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 612/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.5673e-04 - mean_absolute_error: 0.0199\n",
      "Epoch 00612: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.4739e-04 - mean_absolute_error: 0.0175 - val_loss: 2.6411e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 613/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.9912e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 00613: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.5948e-04 - mean_absolute_error: 0.0174 - val_loss: 2.3921e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 614/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7642e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 00614: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.0598e-04 - mean_absolute_error: 0.0168 - val_loss: 2.3298e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 615/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9516e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 00615: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 3.0222e-04 - mean_absolute_error: 0.0167 - val_loss: 2.2209e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 616/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8447e-04 - mean_absolute_error: 0.0153\n",
      "Epoch 00616: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.3065e-04 - mean_absolute_error: 0.0179 - val_loss: 2.5591e-04 - val_mean_absolute_error: 0.0174\n",
      "Epoch 617/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.7479e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 00617: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.4608e-04 - mean_absolute_error: 0.0183 - val_loss: 2.2286e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 618/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.1452e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 00618: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.4496e-04 - mean_absolute_error: 0.0203 - val_loss: 2.3511e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 619/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.0925e-04 - mean_absolute_error: 0.0200\n",
      "Epoch 00619: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.4666e-04 - mean_absolute_error: 0.0206 - val_loss: 2.7498e-04 - val_mean_absolute_error: 0.0172\n",
      "Epoch 620/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.3469e-04 - mean_absolute_error: 0.0187\n",
      "Epoch 00620: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.6646e-04 - mean_absolute_error: 0.0192 - val_loss: 2.3801e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 621/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.7468e-04 - mean_absolute_error: 0.0203\n",
      "Epoch 00621: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.1697e-04 - mean_absolute_error: 0.0170 - val_loss: 4.0710e-04 - val_mean_absolute_error: 0.0217\n",
      "Epoch 622/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.5500e-04 - mean_absolute_error: 0.0207\n",
      "Epoch 00622: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.3992e-04 - mean_absolute_error: 0.0186 - val_loss: 2.2590e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 623/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.1407e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 00623: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.1309e-04 - mean_absolute_error: 0.0186 - val_loss: 2.2650e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 624/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8813e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 00624: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 4.2787e-04 - mean_absolute_error: 0.0189 - val_loss: 2.7262e-04 - val_mean_absolute_error: 0.0187\n",
      "Epoch 625/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7231e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 00625: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 3.4117e-04 - mean_absolute_error: 0.0174 - val_loss: 2.0928e-04 - val_mean_absolute_error: 0.0153\n",
      "Epoch 626/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9888e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 00626: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 3.8777e-04 - mean_absolute_error: 0.0197 - val_loss: 2.4704e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 627/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9494e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 00627: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.0193e-04 - mean_absolute_error: 0.0173 - val_loss: 2.4402e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 628/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.0675e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 00628: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 4.2782e-04 - mean_absolute_error: 0.0181 - val_loss: 2.5971e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 629/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9810e-04 - mean_absolute_error: 0.0194\n",
      "Epoch 00629: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 2.8645e-04 - mean_absolute_error: 0.0158 - val_loss: 2.6588e-04 - val_mean_absolute_error: 0.0181\n",
      "Epoch 630/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.4534e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 00630: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 4.0690e-04 - mean_absolute_error: 0.0185 - val_loss: 2.0491e-04 - val_mean_absolute_error: 0.0154\n",
      "Epoch 631/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.0146e-04 - mean_absolute_error: 0.0204\n",
      "Epoch 00631: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.2695e-04 - mean_absolute_error: 0.0179 - val_loss: 2.2776e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 632/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5553e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 00632: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.9659e-04 - mean_absolute_error: 0.0190 - val_loss: 2.1793e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 633/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2493e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 00633: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 2.8387e-04 - mean_absolute_error: 0.0167 - val_loss: 2.3974e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 634/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.4225e-04 - mean_absolute_error: 0.0199\n",
      "Epoch 00634: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.3969e-04 - mean_absolute_error: 0.0185 - val_loss: 3.0982e-04 - val_mean_absolute_error: 0.0188\n",
      "Epoch 635/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7549e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 00635: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.2363e-04 - mean_absolute_error: 0.0184 - val_loss: 2.3162e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 636/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2046e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 00636: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.7556e-04 - mean_absolute_error: 0.0195 - val_loss: 2.5858e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 637/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1171e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 00637: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 3.8013e-04 - mean_absolute_error: 0.0199 - val_loss: 2.7888e-04 - val_mean_absolute_error: 0.0175\n",
      "Epoch 638/700\n",
      "3/3 [==============================] - ETA: 0s - loss: 4.2246e-04 - mean_absolute_error: 0.0189\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 00638: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 4.2246e-04 - mean_absolute_error: 0.0189 - val_loss: 2.4487e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 639/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3354e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 00639: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 3.1596e-04 - mean_absolute_error: 0.0169 - val_loss: 2.8262e-04 - val_mean_absolute_error: 0.0172\n",
      "Epoch 640/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.9376e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 00640: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.7877e-04 - mean_absolute_error: 0.0180 - val_loss: 2.4689e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 641/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0007e-04 - mean_absolute_error: 0.0144\n",
      "Epoch 00641: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.3125e-04 - mean_absolute_error: 0.0177 - val_loss: 3.0453e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 642/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.9616e-04 - mean_absolute_error: 0.0209\n",
      "Epoch 00642: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 4.3548e-04 - mean_absolute_error: 0.0186 - val_loss: 2.2472e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 643/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0031e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 00643: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.0821e-04 - mean_absolute_error: 0.0171 - val_loss: 2.3957e-04 - val_mean_absolute_error: 0.0176\n",
      "Epoch 644/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.8216e-04 - mean_absolute_error: 0.0187\n",
      "Epoch 00644: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.8593e-04 - mean_absolute_error: 0.0188 - val_loss: 2.4009e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 645/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8809e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 00645: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.6158e-04 - mean_absolute_error: 0.0181 - val_loss: 2.2751e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 646/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.8965e-04 - mean_absolute_error: 0.0228\n",
      "Epoch 00646: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.5419e-04 - mean_absolute_error: 0.0187 - val_loss: 2.7141e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 647/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1207e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 00647: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.2763e-04 - mean_absolute_error: 0.0176 - val_loss: 2.1986e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 648/700\n",
      "3/3 [==============================] - ETA: 0s - loss: 3.6496e-04 - mean_absolute_error: 0.0184\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 00648: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 3.6496e-04 - mean_absolute_error: 0.0184 - val_loss: 2.2720e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 649/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0645e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 00649: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 2.5779e-04 - mean_absolute_error: 0.0155 - val_loss: 2.2715e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 650/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.0678e-04 - mean_absolute_error: 0.0205\n",
      "Epoch 00650: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.5027e-04 - mean_absolute_error: 0.0174 - val_loss: 2.2490e-04 - val_mean_absolute_error: 0.0156\n",
      "Epoch 651/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.0064e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 00651: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.0253e-04 - mean_absolute_error: 0.0167 - val_loss: 2.3780e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 652/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.9364e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 00652: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 2.4026e-04 - mean_absolute_error: 0.0161 - val_loss: 2.2937e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 653/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0497e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 00653: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.5382e-04 - mean_absolute_error: 0.0177 - val_loss: 2.1617e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 654/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2227e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 00654: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 2.9557e-04 - mean_absolute_error: 0.0170 - val_loss: 2.3375e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 655/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9879e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 00655: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.9926e-04 - mean_absolute_error: 0.0185 - val_loss: 2.1642e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 656/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4720e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 00656: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 2.8071e-04 - mean_absolute_error: 0.0164 - val_loss: 2.1503e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 657/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.8468e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 00657: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 4.0816e-04 - mean_absolute_error: 0.0172 - val_loss: 2.4286e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 658/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.4942e-04 - mean_absolute_error: 0.0223\n",
      "Epoch 00658: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.8239e-04 - mean_absolute_error: 0.0178 - val_loss: 2.2400e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 659/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5223e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 00659: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 2s 854ms/step - loss: 2.9004e-04 - mean_absolute_error: 0.0166 - val_loss: 2.4455e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 660/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8879e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 00660: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 2s 872ms/step - loss: 3.7998e-04 - mean_absolute_error: 0.0176 - val_loss: 2.9041e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 661/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2665e-04 - mean_absolute_error: 0.0153\n",
      "Epoch 00661: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 2.9779e-04 - mean_absolute_error: 0.0179 - val_loss: 2.2189e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 662/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9816e-04 - mean_absolute_error: 0.0208\n",
      "Epoch 00662: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.6983e-04 - mean_absolute_error: 0.0192 - val_loss: 3.0463e-04 - val_mean_absolute_error: 0.0190\n",
      "Epoch 663/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0554e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 00663: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.3800e-04 - mean_absolute_error: 0.0185 - val_loss: 2.2993e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 664/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1395e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 00664: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 3.0549e-04 - mean_absolute_error: 0.0169 - val_loss: 2.4703e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 665/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8279e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 00665: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 3.0707e-04 - mean_absolute_error: 0.0170 - val_loss: 2.2367e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 666/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.2540e-04 - mean_absolute_error: 0.0190\n",
      "Epoch 00666: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 4.0258e-04 - mean_absolute_error: 0.0191 - val_loss: 2.6073e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 667/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.5352e-04 - mean_absolute_error: 0.0204\n",
      "Epoch 00667: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 4.6399e-04 - mean_absolute_error: 0.0203 - val_loss: 2.5264e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 668/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.5795e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 00668: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 3.4582e-04 - mean_absolute_error: 0.0179 - val_loss: 2.9365e-04 - val_mean_absolute_error: 0.0178\n",
      "Epoch 669/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.6639e-04 - mean_absolute_error: 0.0205\n",
      "Epoch 00669: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 3.5307e-04 - mean_absolute_error: 0.0173 - val_loss: 2.6244e-04 - val_mean_absolute_error: 0.0183\n",
      "Epoch 670/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5185e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 00670: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 4.4738e-04 - mean_absolute_error: 0.0208 - val_loss: 4.1902e-04 - val_mean_absolute_error: 0.0189\n",
      "Epoch 671/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.9061e-04 - mean_absolute_error: 0.0202\n",
      "Epoch 00671: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 4.6656e-04 - mean_absolute_error: 0.0183 - val_loss: 2.5280e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 672/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.0268e-04 - mean_absolute_error: 0.0204\n",
      "Epoch 00672: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 3.6555e-04 - mean_absolute_error: 0.0191 - val_loss: 2.7930e-04 - val_mean_absolute_error: 0.0195\n",
      "Epoch 673/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0962e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 00673: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 3.0591e-04 - mean_absolute_error: 0.0177 - val_loss: 2.1893e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 674/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0474e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 00674: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 2.6868e-04 - mean_absolute_error: 0.0165 - val_loss: 2.8529e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 675/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.6395e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 00675: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.9360e-04 - mean_absolute_error: 0.0179 - val_loss: 3.1952e-04 - val_mean_absolute_error: 0.0171\n",
      "Epoch 676/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.6402e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 00676: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.6118e-04 - mean_absolute_error: 0.0186 - val_loss: 2.0748e-04 - val_mean_absolute_error: 0.0154\n",
      "Epoch 677/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.8339e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 00677: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 3.3228e-04 - mean_absolute_error: 0.0165 - val_loss: 2.5920e-04 - val_mean_absolute_error: 0.0188\n",
      "Epoch 678/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.9281e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 00678: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 3.1391e-04 - mean_absolute_error: 0.0185 - val_loss: 2.1256e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 679/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.0714e-04 - mean_absolute_error: 0.0193\n",
      "Epoch 00679: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.2930e-04 - mean_absolute_error: 0.0172 - val_loss: 3.2582e-04 - val_mean_absolute_error: 0.0174\n",
      "Epoch 680/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1562e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 00680: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 3.3661e-04 - mean_absolute_error: 0.0181 - val_loss: 2.2030e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 681/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.1400e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 00681: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 3.4085e-04 - mean_absolute_error: 0.0181 - val_loss: 2.2784e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 682/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.6073e-04 - mean_absolute_error: 0.0190\n",
      "Epoch 00682: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 2.8304e-04 - mean_absolute_error: 0.0176 - val_loss: 2.5640e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 683/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0950e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 00683: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.4019e-04 - mean_absolute_error: 0.0174 - val_loss: 2.4186e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 684/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.7111e-04 - mean_absolute_error: 0.0215\n",
      "Epoch 00684: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.4990e-04 - mean_absolute_error: 0.0167 - val_loss: 3.5514e-04 - val_mean_absolute_error: 0.0179\n",
      "Epoch 685/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.2646e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 00685: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.7948e-04 - mean_absolute_error: 0.0181 - val_loss: 3.2316e-04 - val_mean_absolute_error: 0.0176\n",
      "Epoch 686/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.6174e-04 - mean_absolute_error: 0.0192\n",
      "Epoch 00686: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.0366e-04 - mean_absolute_error: 0.0175 - val_loss: 2.2557e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 687/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2759e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 00687: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 3.6522e-04 - mean_absolute_error: 0.0186 - val_loss: 3.4107e-04 - val_mean_absolute_error: 0.0217\n",
      "Epoch 688/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.2169e-04 - mean_absolute_error: 0.0264\n",
      "Epoch 00688: val_loss did not improve from 0.00020\n",
      "3/3 [==============================]\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b - 0s 37ms/step - loss: 4.7221e-04 - mean_absolute_error: 0.0209 - val_loss: 2.3514e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 689/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2623e-04 - mean_absolute_error: 0.0187\n",
      "Epoch 00689: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 3.9272e-04 - mean_absolute_error: 0.0187 - val_loss: 2.9329e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 690/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2437e-04 - mean_absolute_error: 0.0123\n",
      "Epoch 00690: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 2.6737e-04 - mean_absolute_error: 0.0162 - val_loss: 2.5443e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 691/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.7856e-04 - mean_absolute_error: 0.0203\n",
      "Epoch 00691: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 4.0794e-04 - mean_absolute_error: 0.0195 - val_loss: 2.5881e-04 - val_mean_absolute_error: 0.0173\n",
      "Epoch 692/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8413e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 00692: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 4.2046e-04 - mean_absolute_error: 0.0207 - val_loss: 2.3359e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 693/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8978e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 00693: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 3.6571e-04 - mean_absolute_error: 0.0195 - val_loss: 2.1501e-04 - val_mean_absolute_error: 0.0156\n",
      "Epoch 694/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6912e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 00694: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.3006e-04 - mean_absolute_error: 0.0185 - val_loss: 2.8302e-04 - val_mean_absolute_error: 0.0174\n",
      "Epoch 695/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5867e-04 - mean_absolute_error: 0.0152\n",
      "Epoch 00695: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 3.5450e-04 - mean_absolute_error: 0.0171 - val_loss: 2.1457e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 696/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8856e-04 - mean_absolute_error: 0.0190\n",
      "Epoch 00696: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 2.6961e-04 - mean_absolute_error: 0.0171 - val_loss: 2.7054e-04 - val_mean_absolute_error: 0.0177\n",
      "Epoch 697/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.8986e-04 - mean_absolute_error: 0.0192\n",
      "Epoch 00697: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.5349e-04 - mean_absolute_error: 0.0183 - val_loss: 2.4815e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 698/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.2689e-04 - mean_absolute_error: 0.0215\n",
      "Epoch 00698: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 4.0861e-04 - mean_absolute_error: 0.0184 - val_loss: 2.6633e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 699/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.3454e-04 - mean_absolute_error: 0.0201\n",
      "Epoch 00699: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 4.0148e-04 - mean_absolute_error: 0.0187 - val_loss: 3.2043e-04 - val_mean_absolute_error: 0.0175\n",
      "Epoch 700/700\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2610e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 00700: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 3.7212e-04 - mean_absolute_error: 0.0206 - val_loss: 2.6222e-04 - val_mean_absolute_error: 0.0171\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "if os.path.exists(os.path.join(\"results\", model_name + \".h5\")):\n",
    "\tprint(\"Модель уже просчитана, загружаем модель\")\n",
    "\t# model = keras.models.load_model(path_best_model)\n",
    "\n",
    "else:\"\"\"\n",
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "# train the model and save the weights whenever we see\n",
    "# a new optimal model using ModelCheckpoint\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graph(test_df):\n",
    "    \"\"\"\n",
    "    This function plots true close price along with predicted close price\n",
    "    with blue and red colors respectively\n",
    "    \"\"\"\n",
    "    plt.plot(test_df[f'true_close_{LOOKUP_STEP}'], c='b')\n",
    "    plt.plot(test_df[f'close_{LOOKUP_STEP}'], c='r')\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_df(model, data):\n",
    "    \"\"\"\n",
    "    This function takes the `model` and `data` dict to \n",
    "    construct a final dataframe that includes the features along \n",
    "    with true and predicted prices of the testing dataset\n",
    "    \"\"\"\n",
    "    # if predicted future price is higher than the current, \n",
    "    # then calculate the true future price minus the current price, to get the buy profit\n",
    "    buy_profit  = lambda current, pred_future, true_future: true_future - current if pred_future > current else 0\n",
    "    # if the predicted future price is lower than the current price,\n",
    "    # then subtract the true future price from the current price\n",
    "    sell_profit = lambda current, pred_future, true_future: current - true_future if pred_future < current else 0\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_test = data[\"y_test\"]\n",
    "    # perform prediction and get prices\n",
    "    y_pred = model.predict(X_test)\n",
    "    if SCALE:\n",
    "        y_test = np.squeeze(data[\"column_scaler\"][\"close\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "        y_pred = np.squeeze(data[\"column_scaler\"][\"close\"].inverse_transform(y_pred))\n",
    "    test_df = data[\"test_df\"]\n",
    "    # add predicted future prices to the dataframe\n",
    "    test_df[f\"close_{LOOKUP_STEP}\"] = y_pred\n",
    "    # add true future prices to the dataframe\n",
    "    test_df[f\"true_close_{LOOKUP_STEP}\"] = y_test\n",
    "    # sort the dataframe by date\n",
    "    test_df.sort_index(inplace=True)\n",
    "    final_df = test_df\n",
    "    # add the buy profit column\n",
    "    final_df[\"buy_profit\"] = list(map(buy_profit, \n",
    "                                    final_df[\"close\"],\n",
    "                                    final_df[f\"close_{LOOKUP_STEP}\"],\n",
    "                                    final_df[f\"true_close_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    # add the sell profit column\n",
    "    final_df[\"sell_profit\"] = list(map(sell_profit, \n",
    "                                    final_df[\"close\"],\n",
    "                                    final_df[f\"close_{LOOKUP_STEP}\"],\n",
    "                                    final_df[f\"true_close_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    # retrieve the last sequence from data\n",
    "    last_sequence = data[\"last_sequence\"][-N_STEPS:]\n",
    "    # expand dimension\n",
    "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
    "    # get the prediction (scaled from 0 to 1)\n",
    "    prediction = model.predict(last_sequence)\n",
    "    # get the price (by inverting the scaling)\n",
    "    if SCALE:\n",
    "        predicted_price = data[\"column_scaler\"][\"close\"].inverse_transform(prediction)[0][0]\n",
    "    else:\n",
    "        predicted_price = prediction[0][0]\n",
    "    return predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load optimal model weights from results folder\n",
    "model_path = os.path.join(\"results\", model_name) + \".h5\"\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
    "# calculate the mean absolute error (inverse scaling)\n",
    "if SCALE:\n",
    "    mean_absolute_error = data[\"column_scaler\"][\"close\"].inverse_transform([[mae]])[0][0]\n",
    "else:\n",
    "    mean_absolute_error = mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the final dataframe for the testing set\n",
    "final_df = get_final_df(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the future price\n",
    "future_price = predict(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "outputs": [
    {
     "data": {
      "text/plain": "4507.396"
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_price"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we calculate the accuracy by counting the number of positive profits\n",
    "accuracy_score = (len(final_df[final_df['sell_profit'] > 0]) + len(final_df[final_df['buy_profit'] > 0])) / len(final_df)\n",
    "# calculating total buy & sell profit\n",
    "total_buy_profit  = final_df[\"buy_profit\"].sum()\n",
    "total_sell_profit = final_df[\"sell_profit\"].sum()\n",
    "# total profit by adding sell & buy together\n",
    "total_profit = total_buy_profit + total_sell_profit\n",
    "# dividing total profit by number of testing samples (number of trades)\n",
    "profit_per_trade = total_profit / len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future price after 1 days is 4507.40$\n",
      "huber_loss loss: 0.0002043728600256145\n",
      "Mean Absolute Error: 794.2592488414449\n",
      "Accuracy score: 0.5918367346938775\n",
      "Total buy profit: 411.67028808593795\n",
      "Total sell profit: 530.7999267578124\n",
      "Total profit: 942.4702148437503\n",
      "Profit per trade: 19.234086017219393\n"
     ]
    }
   ],
   "source": [
    "# printing metrics\n",
    "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")\n",
    "print(f\"{LOSS} loss:\", loss)\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error)\n",
    "print(\"Accuracy score:\", accuracy_score)\n",
    "print(\"Total buy profit:\", total_buy_profit)\n",
    "print(\"Total sell profit:\", total_sell_profit)\n",
    "print(\"Total profit:\", total_profit)\n",
    "print(\"Profit per trade:\", profit_per_trade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot true/pred prices graph\n",
    "plot_graph(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "            CHNCPIALLMINMEI  USACPIALLMINMEI  BRACPIALLMINMEI  \\\n2001-11-01        70.777981        74.846952        41.783996   \n2003-06-01        70.038462        77.504989        50.467978   \n2003-09-01        71.018201        78.137855        51.136637   \n2003-10-01        71.657364        78.053473        51.284893   \n2004-02-01        73.461853        78.559766        52.437761   \n2004-08-01        73.825029        79.952071        54.385503   \n2004-09-01        74.637104        80.120835        54.565081   \n2005-01-01        74.934531        80.458364        55.980587   \n2005-04-01        75.370012        82.103815        57.147144   \n2005-08-01        74.766753        82.863255        57.657107   \n2005-12-01        75.665965        83.032019        58.824360   \n2006-02-01        77.032870        83.833649        59.414135   \n2007-01-01        78.337664        85.401469        60.939382   \n2007-07-01        80.071589        87.883570        62.081578   \n2007-10-01        81.519373        88.152327        62.673209   \n2008-06-01        85.057527        92.320382        65.686351   \n2008-09-01        85.057442        92.306881        66.391436   \n2009-05-01        84.035292        90.228127        68.593463   \n2009-07-01        83.615115        90.858884        69.005517   \n2009-09-01        84.369315        91.119624        69.274883   \n\n            INDCPIALLMINMEI  BRAPROINDMISMEI  USAPROINDMISMEI  PCUOMFGOMFG  \\\n2001-11-01        39.136622        83.163455        87.329391        132.7   \n2003-06-01        41.209537        86.099642        89.937602        136.3   \n2003-09-01        41.375370        89.402697        90.827986        137.1   \n2003-10-01        41.707036        89.843105        90.928790        138.2   \n2004-02-01        41.789953        92.485549        92.288700        139.3   \n2004-08-01        43.282451        96.449216        93.024557        143.7   \n2004-09-01        43.365368        97.109827        93.111485        144.2   \n2005-01-01        43.614117        97.660336        95.269399        146.2   \n2005-04-01        43.862867        97.219928        96.010213        149.6   \n2005-08-01        44.774949        98.100743        96.527415        151.8   \n2005-12-01        45.604115        99.642169        97.345343        152.8   \n2006-02-01        45.521199       100.633086        97.474594        153.5   \n2007-01-01        48.581447       102.064410        98.980010        156.4   \n2007-07-01        50.494103       106.248280       100.824511        164.9   \n2007-10-01        51.259165       109.551335       100.989445        164.5   \n2008-06-01        53.554351       113.294798        99.203325        182.0   \n2008-09-01        55.849538       113.735205        93.018115        182.9   \n2009-05-01        57.762193        99.421965        84.313900        165.8   \n2009-07-01        61.204973       101.954308        84.991476        167.1   \n2009-09-01        62.352566       105.147261        86.636947        168.6   \n\n            RUSCPIALLMINMEI  PIEATI02RUM661N  RUSPROMANMISMEI  ...  \\\n2001-11-01        25.806427        21.147724        53.721605  ...   \n2003-06-01        32.562486        26.058890        57.717691  ...   \n2003-09-01        32.770267        27.492042        58.958453  ...   \n2003-10-01        33.097970        27.719249        59.986179  ...   \n2004-02-01        34.714798        29.991318        65.642278  ...   \n2004-08-01        36.342252        33.556720        65.130335  ...   \n2004-09-01        36.498523        34.605367        64.243999  ...   \n2005-01-01        38.738902        36.125906        65.838104  ...   \n2005-04-01        40.185975        38.432930        68.838945  ...   \n2005-08-01        40.896901        40.477793        69.427724  ...   \n2005-12-01        41.870245        40.757432        71.069876  ...   \n2006-02-01        43.599628        42.312926        71.088152  ...   \n2007-01-01        46.403498        45.825895        82.986169  ...   \n2007-07-01        48.636678        51.908051        83.886019  ...   \n2007-10-01        49.869693        52.607149        82.686219  ...   \n2008-06-01        55.519400        65.802630        85.585736  ...   \n2008-09-01        56.451465        66.169657        82.986169  ...   \n2009-05-01        61.740867        56.329848        69.188469  ...   \n2009-07-01        62.502614        58.566962        71.488085  ...   \n2009-09-01        62.483863        60.104979        71.688052  ...   \n\n                   ma21        26ema        12ema        MACD          ema  \\\n2001-11-01  1302.703793  1247.306845  1189.125687  -58.181158  1115.276460   \n2003-06-01   981.448097  1002.221903   935.591515  -66.630388   963.154458   \n2003-09-01   964.637623  1001.372698   960.348476  -41.024222   997.010888   \n2003-10-01   960.852385  1005.136488   974.256639  -30.879848  1032.810270   \n2004-02-01   963.138570  1034.665134  1043.940929    9.275795  1135.899103   \n2004-08-01  1022.478091  1065.198685  1089.055767   23.857081  1106.925911   \n2004-09-01  1033.657136  1068.902637  1092.982853   24.080215  1112.028608   \n2005-01-01  1092.479039  1097.564760  1134.423003   36.858243  1185.413457   \n2005-04-01  1121.652367  1114.507639  1151.503453   36.995814  1166.646399   \n2005-08-01  1156.149990  1140.096906  1180.969860   40.872954  1219.884997   \n2005-12-01  1176.120948  1165.177811  1207.572673   42.394862  1244.665024   \n2006-02-01  1191.966657  1181.668302  1228.255789   46.587487  1276.531682   \n2007-01-01  1287.089518  1277.502481  1342.940615   65.438134  1428.099654   \n2007-07-01  1359.762370  1348.671365  1425.179969   76.508604  1471.751666   \n2007-10-01  1396.537133  1383.736422  1462.886240   79.149818  1535.891546   \n2008-06-01  1430.089042  1378.727611  1390.155783   11.428172  1316.586559   \n2008-09-01  1407.217611  1349.338935  1328.230472  -21.108463  1205.289113   \n2009-05-01  1179.003807  1125.020440   982.760056 -142.260384   894.543675   \n2009-07-01  1123.321426  1100.720378   975.227730 -125.492648   962.007064   \n2009-09-01  1081.807138  1091.992829   993.729433  -98.263396  1038.414088   \n\n               momentum      close_1  true_close_1  buy_profit  sell_profit  \n2001-11-01  1138.449951  1071.648438   1148.079956    0.000000    -8.630005  \n2003-06-01   973.500000   962.451111    990.309998    0.000000   -15.809998  \n2003-09-01   994.969971  1037.929077   1050.709961   54.739990     0.000000  \n2003-10-01  1049.709961  1050.734741   1058.199951    7.489990     0.000000  \n2004-02-01  1143.939941  1097.700073   1126.209961    0.000000    18.729980  \n2004-08-01  1103.239990  1123.018188   1114.579956   10.339966     0.000000  \n2004-09-01  1113.579956  1138.211426   1130.199951   15.619995     0.000000  \n2005-01-01  1180.270020  1186.475586   1203.599976   22.329956     0.000000  \n2005-04-01  1155.849976  1175.471558   1191.500000   34.650024     0.000000  \n2005-08-01  1219.329956  1259.066650   1228.810059    8.480103     0.000000  \n2005-12-01  1247.290039  1282.921021   1280.079956   31.789917     0.000000  \n2006-02-01  1279.660034  1299.351807   1294.869995   14.209961     0.000000  \n2007-01-01  1437.239990  1407.298096   1406.819946    0.000000    31.420044  \n2007-07-01  1454.270020  1442.710693   1473.989990    0.000000   -18.719971  \n2007-10-01  1548.380005  1486.721191   1481.140015    0.000000    68.239990  \n2008-06-01  1279.000000  1299.429932   1267.380005  -12.619995     0.000000  \n2008-09-01  1165.359985  1069.889282    968.750000    0.000000   197.609985  \n2009-05-01   918.140015   891.160461    919.320007    0.000000    -0.179993  \n2009-07-01   986.479980   979.624146   1020.619995    0.000000   -33.140015  \n2009-09-01  1056.079956  1046.526489   1036.189941    0.000000    20.890015  \n\n[20 rows x 81 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CHNCPIALLMINMEI</th>\n      <th>USACPIALLMINMEI</th>\n      <th>BRACPIALLMINMEI</th>\n      <th>INDCPIALLMINMEI</th>\n      <th>BRAPROINDMISMEI</th>\n      <th>USAPROINDMISMEI</th>\n      <th>PCUOMFGOMFG</th>\n      <th>RUSCPIALLMINMEI</th>\n      <th>PIEATI02RUM661N</th>\n      <th>RUSPROMANMISMEI</th>\n      <th>...</th>\n      <th>ma21</th>\n      <th>26ema</th>\n      <th>12ema</th>\n      <th>MACD</th>\n      <th>ema</th>\n      <th>momentum</th>\n      <th>close_1</th>\n      <th>true_close_1</th>\n      <th>buy_profit</th>\n      <th>sell_profit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2001-11-01</th>\n      <td>70.777981</td>\n      <td>74.846952</td>\n      <td>41.783996</td>\n      <td>39.136622</td>\n      <td>83.163455</td>\n      <td>87.329391</td>\n      <td>132.7</td>\n      <td>25.806427</td>\n      <td>21.147724</td>\n      <td>53.721605</td>\n      <td>...</td>\n      <td>1302.703793</td>\n      <td>1247.306845</td>\n      <td>1189.125687</td>\n      <td>-58.181158</td>\n      <td>1115.276460</td>\n      <td>1138.449951</td>\n      <td>1071.648438</td>\n      <td>1148.079956</td>\n      <td>0.000000</td>\n      <td>-8.630005</td>\n    </tr>\n    <tr>\n      <th>2003-06-01</th>\n      <td>70.038462</td>\n      <td>77.504989</td>\n      <td>50.467978</td>\n      <td>41.209537</td>\n      <td>86.099642</td>\n      <td>89.937602</td>\n      <td>136.3</td>\n      <td>32.562486</td>\n      <td>26.058890</td>\n      <td>57.717691</td>\n      <td>...</td>\n      <td>981.448097</td>\n      <td>1002.221903</td>\n      <td>935.591515</td>\n      <td>-66.630388</td>\n      <td>963.154458</td>\n      <td>973.500000</td>\n      <td>962.451111</td>\n      <td>990.309998</td>\n      <td>0.000000</td>\n      <td>-15.809998</td>\n    </tr>\n    <tr>\n      <th>2003-09-01</th>\n      <td>71.018201</td>\n      <td>78.137855</td>\n      <td>51.136637</td>\n      <td>41.375370</td>\n      <td>89.402697</td>\n      <td>90.827986</td>\n      <td>137.1</td>\n      <td>32.770267</td>\n      <td>27.492042</td>\n      <td>58.958453</td>\n      <td>...</td>\n      <td>964.637623</td>\n      <td>1001.372698</td>\n      <td>960.348476</td>\n      <td>-41.024222</td>\n      <td>997.010888</td>\n      <td>994.969971</td>\n      <td>1037.929077</td>\n      <td>1050.709961</td>\n      <td>54.739990</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2003-10-01</th>\n      <td>71.657364</td>\n      <td>78.053473</td>\n      <td>51.284893</td>\n      <td>41.707036</td>\n      <td>89.843105</td>\n      <td>90.928790</td>\n      <td>138.2</td>\n      <td>33.097970</td>\n      <td>27.719249</td>\n      <td>59.986179</td>\n      <td>...</td>\n      <td>960.852385</td>\n      <td>1005.136488</td>\n      <td>974.256639</td>\n      <td>-30.879848</td>\n      <td>1032.810270</td>\n      <td>1049.709961</td>\n      <td>1050.734741</td>\n      <td>1058.199951</td>\n      <td>7.489990</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2004-02-01</th>\n      <td>73.461853</td>\n      <td>78.559766</td>\n      <td>52.437761</td>\n      <td>41.789953</td>\n      <td>92.485549</td>\n      <td>92.288700</td>\n      <td>139.3</td>\n      <td>34.714798</td>\n      <td>29.991318</td>\n      <td>65.642278</td>\n      <td>...</td>\n      <td>963.138570</td>\n      <td>1034.665134</td>\n      <td>1043.940929</td>\n      <td>9.275795</td>\n      <td>1135.899103</td>\n      <td>1143.939941</td>\n      <td>1097.700073</td>\n      <td>1126.209961</td>\n      <td>0.000000</td>\n      <td>18.729980</td>\n    </tr>\n    <tr>\n      <th>2004-08-01</th>\n      <td>73.825029</td>\n      <td>79.952071</td>\n      <td>54.385503</td>\n      <td>43.282451</td>\n      <td>96.449216</td>\n      <td>93.024557</td>\n      <td>143.7</td>\n      <td>36.342252</td>\n      <td>33.556720</td>\n      <td>65.130335</td>\n      <td>...</td>\n      <td>1022.478091</td>\n      <td>1065.198685</td>\n      <td>1089.055767</td>\n      <td>23.857081</td>\n      <td>1106.925911</td>\n      <td>1103.239990</td>\n      <td>1123.018188</td>\n      <td>1114.579956</td>\n      <td>10.339966</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2004-09-01</th>\n      <td>74.637104</td>\n      <td>80.120835</td>\n      <td>54.565081</td>\n      <td>43.365368</td>\n      <td>97.109827</td>\n      <td>93.111485</td>\n      <td>144.2</td>\n      <td>36.498523</td>\n      <td>34.605367</td>\n      <td>64.243999</td>\n      <td>...</td>\n      <td>1033.657136</td>\n      <td>1068.902637</td>\n      <td>1092.982853</td>\n      <td>24.080215</td>\n      <td>1112.028608</td>\n      <td>1113.579956</td>\n      <td>1138.211426</td>\n      <td>1130.199951</td>\n      <td>15.619995</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2005-01-01</th>\n      <td>74.934531</td>\n      <td>80.458364</td>\n      <td>55.980587</td>\n      <td>43.614117</td>\n      <td>97.660336</td>\n      <td>95.269399</td>\n      <td>146.2</td>\n      <td>38.738902</td>\n      <td>36.125906</td>\n      <td>65.838104</td>\n      <td>...</td>\n      <td>1092.479039</td>\n      <td>1097.564760</td>\n      <td>1134.423003</td>\n      <td>36.858243</td>\n      <td>1185.413457</td>\n      <td>1180.270020</td>\n      <td>1186.475586</td>\n      <td>1203.599976</td>\n      <td>22.329956</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2005-04-01</th>\n      <td>75.370012</td>\n      <td>82.103815</td>\n      <td>57.147144</td>\n      <td>43.862867</td>\n      <td>97.219928</td>\n      <td>96.010213</td>\n      <td>149.6</td>\n      <td>40.185975</td>\n      <td>38.432930</td>\n      <td>68.838945</td>\n      <td>...</td>\n      <td>1121.652367</td>\n      <td>1114.507639</td>\n      <td>1151.503453</td>\n      <td>36.995814</td>\n      <td>1166.646399</td>\n      <td>1155.849976</td>\n      <td>1175.471558</td>\n      <td>1191.500000</td>\n      <td>34.650024</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2005-08-01</th>\n      <td>74.766753</td>\n      <td>82.863255</td>\n      <td>57.657107</td>\n      <td>44.774949</td>\n      <td>98.100743</td>\n      <td>96.527415</td>\n      <td>151.8</td>\n      <td>40.896901</td>\n      <td>40.477793</td>\n      <td>69.427724</td>\n      <td>...</td>\n      <td>1156.149990</td>\n      <td>1140.096906</td>\n      <td>1180.969860</td>\n      <td>40.872954</td>\n      <td>1219.884997</td>\n      <td>1219.329956</td>\n      <td>1259.066650</td>\n      <td>1228.810059</td>\n      <td>8.480103</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2005-12-01</th>\n      <td>75.665965</td>\n      <td>83.032019</td>\n      <td>58.824360</td>\n      <td>45.604115</td>\n      <td>99.642169</td>\n      <td>97.345343</td>\n      <td>152.8</td>\n      <td>41.870245</td>\n      <td>40.757432</td>\n      <td>71.069876</td>\n      <td>...</td>\n      <td>1176.120948</td>\n      <td>1165.177811</td>\n      <td>1207.572673</td>\n      <td>42.394862</td>\n      <td>1244.665024</td>\n      <td>1247.290039</td>\n      <td>1282.921021</td>\n      <td>1280.079956</td>\n      <td>31.789917</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2006-02-01</th>\n      <td>77.032870</td>\n      <td>83.833649</td>\n      <td>59.414135</td>\n      <td>45.521199</td>\n      <td>100.633086</td>\n      <td>97.474594</td>\n      <td>153.5</td>\n      <td>43.599628</td>\n      <td>42.312926</td>\n      <td>71.088152</td>\n      <td>...</td>\n      <td>1191.966657</td>\n      <td>1181.668302</td>\n      <td>1228.255789</td>\n      <td>46.587487</td>\n      <td>1276.531682</td>\n      <td>1279.660034</td>\n      <td>1299.351807</td>\n      <td>1294.869995</td>\n      <td>14.209961</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2007-01-01</th>\n      <td>78.337664</td>\n      <td>85.401469</td>\n      <td>60.939382</td>\n      <td>48.581447</td>\n      <td>102.064410</td>\n      <td>98.980010</td>\n      <td>156.4</td>\n      <td>46.403498</td>\n      <td>45.825895</td>\n      <td>82.986169</td>\n      <td>...</td>\n      <td>1287.089518</td>\n      <td>1277.502481</td>\n      <td>1342.940615</td>\n      <td>65.438134</td>\n      <td>1428.099654</td>\n      <td>1437.239990</td>\n      <td>1407.298096</td>\n      <td>1406.819946</td>\n      <td>0.000000</td>\n      <td>31.420044</td>\n    </tr>\n    <tr>\n      <th>2007-07-01</th>\n      <td>80.071589</td>\n      <td>87.883570</td>\n      <td>62.081578</td>\n      <td>50.494103</td>\n      <td>106.248280</td>\n      <td>100.824511</td>\n      <td>164.9</td>\n      <td>48.636678</td>\n      <td>51.908051</td>\n      <td>83.886019</td>\n      <td>...</td>\n      <td>1359.762370</td>\n      <td>1348.671365</td>\n      <td>1425.179969</td>\n      <td>76.508604</td>\n      <td>1471.751666</td>\n      <td>1454.270020</td>\n      <td>1442.710693</td>\n      <td>1473.989990</td>\n      <td>0.000000</td>\n      <td>-18.719971</td>\n    </tr>\n    <tr>\n      <th>2007-10-01</th>\n      <td>81.519373</td>\n      <td>88.152327</td>\n      <td>62.673209</td>\n      <td>51.259165</td>\n      <td>109.551335</td>\n      <td>100.989445</td>\n      <td>164.5</td>\n      <td>49.869693</td>\n      <td>52.607149</td>\n      <td>82.686219</td>\n      <td>...</td>\n      <td>1396.537133</td>\n      <td>1383.736422</td>\n      <td>1462.886240</td>\n      <td>79.149818</td>\n      <td>1535.891546</td>\n      <td>1548.380005</td>\n      <td>1486.721191</td>\n      <td>1481.140015</td>\n      <td>0.000000</td>\n      <td>68.239990</td>\n    </tr>\n    <tr>\n      <th>2008-06-01</th>\n      <td>85.057527</td>\n      <td>92.320382</td>\n      <td>65.686351</td>\n      <td>53.554351</td>\n      <td>113.294798</td>\n      <td>99.203325</td>\n      <td>182.0</td>\n      <td>55.519400</td>\n      <td>65.802630</td>\n      <td>85.585736</td>\n      <td>...</td>\n      <td>1430.089042</td>\n      <td>1378.727611</td>\n      <td>1390.155783</td>\n      <td>11.428172</td>\n      <td>1316.586559</td>\n      <td>1279.000000</td>\n      <td>1299.429932</td>\n      <td>1267.380005</td>\n      <td>-12.619995</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2008-09-01</th>\n      <td>85.057442</td>\n      <td>92.306881</td>\n      <td>66.391436</td>\n      <td>55.849538</td>\n      <td>113.735205</td>\n      <td>93.018115</td>\n      <td>182.9</td>\n      <td>56.451465</td>\n      <td>66.169657</td>\n      <td>82.986169</td>\n      <td>...</td>\n      <td>1407.217611</td>\n      <td>1349.338935</td>\n      <td>1328.230472</td>\n      <td>-21.108463</td>\n      <td>1205.289113</td>\n      <td>1165.359985</td>\n      <td>1069.889282</td>\n      <td>968.750000</td>\n      <td>0.000000</td>\n      <td>197.609985</td>\n    </tr>\n    <tr>\n      <th>2009-05-01</th>\n      <td>84.035292</td>\n      <td>90.228127</td>\n      <td>68.593463</td>\n      <td>57.762193</td>\n      <td>99.421965</td>\n      <td>84.313900</td>\n      <td>165.8</td>\n      <td>61.740867</td>\n      <td>56.329848</td>\n      <td>69.188469</td>\n      <td>...</td>\n      <td>1179.003807</td>\n      <td>1125.020440</td>\n      <td>982.760056</td>\n      <td>-142.260384</td>\n      <td>894.543675</td>\n      <td>918.140015</td>\n      <td>891.160461</td>\n      <td>919.320007</td>\n      <td>0.000000</td>\n      <td>-0.179993</td>\n    </tr>\n    <tr>\n      <th>2009-07-01</th>\n      <td>83.615115</td>\n      <td>90.858884</td>\n      <td>69.005517</td>\n      <td>61.204973</td>\n      <td>101.954308</td>\n      <td>84.991476</td>\n      <td>167.1</td>\n      <td>62.502614</td>\n      <td>58.566962</td>\n      <td>71.488085</td>\n      <td>...</td>\n      <td>1123.321426</td>\n      <td>1100.720378</td>\n      <td>975.227730</td>\n      <td>-125.492648</td>\n      <td>962.007064</td>\n      <td>986.479980</td>\n      <td>979.624146</td>\n      <td>1020.619995</td>\n      <td>0.000000</td>\n      <td>-33.140015</td>\n    </tr>\n    <tr>\n      <th>2009-09-01</th>\n      <td>84.369315</td>\n      <td>91.119624</td>\n      <td>69.274883</td>\n      <td>62.352566</td>\n      <td>105.147261</td>\n      <td>86.636947</td>\n      <td>168.6</td>\n      <td>62.483863</td>\n      <td>60.104979</td>\n      <td>71.688052</td>\n      <td>...</td>\n      <td>1081.807138</td>\n      <td>1091.992829</td>\n      <td>993.729433</td>\n      <td>-98.263396</td>\n      <td>1038.414088</td>\n      <td>1056.079956</td>\n      <td>1046.526489</td>\n      <td>1036.189941</td>\n      <td>0.000000</td>\n      <td>20.890015</td>\n    </tr>\n  </tbody>\n</table>\n<p>20 rows × 81 columns</p>\n</div>"
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "            CHNCPIALLMINMEI  USACPIALLMINMEI  BRACPIALLMINMEI  \\\n2014-05-01        98.300000       100.372547        91.471949   \n2014-11-01        98.600000        99.634625        93.464701   \n2015-08-01       100.400000       100.548062       100.847560   \n2016-01-01       101.100000        99.957387       105.570863   \n2016-03-01       102.200000       100.470430       106.978945   \n2016-09-01       102.400000       101.861048       109.986054   \n2016-12-01       102.600000       101.862736       110.802041   \n2017-08-01       103.500000       103.587084       112.597119   \n2017-09-01       104.100000       104.135568       112.777160   \n2017-12-01       104.500000       104.011105       114.067844   \n2018-02-01       106.500000       105.051958       114.764808   \n2018-07-01       105.200000       106.324019       117.423900   \n2018-08-01       105.900000       106.383086       117.318334   \n2018-09-01       106.600000       106.506706       117.881427   \n2019-04-01       107.800000       107.818427       120.808261   \n2020-01-01       112.800000       108.840716       123.695420   \n2020-06-01       110.400000       108.767304       123.557141   \n2020-08-01       111.500000       109.662176       124.299579   \n2020-11-01       110.700000       109.793390       127.293928   \n2021-07-01       112.103816       115.182877       135.155660   \n\n            INDCPIALLMINMEI  BRAPROINDMISMEI  USAPROINDMISMEI  PCUOMFGOMFG  \\\n2014-05-01        93.337584       108.780622       101.331603        197.2   \n2014-11-01        96.780363       108.560418       102.753363        192.7   \n2015-08-01       100.988205        99.421965       100.022244        187.0   \n2016-01-01       102.900861        94.687586        98.603061        179.7   \n2016-03-01       102.518330        94.137077        97.330079        179.3   \n2016-09-01       105.961109        93.036058        97.737061        183.0   \n2016-12-01       105.196047        94.137077        98.024307        183.9   \n2017-08-01       109.021358        95.788604        98.826871        188.4   \n2017-09-01       109.021358        96.229012        98.869493        189.6   \n2017-12-01       109.403889       100.522984       100.476307        191.3   \n2018-02-01       109.786420        97.550234       100.753740        193.5   \n2018-07-01       115.141855        99.311863       102.634916        198.5   \n2018-08-01       115.141855        97.990641       103.247965        198.6   \n2018-09-01       115.141855        95.238095       103.213869        198.9   \n2019-04-01       119.349697        96.339114       101.353211        199.2   \n2020-01-01       126.235257        95.348197       100.200955        196.6   \n2020-06-01       127.000319        84.558216        90.358857        190.2   \n2020-08-01       129.295505        95.127993        95.047373        193.0   \n2020-11-01       132.101027        98.871456        96.304695        194.5   \n2021-07-01       135.296131        94.247179       100.517441        219.2   \n\n            RUSCPIALLMINMEI  PIEATI02RUM661N  RUSPROMANMISMEI  ...  \\\n2014-05-01        85.829110        87.550323        92.984503  ...   \n2014-11-01        89.406067        88.917585        97.383769  ...   \n2015-08-01       100.711169       102.779014       101.383103  ...   \n2016-01-01       104.586033        99.022299        97.883686  ...   \n2016-03-01       105.729052       100.832293       100.183303  ...   \n2016-09-01       107.785079       106.229722       101.083153  ...   \n2016-12-01       109.159747       108.117845       102.782870  ...   \n2017-08-01       111.145962       110.921862       107.682053  ...   \n2017-09-01       110.979243       113.596374       109.581736  ...   \n2017-12-01       111.913917       117.342856       105.982336  ...   \n2018-02-01       112.496598       118.691335       108.881853  ...   \n2018-07-01       114.547407       128.372850       109.481753  ...   \n2018-08-01       114.558862       128.665835       109.081820  ...   \n2018-09-01       114.742156       130.391189       109.681720  ...   \n2019-04-01       119.111999       130.300038       114.880853  ...   \n2020-01-01       120.727146       126.960013       115.180803  ...   \n2020-06-01       123.405070       119.147089       111.881353  ...   \n2020-08-01       123.787452       125.592751       117.480420  ...   \n2020-11-01       125.114767       128.086376       117.980337  ...   \n2021-07-01       131.848088       159.858933       120.079987  ...   \n\n                   ma21        26ema        12ema        MACD          ema  \\\n2014-05-01  1659.268090  1642.946809  1783.187832  140.241024  1907.823310   \n2014-11-01  1813.752854  1773.897175  1922.308224  148.411049  2046.275613   \n2015-08-01  1984.797137  1917.373328  2028.240003  110.866675  2012.847784   \n2016-01-01  2023.715716  1947.771757  2018.722471   70.950714  1977.234906   \n2016-03-01  2028.866670  1954.999737  2013.773481   58.773744  2022.237201   \n2016-09-01  2063.299531  2021.240190  2093.536543   72.296353  2166.476743   \n2016-12-01  2082.176206  2056.199425  2133.186008   76.986582  2218.910308   \n2017-08-01  2213.355236  2216.892534  2342.164883  125.272349  2465.084166   \n2017-09-01  2235.994292  2239.297541  2369.425687  130.128146  2501.268127   \n2017-12-01  2329.529541  2320.807592  2475.105430  154.297838  2654.157428   \n2018-02-01  2395.024309  2384.419855  2557.225801  172.805946  2731.639779   \n2018-07-01  2527.930036  2489.155965  2649.428094  160.272129  2780.360137   \n2018-08-01  2561.392415  2519.701452  2688.211467  168.510015  2861.133392   \n2018-09-01  2593.542411  2548.907270  2722.945085  174.037815  2896.364451   \n2019-04-01  2715.096703  2637.938374  2761.575815  123.637441  2898.789006   \n2020-01-01  2881.678583  2844.245092  3010.096993  165.851901  3212.542273   \n2020-06-01  2907.412388  2871.982877  2973.602880  101.620002  3060.460868   \n2020-08-01  2969.294306  2945.901300  3093.364808  147.463508  3400.506828   \n2020-11-01  3076.872861  3044.670027  3227.325448  182.655421  3516.133499   \n2021-07-01  3502.453788  3522.284910  3893.330483  371.045572  4348.059534   \n\n               momentum      close_1  true_close_1  buy_profit  sell_profit  \n2014-05-01  1922.569946  1860.521118   1960.229980    0.000000   -36.660034  \n2014-11-01  2066.560059  1984.935059   2058.899902    0.000000     8.660156  \n2015-08-01  1971.180054  1987.589111   1920.030029  -52.150024     0.000000  \n2016-01-01  1939.239990  1998.918823   1932.229980   -8.010010     0.000000  \n2016-03-01  2058.739990  2067.511719   2065.300049    5.560059     0.000000  \n2016-09-01  2167.270020  2223.591309   2126.149902  -42.120117     0.000000  \n2016-12-01  2237.830078  2246.772461   2278.870117   40.040039     0.000000  \n2017-08-01  2470.649902  2488.587158   2519.360107   47.710205     0.000000  \n2017-09-01  2518.360107  2532.872803   2575.260010   55.899902     0.000000  \n2017-12-01  2672.610107  2631.301514   2823.810059    0.000000  -150.199951  \n2018-02-01  2712.830078  2684.319336   2640.870117    0.000000    72.959961  \n2018-07-01  2815.290039  2772.105469   2901.520020    0.000000   -85.229980  \n2018-08-01  2900.520020  2834.293213   2913.979980    0.000000   -12.459961  \n2018-09-01  2912.979980  2861.195557   2711.739990    0.000000   202.239990  \n2019-04-01  2944.830078  2898.736328   2752.060059    0.000000   193.770020  \n2020-01-01  3224.520020  3107.958740   2954.219971    0.000000   271.300049  \n2020-06-01  3099.290039  3366.652100   3271.120117  170.830078     0.000000  \n2020-08-01  3499.310059  3518.934814   3363.000000 -137.310059     0.000000  \n2020-11-01  3620.629883  3648.608398   3756.070068  134.440186     0.000000  \n2021-07-01  4394.259766  4348.806152   4522.680176    0.000000  -127.420410  \n\n[20 rows x 81 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CHNCPIALLMINMEI</th>\n      <th>USACPIALLMINMEI</th>\n      <th>BRACPIALLMINMEI</th>\n      <th>INDCPIALLMINMEI</th>\n      <th>BRAPROINDMISMEI</th>\n      <th>USAPROINDMISMEI</th>\n      <th>PCUOMFGOMFG</th>\n      <th>RUSCPIALLMINMEI</th>\n      <th>PIEATI02RUM661N</th>\n      <th>RUSPROMANMISMEI</th>\n      <th>...</th>\n      <th>ma21</th>\n      <th>26ema</th>\n      <th>12ema</th>\n      <th>MACD</th>\n      <th>ema</th>\n      <th>momentum</th>\n      <th>close_1</th>\n      <th>true_close_1</th>\n      <th>buy_profit</th>\n      <th>sell_profit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2014-05-01</th>\n      <td>98.300000</td>\n      <td>100.372547</td>\n      <td>91.471949</td>\n      <td>93.337584</td>\n      <td>108.780622</td>\n      <td>101.331603</td>\n      <td>197.2</td>\n      <td>85.829110</td>\n      <td>87.550323</td>\n      <td>92.984503</td>\n      <td>...</td>\n      <td>1659.268090</td>\n      <td>1642.946809</td>\n      <td>1783.187832</td>\n      <td>140.241024</td>\n      <td>1907.823310</td>\n      <td>1922.569946</td>\n      <td>1860.521118</td>\n      <td>1960.229980</td>\n      <td>0.000000</td>\n      <td>-36.660034</td>\n    </tr>\n    <tr>\n      <th>2014-11-01</th>\n      <td>98.600000</td>\n      <td>99.634625</td>\n      <td>93.464701</td>\n      <td>96.780363</td>\n      <td>108.560418</td>\n      <td>102.753363</td>\n      <td>192.7</td>\n      <td>89.406067</td>\n      <td>88.917585</td>\n      <td>97.383769</td>\n      <td>...</td>\n      <td>1813.752854</td>\n      <td>1773.897175</td>\n      <td>1922.308224</td>\n      <td>148.411049</td>\n      <td>2046.275613</td>\n      <td>2066.560059</td>\n      <td>1984.935059</td>\n      <td>2058.899902</td>\n      <td>0.000000</td>\n      <td>8.660156</td>\n    </tr>\n    <tr>\n      <th>2015-08-01</th>\n      <td>100.400000</td>\n      <td>100.548062</td>\n      <td>100.847560</td>\n      <td>100.988205</td>\n      <td>99.421965</td>\n      <td>100.022244</td>\n      <td>187.0</td>\n      <td>100.711169</td>\n      <td>102.779014</td>\n      <td>101.383103</td>\n      <td>...</td>\n      <td>1984.797137</td>\n      <td>1917.373328</td>\n      <td>2028.240003</td>\n      <td>110.866675</td>\n      <td>2012.847784</td>\n      <td>1971.180054</td>\n      <td>1987.589111</td>\n      <td>1920.030029</td>\n      <td>-52.150024</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2016-01-01</th>\n      <td>101.100000</td>\n      <td>99.957387</td>\n      <td>105.570863</td>\n      <td>102.900861</td>\n      <td>94.687586</td>\n      <td>98.603061</td>\n      <td>179.7</td>\n      <td>104.586033</td>\n      <td>99.022299</td>\n      <td>97.883686</td>\n      <td>...</td>\n      <td>2023.715716</td>\n      <td>1947.771757</td>\n      <td>2018.722471</td>\n      <td>70.950714</td>\n      <td>1977.234906</td>\n      <td>1939.239990</td>\n      <td>1998.918823</td>\n      <td>1932.229980</td>\n      <td>-8.010010</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2016-03-01</th>\n      <td>102.200000</td>\n      <td>100.470430</td>\n      <td>106.978945</td>\n      <td>102.518330</td>\n      <td>94.137077</td>\n      <td>97.330079</td>\n      <td>179.3</td>\n      <td>105.729052</td>\n      <td>100.832293</td>\n      <td>100.183303</td>\n      <td>...</td>\n      <td>2028.866670</td>\n      <td>1954.999737</td>\n      <td>2013.773481</td>\n      <td>58.773744</td>\n      <td>2022.237201</td>\n      <td>2058.739990</td>\n      <td>2067.511719</td>\n      <td>2065.300049</td>\n      <td>5.560059</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2016-09-01</th>\n      <td>102.400000</td>\n      <td>101.861048</td>\n      <td>109.986054</td>\n      <td>105.961109</td>\n      <td>93.036058</td>\n      <td>97.737061</td>\n      <td>183.0</td>\n      <td>107.785079</td>\n      <td>106.229722</td>\n      <td>101.083153</td>\n      <td>...</td>\n      <td>2063.299531</td>\n      <td>2021.240190</td>\n      <td>2093.536543</td>\n      <td>72.296353</td>\n      <td>2166.476743</td>\n      <td>2167.270020</td>\n      <td>2223.591309</td>\n      <td>2126.149902</td>\n      <td>-42.120117</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2016-12-01</th>\n      <td>102.600000</td>\n      <td>101.862736</td>\n      <td>110.802041</td>\n      <td>105.196047</td>\n      <td>94.137077</td>\n      <td>98.024307</td>\n      <td>183.9</td>\n      <td>109.159747</td>\n      <td>108.117845</td>\n      <td>102.782870</td>\n      <td>...</td>\n      <td>2082.176206</td>\n      <td>2056.199425</td>\n      <td>2133.186008</td>\n      <td>76.986582</td>\n      <td>2218.910308</td>\n      <td>2237.830078</td>\n      <td>2246.772461</td>\n      <td>2278.870117</td>\n      <td>40.040039</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2017-08-01</th>\n      <td>103.500000</td>\n      <td>103.587084</td>\n      <td>112.597119</td>\n      <td>109.021358</td>\n      <td>95.788604</td>\n      <td>98.826871</td>\n      <td>188.4</td>\n      <td>111.145962</td>\n      <td>110.921862</td>\n      <td>107.682053</td>\n      <td>...</td>\n      <td>2213.355236</td>\n      <td>2216.892534</td>\n      <td>2342.164883</td>\n      <td>125.272349</td>\n      <td>2465.084166</td>\n      <td>2470.649902</td>\n      <td>2488.587158</td>\n      <td>2519.360107</td>\n      <td>47.710205</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2017-09-01</th>\n      <td>104.100000</td>\n      <td>104.135568</td>\n      <td>112.777160</td>\n      <td>109.021358</td>\n      <td>96.229012</td>\n      <td>98.869493</td>\n      <td>189.6</td>\n      <td>110.979243</td>\n      <td>113.596374</td>\n      <td>109.581736</td>\n      <td>...</td>\n      <td>2235.994292</td>\n      <td>2239.297541</td>\n      <td>2369.425687</td>\n      <td>130.128146</td>\n      <td>2501.268127</td>\n      <td>2518.360107</td>\n      <td>2532.872803</td>\n      <td>2575.260010</td>\n      <td>55.899902</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2017-12-01</th>\n      <td>104.500000</td>\n      <td>104.011105</td>\n      <td>114.067844</td>\n      <td>109.403889</td>\n      <td>100.522984</td>\n      <td>100.476307</td>\n      <td>191.3</td>\n      <td>111.913917</td>\n      <td>117.342856</td>\n      <td>105.982336</td>\n      <td>...</td>\n      <td>2329.529541</td>\n      <td>2320.807592</td>\n      <td>2475.105430</td>\n      <td>154.297838</td>\n      <td>2654.157428</td>\n      <td>2672.610107</td>\n      <td>2631.301514</td>\n      <td>2823.810059</td>\n      <td>0.000000</td>\n      <td>-150.199951</td>\n    </tr>\n    <tr>\n      <th>2018-02-01</th>\n      <td>106.500000</td>\n      <td>105.051958</td>\n      <td>114.764808</td>\n      <td>109.786420</td>\n      <td>97.550234</td>\n      <td>100.753740</td>\n      <td>193.5</td>\n      <td>112.496598</td>\n      <td>118.691335</td>\n      <td>108.881853</td>\n      <td>...</td>\n      <td>2395.024309</td>\n      <td>2384.419855</td>\n      <td>2557.225801</td>\n      <td>172.805946</td>\n      <td>2731.639779</td>\n      <td>2712.830078</td>\n      <td>2684.319336</td>\n      <td>2640.870117</td>\n      <td>0.000000</td>\n      <td>72.959961</td>\n    </tr>\n    <tr>\n      <th>2018-07-01</th>\n      <td>105.200000</td>\n      <td>106.324019</td>\n      <td>117.423900</td>\n      <td>115.141855</td>\n      <td>99.311863</td>\n      <td>102.634916</td>\n      <td>198.5</td>\n      <td>114.547407</td>\n      <td>128.372850</td>\n      <td>109.481753</td>\n      <td>...</td>\n      <td>2527.930036</td>\n      <td>2489.155965</td>\n      <td>2649.428094</td>\n      <td>160.272129</td>\n      <td>2780.360137</td>\n      <td>2815.290039</td>\n      <td>2772.105469</td>\n      <td>2901.520020</td>\n      <td>0.000000</td>\n      <td>-85.229980</td>\n    </tr>\n    <tr>\n      <th>2018-08-01</th>\n      <td>105.900000</td>\n      <td>106.383086</td>\n      <td>117.318334</td>\n      <td>115.141855</td>\n      <td>97.990641</td>\n      <td>103.247965</td>\n      <td>198.6</td>\n      <td>114.558862</td>\n      <td>128.665835</td>\n      <td>109.081820</td>\n      <td>...</td>\n      <td>2561.392415</td>\n      <td>2519.701452</td>\n      <td>2688.211467</td>\n      <td>168.510015</td>\n      <td>2861.133392</td>\n      <td>2900.520020</td>\n      <td>2834.293213</td>\n      <td>2913.979980</td>\n      <td>0.000000</td>\n      <td>-12.459961</td>\n    </tr>\n    <tr>\n      <th>2018-09-01</th>\n      <td>106.600000</td>\n      <td>106.506706</td>\n      <td>117.881427</td>\n      <td>115.141855</td>\n      <td>95.238095</td>\n      <td>103.213869</td>\n      <td>198.9</td>\n      <td>114.742156</td>\n      <td>130.391189</td>\n      <td>109.681720</td>\n      <td>...</td>\n      <td>2593.542411</td>\n      <td>2548.907270</td>\n      <td>2722.945085</td>\n      <td>174.037815</td>\n      <td>2896.364451</td>\n      <td>2912.979980</td>\n      <td>2861.195557</td>\n      <td>2711.739990</td>\n      <td>0.000000</td>\n      <td>202.239990</td>\n    </tr>\n    <tr>\n      <th>2019-04-01</th>\n      <td>107.800000</td>\n      <td>107.818427</td>\n      <td>120.808261</td>\n      <td>119.349697</td>\n      <td>96.339114</td>\n      <td>101.353211</td>\n      <td>199.2</td>\n      <td>119.111999</td>\n      <td>130.300038</td>\n      <td>114.880853</td>\n      <td>...</td>\n      <td>2715.096703</td>\n      <td>2637.938374</td>\n      <td>2761.575815</td>\n      <td>123.637441</td>\n      <td>2898.789006</td>\n      <td>2944.830078</td>\n      <td>2898.736328</td>\n      <td>2752.060059</td>\n      <td>0.000000</td>\n      <td>193.770020</td>\n    </tr>\n    <tr>\n      <th>2020-01-01</th>\n      <td>112.800000</td>\n      <td>108.840716</td>\n      <td>123.695420</td>\n      <td>126.235257</td>\n      <td>95.348197</td>\n      <td>100.200955</td>\n      <td>196.6</td>\n      <td>120.727146</td>\n      <td>126.960013</td>\n      <td>115.180803</td>\n      <td>...</td>\n      <td>2881.678583</td>\n      <td>2844.245092</td>\n      <td>3010.096993</td>\n      <td>165.851901</td>\n      <td>3212.542273</td>\n      <td>3224.520020</td>\n      <td>3107.958740</td>\n      <td>2954.219971</td>\n      <td>0.000000</td>\n      <td>271.300049</td>\n    </tr>\n    <tr>\n      <th>2020-06-01</th>\n      <td>110.400000</td>\n      <td>108.767304</td>\n      <td>123.557141</td>\n      <td>127.000319</td>\n      <td>84.558216</td>\n      <td>90.358857</td>\n      <td>190.2</td>\n      <td>123.405070</td>\n      <td>119.147089</td>\n      <td>111.881353</td>\n      <td>...</td>\n      <td>2907.412388</td>\n      <td>2871.982877</td>\n      <td>2973.602880</td>\n      <td>101.620002</td>\n      <td>3060.460868</td>\n      <td>3099.290039</td>\n      <td>3366.652100</td>\n      <td>3271.120117</td>\n      <td>170.830078</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2020-08-01</th>\n      <td>111.500000</td>\n      <td>109.662176</td>\n      <td>124.299579</td>\n      <td>129.295505</td>\n      <td>95.127993</td>\n      <td>95.047373</td>\n      <td>193.0</td>\n      <td>123.787452</td>\n      <td>125.592751</td>\n      <td>117.480420</td>\n      <td>...</td>\n      <td>2969.294306</td>\n      <td>2945.901300</td>\n      <td>3093.364808</td>\n      <td>147.463508</td>\n      <td>3400.506828</td>\n      <td>3499.310059</td>\n      <td>3518.934814</td>\n      <td>3363.000000</td>\n      <td>-137.310059</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2020-11-01</th>\n      <td>110.700000</td>\n      <td>109.793390</td>\n      <td>127.293928</td>\n      <td>132.101027</td>\n      <td>98.871456</td>\n      <td>96.304695</td>\n      <td>194.5</td>\n      <td>125.114767</td>\n      <td>128.086376</td>\n      <td>117.980337</td>\n      <td>...</td>\n      <td>3076.872861</td>\n      <td>3044.670027</td>\n      <td>3227.325448</td>\n      <td>182.655421</td>\n      <td>3516.133499</td>\n      <td>3620.629883</td>\n      <td>3648.608398</td>\n      <td>3756.070068</td>\n      <td>134.440186</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2021-07-01</th>\n      <td>112.103816</td>\n      <td>115.182877</td>\n      <td>135.155660</td>\n      <td>135.296131</td>\n      <td>94.247179</td>\n      <td>100.517441</td>\n      <td>219.2</td>\n      <td>131.848088</td>\n      <td>159.858933</td>\n      <td>120.079987</td>\n      <td>...</td>\n      <td>3502.453788</td>\n      <td>3522.284910</td>\n      <td>3893.330483</td>\n      <td>371.045572</td>\n      <td>4348.059534</td>\n      <td>4394.259766</td>\n      <td>4348.806152</td>\n      <td>4522.680176</td>\n      <td>0.000000</td>\n      <td>-127.420410</td>\n    </tr>\n  </tbody>\n</table>\n<p>20 rows × 81 columns</p>\n</div>"
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the final dataframe to csv-results folder\n",
    "csv_results_folder = \"csv-results\"\n",
    "if not os.path.isdir(csv_results_folder):\n",
    "    os.mkdir(csv_results_folder)\n",
    "csv_filename = os.path.join(csv_results_folder, model_name + \".csv\")\n",
    "final_df.to_csv(csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "candles = final_df\n",
    "import datetime\n",
    "candles.rename(columns={'close_15':'predict','true_close_15':'test'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "outputs": [],
   "source": [
    "# Рисует основной график\n",
    "def draw_candles(candles):\n",
    "    # Добавим на график несколько ЕМА-средних\n",
    "    # candles['ema100'] = pd.Series.ewm(candles['close'], span=100).mean()\n",
    "    # candles['ema50'] = pd.Series.ewm(candles['close'], span=50).mean()\n",
    "    # candles['ema20'] = pd.Series.ewm(candles['close'], span=20).mean()\n",
    "    # candles['26ema'] = pd.Series.ewm(candles['close'], span=26).mean()\n",
    "    # candles['12ema'] = pd.Series.ewm(candles['close'], span=12).mean()\n",
    "    # candles['MACD'] = (candles['12ema']-candles['26ema'])\n",
    "    plt.style.use('ggplot')  # 'seaborn-paper'\n",
    "    # Отображаем график по цене закрытия свечей и ЕМА-шки\n",
    "    fig = candles.plot(y=['test', 'predict','26ema','MACD','12ema'], figsize=(25, 16))\n",
    "    # Добавляем заголовок\n",
    "    fig.set_title('График ' + ticker)\n",
    "    # Рисуем шкалу с датами\n",
    "    PlotDatesX(fig, candles)\n",
    "\n",
    "# Определяет начальную и конечную позицию Х (по индексу свечей) для заданной даты. Пригодится при отрисовке ценовых уровней\n",
    "def DateX(date, candles):\n",
    "    # Цикл по датам в свечах, результат - список X-координат, соответствующих заданной дате\n",
    "    xpositions = [index for index, row in candles.iterrows() if row['Date'].date() == date]\n",
    "    # Возвращает список - пару начальная координата Х и конечная координата Х для заданной даты на графике\n",
    "    if xpositions == []:\n",
    "        return [len(candles) - 1, len(candles)]  # На случай если за текущую дату нет еще свечей\n",
    "    return [xpositions[0], xpositions[-1]]\n",
    "\n",
    "\n",
    "# Рисует метки дат на оси Х\n",
    "def PlotDatesX(fig, candles):\n",
    "    # Составляем список дат (только уникальные даты) из столбца DT. Они будут метками на оси Х. Сортировка по датам\n",
    "    # обязательна, т.к. при создании множества(set) даже из отсортированного списка, множество может не сохранить порядок списка\n",
    "    dates = sorted(set(map(lambda dt: datetime.date(dt), candles['Date'])))\n",
    "    # Создаем список координат Х для каждой метки (даты). Нам нужна только первая позиция - [0].\n",
    "    xlabel = [DateX(d, candles)[0] for d in dates]\n",
    "    # Рисуем ось Х, разделенную по датам\n",
    "    fig.set_xticklabels(dates)\n",
    "    fig.set_xticks(xlabel)\n",
    "    return dates, xlabel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "outputs": [
    {
     "data": {
      "text/plain": "            CHNCPIALLMINMEI  USACPIALLMINMEI  BRACPIALLMINMEI  \\\n2014-05-01        98.300000       100.372547        91.471949   \n2014-11-01        98.600000        99.634625        93.464701   \n2015-08-01       100.400000       100.548062       100.847560   \n2016-01-01       101.100000        99.957387       105.570863   \n2016-03-01       102.200000       100.470430       106.978945   \n2016-09-01       102.400000       101.861048       109.986054   \n2016-12-01       102.600000       101.862736       110.802041   \n2017-08-01       103.500000       103.587084       112.597119   \n2017-09-01       104.100000       104.135568       112.777160   \n2017-12-01       104.500000       104.011105       114.067844   \n2018-02-01       106.500000       105.051958       114.764808   \n2018-07-01       105.200000       106.324019       117.423900   \n2018-08-01       105.900000       106.383086       117.318334   \n2018-09-01       106.600000       106.506706       117.881427   \n2019-04-01       107.800000       107.818427       120.808261   \n2020-01-01       112.800000       108.840716       123.695420   \n2020-06-01       110.400000       108.767304       123.557141   \n2020-08-01       111.500000       109.662176       124.299579   \n2020-11-01       110.700000       109.793390       127.293928   \n2021-07-01       112.103816       115.182877       135.155660   \n\n            INDCPIALLMINMEI  BRAPROINDMISMEI  USAPROINDMISMEI  PCUOMFGOMFG  \\\n2014-05-01        93.337584       108.780622       101.331603        197.2   \n2014-11-01        96.780363       108.560418       102.753363        192.7   \n2015-08-01       100.988205        99.421965       100.022244        187.0   \n2016-01-01       102.900861        94.687586        98.603061        179.7   \n2016-03-01       102.518330        94.137077        97.330079        179.3   \n2016-09-01       105.961109        93.036058        97.737061        183.0   \n2016-12-01       105.196047        94.137077        98.024307        183.9   \n2017-08-01       109.021358        95.788604        98.826871        188.4   \n2017-09-01       109.021358        96.229012        98.869493        189.6   \n2017-12-01       109.403889       100.522984       100.476307        191.3   \n2018-02-01       109.786420        97.550234       100.753740        193.5   \n2018-07-01       115.141855        99.311863       102.634916        198.5   \n2018-08-01       115.141855        97.990641       103.247965        198.6   \n2018-09-01       115.141855        95.238095       103.213869        198.9   \n2019-04-01       119.349697        96.339114       101.353211        199.2   \n2020-01-01       126.235257        95.348197       100.200955        196.6   \n2020-06-01       127.000319        84.558216        90.358857        190.2   \n2020-08-01       129.295505        95.127993        95.047373        193.0   \n2020-11-01       132.101027        98.871456        96.304695        194.5   \n2021-07-01       135.296131        94.247179       100.517441        219.2   \n\n            RUSCPIALLMINMEI  PIEATI02RUM661N  RUSPROMANMISMEI  ...  \\\n2014-05-01        85.829110        87.550323        92.984503  ...   \n2014-11-01        89.406067        88.917585        97.383769  ...   \n2015-08-01       100.711169       102.779014       101.383103  ...   \n2016-01-01       104.586033        99.022299        97.883686  ...   \n2016-03-01       105.729052       100.832293       100.183303  ...   \n2016-09-01       107.785079       106.229722       101.083153  ...   \n2016-12-01       109.159747       108.117845       102.782870  ...   \n2017-08-01       111.145962       110.921862       107.682053  ...   \n2017-09-01       110.979243       113.596374       109.581736  ...   \n2017-12-01       111.913917       117.342856       105.982336  ...   \n2018-02-01       112.496598       118.691335       108.881853  ...   \n2018-07-01       114.547407       128.372850       109.481753  ...   \n2018-08-01       114.558862       128.665835       109.081820  ...   \n2018-09-01       114.742156       130.391189       109.681720  ...   \n2019-04-01       119.111999       130.300038       114.880853  ...   \n2020-01-01       120.727146       126.960013       115.180803  ...   \n2020-06-01       123.405070       119.147089       111.881353  ...   \n2020-08-01       123.787452       125.592751       117.480420  ...   \n2020-11-01       125.114767       128.086376       117.980337  ...   \n2021-07-01       131.848088       159.858933       120.079987  ...   \n\n                    ema     momentum      close_1  true_close_1  buy_profit  \\\n2014-05-01  1907.823310  1922.569946  1860.521118   1960.229980    0.000000   \n2014-11-01  2046.275613  2066.560059  1984.935059   2058.899902    0.000000   \n2015-08-01  2012.847784  1971.180054  1987.589111   1920.030029  -52.150024   \n2016-01-01  1977.234906  1939.239990  1998.918823   1932.229980   -8.010010   \n2016-03-01  2022.237201  2058.739990  2067.511719   2065.300049    5.560059   \n2016-09-01  2166.476743  2167.270020  2223.591309   2126.149902  -42.120117   \n2016-12-01  2218.910308  2237.830078  2246.772461   2278.870117   40.040039   \n2017-08-01  2465.084166  2470.649902  2488.587158   2519.360107   47.710205   \n2017-09-01  2501.268127  2518.360107  2532.872803   2575.260010   55.899902   \n2017-12-01  2654.157428  2672.610107  2631.301514   2823.810059    0.000000   \n2018-02-01  2731.639779  2712.830078  2684.319336   2640.870117    0.000000   \n2018-07-01  2780.360137  2815.290039  2772.105469   2901.520020    0.000000   \n2018-08-01  2861.133392  2900.520020  2834.293213   2913.979980    0.000000   \n2018-09-01  2896.364451  2912.979980  2861.195557   2711.739990    0.000000   \n2019-04-01  2898.789006  2944.830078  2898.736328   2752.060059    0.000000   \n2020-01-01  3212.542273  3224.520020  3107.958740   2954.219971    0.000000   \n2020-06-01  3060.460868  3099.290039  3366.652100   3271.120117  170.830078   \n2020-08-01  3400.506828  3499.310059  3518.934814   3363.000000 -137.310059   \n2020-11-01  3516.133499  3620.629883  3648.608398   3756.070068  134.440186   \n2021-07-01  4348.059534  4394.259766  4348.806152   4522.680176    0.000000   \n\n            sell_profit       Date       ema100        ema50        ema20  \n2014-05-01   -36.660034 2014-05-01  1332.297764  1367.972110  1475.366874  \n2014-11-01     8.660156 2014-11-01  1363.807609  1406.576620  1534.419631  \n2015-08-01     0.000000 2015-08-01  1389.292058  1437.297139  1577.877827  \n2016-01-01     0.000000 2016-01-01  1411.872278  1464.207950  1613.706344  \n2016-03-01     0.000000 2016-03-01  1437.873914  1495.623937  1657.648062  \n2016-09-01     0.000000 2016-09-01  1466.603620  1530.633971  1707.788416  \n2016-12-01     0.000000 2016-12-01  1496.396766  1567.027302  1759.780158  \n2017-08-01     0.000000 2017-08-01  1533.329301  1612.955660  1829.290520  \n2017-09-01     0.000000 2017-09-01  1570.007296  1658.448808  1896.510433  \n2017-12-01  -150.199951 2017-12-01  1610.356783  1708.847418  1972.043899  \n2018-02-01    72.959961 2018-02-01  1650.036519  1758.225673  2044.003800  \n2018-07-01   -85.229980 2018-07-01  1691.306959  1809.701491  2118.790045  \n2018-08-01   -12.459961 2018-08-01  1733.475795  1862.322984  2194.466600  \n2018-09-01   202.239990 2018-09-01  1773.999880  1912.557710  2263.930837  \n2019-04-01   193.770020 2019-04-01  1813.650253  1961.495759  2329.677862  \n2020-01-01   271.300049 2020-01-01  1860.761499  2020.878959  2415.950977  \n2020-06-01     0.000000 2020-06-01  1901.568478  2071.198457  2481.785347  \n2020-08-01     0.000000 2020-08-01  1953.519674  2137.330636  2579.674544  \n2020-11-01     0.000000 2020-11-01  2007.045600  2205.535145  2679.728513  \n2021-07-01  -127.420410 2021-07-01  2082.747988  2305.481273  2844.333181  \n\n[20 rows x 85 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CHNCPIALLMINMEI</th>\n      <th>USACPIALLMINMEI</th>\n      <th>BRACPIALLMINMEI</th>\n      <th>INDCPIALLMINMEI</th>\n      <th>BRAPROINDMISMEI</th>\n      <th>USAPROINDMISMEI</th>\n      <th>PCUOMFGOMFG</th>\n      <th>RUSCPIALLMINMEI</th>\n      <th>PIEATI02RUM661N</th>\n      <th>RUSPROMANMISMEI</th>\n      <th>...</th>\n      <th>ema</th>\n      <th>momentum</th>\n      <th>close_1</th>\n      <th>true_close_1</th>\n      <th>buy_profit</th>\n      <th>sell_profit</th>\n      <th>Date</th>\n      <th>ema100</th>\n      <th>ema50</th>\n      <th>ema20</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2014-05-01</th>\n      <td>98.300000</td>\n      <td>100.372547</td>\n      <td>91.471949</td>\n      <td>93.337584</td>\n      <td>108.780622</td>\n      <td>101.331603</td>\n      <td>197.2</td>\n      <td>85.829110</td>\n      <td>87.550323</td>\n      <td>92.984503</td>\n      <td>...</td>\n      <td>1907.823310</td>\n      <td>1922.569946</td>\n      <td>1860.521118</td>\n      <td>1960.229980</td>\n      <td>0.000000</td>\n      <td>-36.660034</td>\n      <td>2014-05-01</td>\n      <td>1332.297764</td>\n      <td>1367.972110</td>\n      <td>1475.366874</td>\n    </tr>\n    <tr>\n      <th>2014-11-01</th>\n      <td>98.600000</td>\n      <td>99.634625</td>\n      <td>93.464701</td>\n      <td>96.780363</td>\n      <td>108.560418</td>\n      <td>102.753363</td>\n      <td>192.7</td>\n      <td>89.406067</td>\n      <td>88.917585</td>\n      <td>97.383769</td>\n      <td>...</td>\n      <td>2046.275613</td>\n      <td>2066.560059</td>\n      <td>1984.935059</td>\n      <td>2058.899902</td>\n      <td>0.000000</td>\n      <td>8.660156</td>\n      <td>2014-11-01</td>\n      <td>1363.807609</td>\n      <td>1406.576620</td>\n      <td>1534.419631</td>\n    </tr>\n    <tr>\n      <th>2015-08-01</th>\n      <td>100.400000</td>\n      <td>100.548062</td>\n      <td>100.847560</td>\n      <td>100.988205</td>\n      <td>99.421965</td>\n      <td>100.022244</td>\n      <td>187.0</td>\n      <td>100.711169</td>\n      <td>102.779014</td>\n      <td>101.383103</td>\n      <td>...</td>\n      <td>2012.847784</td>\n      <td>1971.180054</td>\n      <td>1987.589111</td>\n      <td>1920.030029</td>\n      <td>-52.150024</td>\n      <td>0.000000</td>\n      <td>2015-08-01</td>\n      <td>1389.292058</td>\n      <td>1437.297139</td>\n      <td>1577.877827</td>\n    </tr>\n    <tr>\n      <th>2016-01-01</th>\n      <td>101.100000</td>\n      <td>99.957387</td>\n      <td>105.570863</td>\n      <td>102.900861</td>\n      <td>94.687586</td>\n      <td>98.603061</td>\n      <td>179.7</td>\n      <td>104.586033</td>\n      <td>99.022299</td>\n      <td>97.883686</td>\n      <td>...</td>\n      <td>1977.234906</td>\n      <td>1939.239990</td>\n      <td>1998.918823</td>\n      <td>1932.229980</td>\n      <td>-8.010010</td>\n      <td>0.000000</td>\n      <td>2016-01-01</td>\n      <td>1411.872278</td>\n      <td>1464.207950</td>\n      <td>1613.706344</td>\n    </tr>\n    <tr>\n      <th>2016-03-01</th>\n      <td>102.200000</td>\n      <td>100.470430</td>\n      <td>106.978945</td>\n      <td>102.518330</td>\n      <td>94.137077</td>\n      <td>97.330079</td>\n      <td>179.3</td>\n      <td>105.729052</td>\n      <td>100.832293</td>\n      <td>100.183303</td>\n      <td>...</td>\n      <td>2022.237201</td>\n      <td>2058.739990</td>\n      <td>2067.511719</td>\n      <td>2065.300049</td>\n      <td>5.560059</td>\n      <td>0.000000</td>\n      <td>2016-03-01</td>\n      <td>1437.873914</td>\n      <td>1495.623937</td>\n      <td>1657.648062</td>\n    </tr>\n    <tr>\n      <th>2016-09-01</th>\n      <td>102.400000</td>\n      <td>101.861048</td>\n      <td>109.986054</td>\n      <td>105.961109</td>\n      <td>93.036058</td>\n      <td>97.737061</td>\n      <td>183.0</td>\n      <td>107.785079</td>\n      <td>106.229722</td>\n      <td>101.083153</td>\n      <td>...</td>\n      <td>2166.476743</td>\n      <td>2167.270020</td>\n      <td>2223.591309</td>\n      <td>2126.149902</td>\n      <td>-42.120117</td>\n      <td>0.000000</td>\n      <td>2016-09-01</td>\n      <td>1466.603620</td>\n      <td>1530.633971</td>\n      <td>1707.788416</td>\n    </tr>\n    <tr>\n      <th>2016-12-01</th>\n      <td>102.600000</td>\n      <td>101.862736</td>\n      <td>110.802041</td>\n      <td>105.196047</td>\n      <td>94.137077</td>\n      <td>98.024307</td>\n      <td>183.9</td>\n      <td>109.159747</td>\n      <td>108.117845</td>\n      <td>102.782870</td>\n      <td>...</td>\n      <td>2218.910308</td>\n      <td>2237.830078</td>\n      <td>2246.772461</td>\n      <td>2278.870117</td>\n      <td>40.040039</td>\n      <td>0.000000</td>\n      <td>2016-12-01</td>\n      <td>1496.396766</td>\n      <td>1567.027302</td>\n      <td>1759.780158</td>\n    </tr>\n    <tr>\n      <th>2017-08-01</th>\n      <td>103.500000</td>\n      <td>103.587084</td>\n      <td>112.597119</td>\n      <td>109.021358</td>\n      <td>95.788604</td>\n      <td>98.826871</td>\n      <td>188.4</td>\n      <td>111.145962</td>\n      <td>110.921862</td>\n      <td>107.682053</td>\n      <td>...</td>\n      <td>2465.084166</td>\n      <td>2470.649902</td>\n      <td>2488.587158</td>\n      <td>2519.360107</td>\n      <td>47.710205</td>\n      <td>0.000000</td>\n      <td>2017-08-01</td>\n      <td>1533.329301</td>\n      <td>1612.955660</td>\n      <td>1829.290520</td>\n    </tr>\n    <tr>\n      <th>2017-09-01</th>\n      <td>104.100000</td>\n      <td>104.135568</td>\n      <td>112.777160</td>\n      <td>109.021358</td>\n      <td>96.229012</td>\n      <td>98.869493</td>\n      <td>189.6</td>\n      <td>110.979243</td>\n      <td>113.596374</td>\n      <td>109.581736</td>\n      <td>...</td>\n      <td>2501.268127</td>\n      <td>2518.360107</td>\n      <td>2532.872803</td>\n      <td>2575.260010</td>\n      <td>55.899902</td>\n      <td>0.000000</td>\n      <td>2017-09-01</td>\n      <td>1570.007296</td>\n      <td>1658.448808</td>\n      <td>1896.510433</td>\n    </tr>\n    <tr>\n      <th>2017-12-01</th>\n      <td>104.500000</td>\n      <td>104.011105</td>\n      <td>114.067844</td>\n      <td>109.403889</td>\n      <td>100.522984</td>\n      <td>100.476307</td>\n      <td>191.3</td>\n      <td>111.913917</td>\n      <td>117.342856</td>\n      <td>105.982336</td>\n      <td>...</td>\n      <td>2654.157428</td>\n      <td>2672.610107</td>\n      <td>2631.301514</td>\n      <td>2823.810059</td>\n      <td>0.000000</td>\n      <td>-150.199951</td>\n      <td>2017-12-01</td>\n      <td>1610.356783</td>\n      <td>1708.847418</td>\n      <td>1972.043899</td>\n    </tr>\n    <tr>\n      <th>2018-02-01</th>\n      <td>106.500000</td>\n      <td>105.051958</td>\n      <td>114.764808</td>\n      <td>109.786420</td>\n      <td>97.550234</td>\n      <td>100.753740</td>\n      <td>193.5</td>\n      <td>112.496598</td>\n      <td>118.691335</td>\n      <td>108.881853</td>\n      <td>...</td>\n      <td>2731.639779</td>\n      <td>2712.830078</td>\n      <td>2684.319336</td>\n      <td>2640.870117</td>\n      <td>0.000000</td>\n      <td>72.959961</td>\n      <td>2018-02-01</td>\n      <td>1650.036519</td>\n      <td>1758.225673</td>\n      <td>2044.003800</td>\n    </tr>\n    <tr>\n      <th>2018-07-01</th>\n      <td>105.200000</td>\n      <td>106.324019</td>\n      <td>117.423900</td>\n      <td>115.141855</td>\n      <td>99.311863</td>\n      <td>102.634916</td>\n      <td>198.5</td>\n      <td>114.547407</td>\n      <td>128.372850</td>\n      <td>109.481753</td>\n      <td>...</td>\n      <td>2780.360137</td>\n      <td>2815.290039</td>\n      <td>2772.105469</td>\n      <td>2901.520020</td>\n      <td>0.000000</td>\n      <td>-85.229980</td>\n      <td>2018-07-01</td>\n      <td>1691.306959</td>\n      <td>1809.701491</td>\n      <td>2118.790045</td>\n    </tr>\n    <tr>\n      <th>2018-08-01</th>\n      <td>105.900000</td>\n      <td>106.383086</td>\n      <td>117.318334</td>\n      <td>115.141855</td>\n      <td>97.990641</td>\n      <td>103.247965</td>\n      <td>198.6</td>\n      <td>114.558862</td>\n      <td>128.665835</td>\n      <td>109.081820</td>\n      <td>...</td>\n      <td>2861.133392</td>\n      <td>2900.520020</td>\n      <td>2834.293213</td>\n      <td>2913.979980</td>\n      <td>0.000000</td>\n      <td>-12.459961</td>\n      <td>2018-08-01</td>\n      <td>1733.475795</td>\n      <td>1862.322984</td>\n      <td>2194.466600</td>\n    </tr>\n    <tr>\n      <th>2018-09-01</th>\n      <td>106.600000</td>\n      <td>106.506706</td>\n      <td>117.881427</td>\n      <td>115.141855</td>\n      <td>95.238095</td>\n      <td>103.213869</td>\n      <td>198.9</td>\n      <td>114.742156</td>\n      <td>130.391189</td>\n      <td>109.681720</td>\n      <td>...</td>\n      <td>2896.364451</td>\n      <td>2912.979980</td>\n      <td>2861.195557</td>\n      <td>2711.739990</td>\n      <td>0.000000</td>\n      <td>202.239990</td>\n      <td>2018-09-01</td>\n      <td>1773.999880</td>\n      <td>1912.557710</td>\n      <td>2263.930837</td>\n    </tr>\n    <tr>\n      <th>2019-04-01</th>\n      <td>107.800000</td>\n      <td>107.818427</td>\n      <td>120.808261</td>\n      <td>119.349697</td>\n      <td>96.339114</td>\n      <td>101.353211</td>\n      <td>199.2</td>\n      <td>119.111999</td>\n      <td>130.300038</td>\n      <td>114.880853</td>\n      <td>...</td>\n      <td>2898.789006</td>\n      <td>2944.830078</td>\n      <td>2898.736328</td>\n      <td>2752.060059</td>\n      <td>0.000000</td>\n      <td>193.770020</td>\n      <td>2019-04-01</td>\n      <td>1813.650253</td>\n      <td>1961.495759</td>\n      <td>2329.677862</td>\n    </tr>\n    <tr>\n      <th>2020-01-01</th>\n      <td>112.800000</td>\n      <td>108.840716</td>\n      <td>123.695420</td>\n      <td>126.235257</td>\n      <td>95.348197</td>\n      <td>100.200955</td>\n      <td>196.6</td>\n      <td>120.727146</td>\n      <td>126.960013</td>\n      <td>115.180803</td>\n      <td>...</td>\n      <td>3212.542273</td>\n      <td>3224.520020</td>\n      <td>3107.958740</td>\n      <td>2954.219971</td>\n      <td>0.000000</td>\n      <td>271.300049</td>\n      <td>2020-01-01</td>\n      <td>1860.761499</td>\n      <td>2020.878959</td>\n      <td>2415.950977</td>\n    </tr>\n    <tr>\n      <th>2020-06-01</th>\n      <td>110.400000</td>\n      <td>108.767304</td>\n      <td>123.557141</td>\n      <td>127.000319</td>\n      <td>84.558216</td>\n      <td>90.358857</td>\n      <td>190.2</td>\n      <td>123.405070</td>\n      <td>119.147089</td>\n      <td>111.881353</td>\n      <td>...</td>\n      <td>3060.460868</td>\n      <td>3099.290039</td>\n      <td>3366.652100</td>\n      <td>3271.120117</td>\n      <td>170.830078</td>\n      <td>0.000000</td>\n      <td>2020-06-01</td>\n      <td>1901.568478</td>\n      <td>2071.198457</td>\n      <td>2481.785347</td>\n    </tr>\n    <tr>\n      <th>2020-08-01</th>\n      <td>111.500000</td>\n      <td>109.662176</td>\n      <td>124.299579</td>\n      <td>129.295505</td>\n      <td>95.127993</td>\n      <td>95.047373</td>\n      <td>193.0</td>\n      <td>123.787452</td>\n      <td>125.592751</td>\n      <td>117.480420</td>\n      <td>...</td>\n      <td>3400.506828</td>\n      <td>3499.310059</td>\n      <td>3518.934814</td>\n      <td>3363.000000</td>\n      <td>-137.310059</td>\n      <td>0.000000</td>\n      <td>2020-08-01</td>\n      <td>1953.519674</td>\n      <td>2137.330636</td>\n      <td>2579.674544</td>\n    </tr>\n    <tr>\n      <th>2020-11-01</th>\n      <td>110.700000</td>\n      <td>109.793390</td>\n      <td>127.293928</td>\n      <td>132.101027</td>\n      <td>98.871456</td>\n      <td>96.304695</td>\n      <td>194.5</td>\n      <td>125.114767</td>\n      <td>128.086376</td>\n      <td>117.980337</td>\n      <td>...</td>\n      <td>3516.133499</td>\n      <td>3620.629883</td>\n      <td>3648.608398</td>\n      <td>3756.070068</td>\n      <td>134.440186</td>\n      <td>0.000000</td>\n      <td>2020-11-01</td>\n      <td>2007.045600</td>\n      <td>2205.535145</td>\n      <td>2679.728513</td>\n    </tr>\n    <tr>\n      <th>2021-07-01</th>\n      <td>112.103816</td>\n      <td>115.182877</td>\n      <td>135.155660</td>\n      <td>135.296131</td>\n      <td>94.247179</td>\n      <td>100.517441</td>\n      <td>219.2</td>\n      <td>131.848088</td>\n      <td>159.858933</td>\n      <td>120.079987</td>\n      <td>...</td>\n      <td>4348.059534</td>\n      <td>4394.259766</td>\n      <td>4348.806152</td>\n      <td>4522.680176</td>\n      <td>0.000000</td>\n      <td>-127.420410</td>\n      <td>2021-07-01</td>\n      <td>2082.747988</td>\n      <td>2305.481273</td>\n      <td>2844.333181</td>\n    </tr>\n  </tbody>\n</table>\n<p>20 rows × 85 columns</p>\n</div>"
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candles.rename(columns={'close_15':'predict','true_close_15':'test'}, inplace=True)\n",
    "candles['Date'] = candles.index\n",
    "candles['ema100'] = pd.Series.ewm(candles['close'], span=100).mean()\n",
    "candles['ema50'] = pd.Series.ewm(candles['close'], span=50).mean()\n",
    "candles['ema20'] = pd.Series.ewm(candles['close'], span=20).mean()\n",
    "candles['26ema'] = pd.Series.ewm(candles['close'], span=26).mean()\n",
    "candles['12ema'] = pd.Series.ewm(candles['close'], span=12).mean()\n",
    "candles['MACD'] = (candles['12ema']-candles['26ema'])\n",
    "# candles['Date']= datetime.datetime.strptime(str(candles['Date']), '%Y-%m-%d').date()#pd.to_datetime(candles['Date'],format='%Y%m%d')\n",
    "candles.tail(20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['test', 'predict'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_19772/1031423139.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mdraw_candles\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcandles\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtail\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1000\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_19772/2604581198.py\u001B[0m in \u001B[0;36mdraw_candles\u001B[1;34m(candles)\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstyle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0muse\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'ggplot'\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# 'seaborn-paper'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m     \u001B[1;31m# Отображаем график по цене закрытия свечей и ЕМА-шки\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m     \u001B[0mfig\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcandles\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mplot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'test'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'predict'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;34m'26ema'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;34m'MACD'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;34m'12ema'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfigsize\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m25\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m16\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     13\u001B[0m     \u001B[1;31m# Добавляем заголовок\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m     \u001B[0mfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_title\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'График '\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mticker\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\sem\\pycharmprojects\\python.neural.market\\venv\\lib\\site-packages\\pandas\\plotting\\_core.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    956\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    957\u001B[0m                 \u001B[1;31m# don't overwrite\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 958\u001B[1;33m                 \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    959\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    960\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mABCSeries\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\sem\\pycharmprojects\\python.neural.market\\venv\\lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3462\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mis_iterator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3463\u001B[0m                 \u001B[0mkey\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3464\u001B[1;33m             \u001B[0mindexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_listlike_indexer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3465\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3466\u001B[0m         \u001B[1;31m# take() does not accept boolean indexers\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\sem\\pycharmprojects\\python.neural.market\\venv\\lib\\site-packages\\pandas\\core\\indexing.py\u001B[0m in \u001B[0;36m_get_listlike_indexer\u001B[1;34m(self, key, axis)\u001B[0m\n\u001B[0;32m   1312\u001B[0m             \u001B[0mkeyarr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindexer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnew_indexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0max\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reindex_non_unique\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkeyarr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1313\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1314\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_validate_read_indexer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkeyarr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindexer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1315\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1316\u001B[0m         if needs_i8_conversion(ax.dtype) or isinstance(\n",
      "\u001B[1;32mc:\\users\\sem\\pycharmprojects\\python.neural.market\\venv\\lib\\site-packages\\pandas\\core\\indexing.py\u001B[0m in \u001B[0;36m_validate_read_indexer\u001B[1;34m(self, key, indexer, axis)\u001B[0m\n\u001B[0;32m   1375\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1376\u001B[0m             \u001B[0mnot_found\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mensure_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mmissing_mask\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnonzero\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munique\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1377\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"{not_found} not in index\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1378\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1379\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: \"['test', 'predict'] not in index\""
     ]
    }
   ],
   "source": [
    "\n",
    "draw_candles(candles.tail(1000))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}